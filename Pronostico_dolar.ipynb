{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa8ccfdb-8844-4d99-9b85-cfd95e239459",
   "metadata": {},
   "source": [
    "**Proyecto: Pronóstico del dolar con el algoritmo de previsión DeepAR AWS**\n",
    "\n",
    "Universidad Sergio Arboleda\n",
    "\n",
    "Electiva 3 - Maestría en IA\n",
    "\n",
    "Edwar A. Hernández"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff46b57f-d716-4e99-939b-858f4712d07e",
   "metadata": {},
   "source": [
    "### Ajustes iniciales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6035b409-6fd2-4c9d-aab8-89e86d1071dc",
   "metadata": {},
   "source": [
    "#### Cargue de librerías requeridas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf4ecec-1416-4561-9a5d-b963732f18d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import zipfile\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "from random import shuffle\n",
    "import random\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "import boto3\n",
    "import s3fs\n",
    "import sagemaker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import timedelta\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import IntSlider, FloatSlider, Checkbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd9bf12-a288-4e13-bd90-acabca34484f",
   "metadata": {},
   "source": [
    "#### Ajuste de semillas para reproducibilidad de resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b8cb79d-7311-44a4-96bf-a755a4e6a421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e30c2ac-01b4-4288-8af1-3520c9e4fee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "613afb95-834a-4873-bb4b-49c3f969e17a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_bucket = \"pronostico-dolar-04-2023\"  # reemplazar con un bucket existente si es necesario\n",
    "s3_prefix = \"deepar-dolar-notebook\"  # prefijo utilizado para todos los datos almacenados en el bucket\n",
    "\n",
    "role = sagemaker.get_execution_role() # 'arn:aws:iam::503179242072:role/LabRole' IAM role usado por SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "257efbbe-36f5-4571-b326-11b1a1a3d598",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = sagemaker_session.boto_region_name\n",
    "\n",
    "s3_data_path = \"s3://{}/{}/data\".format(s3_bucket, s3_prefix) #Configuración path carpeta datos de entrada\n",
    "s3_output_path = \"s3://{}/{}/output\".format(s3_bucket, s3_prefix) #Configuración path carpeta datos de salida"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bf8b92-1ac7-4b60-9ef6-a1bb9d429574",
   "metadata": {},
   "source": [
    "#### Configuración de imagen del contenedor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7f32ec4-fa30-4442-ba19-b6351b7615c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_name = sagemaker.image_uris.retrieve(\"forecasting-deepar\", region)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85db6b0-1f80-4703-bb06-2dd0ce226cb0",
   "metadata": {},
   "source": [
    "#### EDA y ajustes iniciales del dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "221646c1-45e0-4984-b175-e2a7b13302c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "FILE_NAME = \"Serie_historica_dolar_Deep_AR.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b900b28e-2892-402a-a550-be7404861306",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(FILE_NAME, index_col=0, parse_dates=True, dayfirst=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53b312e7-3c99-4863-935f-36142bee866f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>start</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2022-04-10</th>\n",
       "      <td>3777.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-11</th>\n",
       "      <td>3777.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-12</th>\n",
       "      <td>3744.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-13</th>\n",
       "      <td>3736.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022-04-14</th>\n",
       "      <td>3737.32</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target\n",
       "start              \n",
       "2022-04-10  3777.41\n",
       "2022-04-11  3777.41\n",
       "2022-04-12  3744.16\n",
       "2022-04-13  3736.70\n",
       "2022-04-14  3737.32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "977f6772-ae45-4e3f-8334-0e5cc792f377",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "timeseries = data.iloc[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8c5d029a-e41f-44ee-b402-184b91f6318b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA1sAAAHkCAYAAADIPzWMAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAADYpUlEQVR4nOzdd3hUZfbA8e/NzKT3hFRC771JswACiopYVtlVxAKWtSGKurq7/oS16yoqrh1Bxd5QFBFQRJAqRXonENJ7L1Pu74+ZO8mQNjOZ9PN5njyQO/feeadmzpzznldRVVVFCCGEEEIIIYRHeTX3AIQQQgghhBCiLZJgSwghhBBCCCEagQRbQgghhBBCCNEIJNgSQgghhBBCiEYgwZYQQgghhBBCNAIJtoQQQgghhBCiEUiwJYQQQgghhBCNQIItIYQQQgghhGgEEmwJIYQQQgghRCOQYEu0K3v27OGWW26ha9eu+Pr6EhgYyLBhw3j++efJyclp7uE1utdff52lS5c26xhuvvlmunTp0qjXkZKSwvz589m9e3ejXo/m119/RVEUfv31V4+cLzExEUVRPPJYzZ8/H0VR6v0ZP348YH18qm739vame/fuPPjggxQUFFQ7v7bfzTffXOP1/+c//7Hvk5iY2KDb4I6GPt/Gjx9vv29cpSgK8+fPd/u6XfH9999z4403MnDgQAwGQ533l9FoZMGCBXTp0gUfHx/69OnDokWLXL7OV199FUVRGDBgQI2Xa8/j//73vy6fu6qaHn9nHhez2cxLL73ElClT6NixI/7+/vTt25dHHnmEvLy8Go9ZtGgRffr0wcfHh65du7JgwQKMRqPDPl9//TXXXXcdPXr0wM/Pjy5dujBjxgyOHj3qsF9BQQFPPfUU48ePJyYmhsDAQAYOHMhzzz1HWVmZy/dDffbv389dd93FmDFjCAgIqPc96dNPP2XIkCH4+voSFxfH3LlzKSoqcuk69+7di6IoGAwGUlNTa9ynS5cuTJ061aXznq2m91hn3xecfbw0a9euZcyYMfj7+xMZGcnNN99MRkaGwz47duzg7rvvZuDAgQQFBREdHc2kSZP45Zdfqp3vk08+4YILLiA6OhofHx/i4uK4/PLL2bRpk2t3gmi1JNgS7cY777zD8OHD2b59Ow899BCrVq3im2++4dprr+XNN99k9uzZzT3ERtcSgq3HHnuMb775plGvIyUlhQULFjRZsNWS3XrrrWzevNn+8/XXXwNw7733Omx//fXX7cf4+fnZt3/33XdMmDCBF198kWuuuabG6wgKCuKLL76gsLDQYbuqqixdupTg4ODGu4ECgG+++YYtW7bQr18/Bg8eXOe+d911F8888wx33303P/30E1dddRX33XcfTz/9tEvX+d577wHWD/lbt251e+z10Z7DriotLWX+/Pl07tyZl19+mZUrV3Lbbbfx9ttvc+6551JaWuqw/1NPPcV9993H1VdfzU8//cRdd93F008/zd133+2w33PPPUdJSQn/+te/WLVqFU8++SS7du1i2LBh7N+/377f6dOnefnllxk2bBhvv/023333Hddccw3z589n6tSpqKrq3h1Siz/++IPly5cTHh7OxIkT69z3o48+4rrrruOcc87hxx9/5PHHH2fp0qVcffXVLl3nu+++C4DJZOKDDz5we+z1GTZsGJs3b2bYsGEuH+vs4wWwfv16LrnkEqKjo/n222955ZVXWLt2LRMnTqS8vNy+3yeffMK2bduYNWsW3377Le+++y4+Pj5MnDix2v2QnZ3Nueeey+uvv87q1at56aWXSE9P54ILLmD9+vXu3SGidVGFaAc2bdqk6nQ6dcqUKWpZWVm1y8vLy9Vvv/22GUbWNIqLi1VVVdX+/fur48aNa97BNIHt27ergLpkyZImub5169apgLpu3TqPnO/kyZONNn7t3C+88EKNl990001qQEBAte0TJkxQAfXEiRMO2wH1hhtuUP38/NS3337b4bK1a9eqgHrbbbepgHry5Em3xvz444+r7v65uummm9TOnTu7dayqquq4cePcfs0A6uOPP+72dVdlMplqfO/SmM1m+//vvvvuWu+vffv2qYqiqE8//bTD9ttuu0318/NTs7OznRqP9hq77LLL7I/x2ep7rtVHe9+qiTOPi8lkUrOysqpt/+KLL1RA/fDDD+3bsrKyVF9fX/X222932Pepp55SFUVR9+/fb9+Wnp5e7ZzJycmqwWBQZ8+ebd9WVFSkFhUVVdv3hRdeUAF1w4YNdY7fVVWfA9ptrOk9yWQyqbGxsepFF13ksP2jjz5SAXXlypVOXV9ZWZkaERGhDh48WI2Pj1d79epV436dO3dWL7vsMudvSBUVFRWq0Wis8TJn3xecfbxUVVXPOecctV+/fg7X+fvvv6uA+vrrr9d5TpPJpA4aNEjt3r17vWPKy8tTDQaDOnPmzHr3Fa2fZLZEu/D000+jKApvv/02Pj4+1S739vZm2rRp9t8tFgvPP/+8vZwkKiqKG2+8kTNnzjgcN378eAYMGMDmzZsZO3asvURhyZIlAPzwww8MGzYMf39/Bg4cyKpVqxyO18ogdu3axdVXX01wcDAhISHccMMNZGZmOuzr6ph+++03xo4di7+/P7NmzaJLly7s37+f9evX28u6tPKqsrIy5s2bx5AhQwgJCSE8PJwxY8bw7bffVruvFEXhnnvuYcmSJfTu3Rs/Pz9GjBjBli1bUFWVF154ga5duxIYGMiFF17IsWPHHI6vqaxLVVVef/11hgwZgp+fH2FhYVxzzTWcOHGixtu2fft2zj//fPz9/enWrRvPPvssFosFsJabnHPOOQDccsst9ttatZzru+++s5eJBAUFMXnyZKe/OT906BBTpkyxl5j8/e9/r5bR0WjfiAYHB+Pv78+5557Lzz//7NT1nO3YsWPccsst9OzZE39/f+Lj47n88svZu3evW+dz1YgRIwBIT0+vdllISAhXXXWVPdOhee+99zj33HPp1auX09fzww8/MGTIEHsZV20laM4+Z5ylqirPP/88nTt3xtfXl2HDhvHjjz/WuO/p06e54YYbiIqKwsfHh759+/Liiy/an4O1yczM5K677qJfv34EBgYSFRXFhRdeyIYNGxz208rvnn/+eZ588km6du2Kj48P69atq/XcXl7O/Tlfvnw5qqpyyy23OGy/5ZZbKC0trfYeVZvFixcD8OyzzzJ27Fg+/fRTSkpKatzXYrHw1FNP0alTJ3x9fRkxYkS114H2Xrhz506uueYawsLC6N69u8NlrtLpdERERFTbPnLkSACSkpLs21atWkVZWVmN94uqqixfvty+LSoqqto54+Li6Nixo8M5AwICCAgIcOr6PcHZ58CWLVtITU2tdluvvfZaAgMDna48WL58OdnZ2dx6663cdNNNHDlyhI0bN9a6/zfffMOgQYPw9fWlW7duvPrqqw6Xa6WCH374IfPmzSM+Ph4fHx+OHTvWoFJtZx+v5ORktm/fzsyZM9Hr9fbtY8eOpVevXg73S03n1Ol0DB8+3KnHNSgoCF9fX4frEW2XBFuizTObzfzyyy8MHz6chIQEp4658847+cc//sHkyZP57rvveOKJJ1i1ahVjx44lKyvLYd+0tDRuueUWbr31Vr799lsGDhzIrFmz+M9//sOjjz7Kww8/zFdffUVgYCBXXnklKSkp1a7vqquuokePHnz55ZfMnz+f5cuXc/HFFzvMFXBlTKmpqdxwww1cf/31rFy5krvuuotvvvmGbt26MXToUHuJmPbHo7y8nJycHB588EGWL1/OJ598wnnnncfVV19dY2nI999/z7vvvsuzzz7LJ598QmFhIZdddhnz5s3j999/57XXXuPtt9/mwIED/OUvf6m3XOaOO+5g7ty5TJo0ieXLl/P666+zf/9+xo4dW+3DfVpaGjNmzOCGG27gu+++45JLLuHRRx9l2bJlgLXcRAt2//3vf9tv66233grAxx9/zBVXXEFwcDCffPIJixcvJjc3l/Hjx9f5QQGsgca4cePYt28fr7/+Oh9++CFFRUXcc8891fZdtmwZF110EcHBwbz//vt8/vnnhIeHc/HFF7sVcKWkpBAREcGzzz7LqlWr+N///oder2fUqFEcPnzY5fO56uTJk+j1erp161bj5bNnz2bLli0cPHgQgLy8PL7++muXynN//vlnrrjiCoKCgvj000954YUX+Pzzz+2PZ1WuPGecsWDBAvvra/ny5dx5553cdttt1e7bzMxMxo4dy+rVq3niiSf47rvvmDRpEg8++GCNz4OqtHmhjz/+OD/88ANLliyhW7dujB8/vsYPka+++iq//PIL//3vf/nxxx/p06ePy7frbPv27aNDhw7ExMQ4bB80aJD98vqUlpbyySefcM455zBgwABmzZpFYWEhX3zxRY37v/baa6xatYqXX36ZZcuW4eXlxSWXXFLjFxxXX301PXr04IsvvuDNN9904xbWT5tX079/f/s27XYPHDjQYd/Y2FgiIyPrvV9OnDjBqVOnHM7pyvU3Je22aI+5xmAw0KdPH6eeA2ANuH18fJgxYwazZs1CURR7EH623bt3M3fuXO6//36++eYbxo4dy3333VfjlymPPvoop0+f5s0332TFihU1BjYNVdPjVdv9om2r734xmUxs2LCh1sfVbDZjNBpJTEzkzjvvRFXVaiWqoo1qvqSaEE0jLS1NBdS//e1vTu1/8OBBFVDvuusuh+1bt25VAfWf//ynfdu4ceNUQP3jjz/s27Kzs1WdTqf6+fmpycnJ9u27d+9WAfXVV1+1b9PKIO6//36H69LKOZYtW+b2mH7++edqt83ZMkKTyaQajUZ19uzZ6tChQx0uA9SYmBiH8pjly5ergDpkyBDVYrHYt7/88ssqoO7Zs8e+7eyyrs2bN6uA+uKLLzpcT1JSkurn56c+/PDD1W7b1q1bHfbt16+fevHFF9t/r62M0Gw2q3FxcerAgQMdSm4KCwvVqKgodezYsXXeL//4xz9URVHU3bt3O2yfPHmyQ8lOcXGxGh4erl5++eXVrn/w4MHqyJEj67weZ8oITSaTWlFRofbs2bPa88eZc9dXRmg0GlWj0ahmZWWpb7zxhurl5eXwPNMA6t13361aLBa1a9eu6oMPPqiqqqr+73//UwMDA9XCwkJ72VR9ZYSjRo1S4+Li1NLSUvu2goICNTw83KFcyJXnjDNlhLm5uaqvr6961VVXOWzXyoeqvmYeeeSRGp+Dd955p6ooinr48GGH+6auMkLtdTZx4kSH69Yeo+7du6sVFRV1jr0mdZURTp48We3du3eNl3l7e1cro6vJBx98oALqm2++qaqq9fUTGBionn/++Q77abejtsd00qRJ9m3ae+H//d//Vbu+msrF3C3vPHPmjBodHa2OGDHC4T3gtttuU318fGo8plevXtVK7qoyGo3q+PHj1eDgYPX06dN1Xv+ff/6p+vn5VXuueVpdZYRPPfWUCqipqanVLrvoootqLQesKjExUfXy8nL4uzpu3Dg1ICBALSgocNi3c+fOtb5vBgcH28tFtXLsCy64oNr11VSq7W55cW2Pl/Z3d/PmzdWOuf3221Vvb+86z/uvf/1LBdTly5fXeHnv3r1VQAXU2NhYdePGjS6PXbROktkS4ixaqc7Z3dVGjhxJ3759q2UlYmNjGT58uP338PBwoqKiGDJkCHFxcfbtffv2BeDUqVPVrnPGjBkOv0+fPh29Xm8fi6tjCgsL48ILL6zvpjr44osvOPfccwkMDESv12MwGFi8eLE9U1HVhAkTHMpjtNt2ySWXOJT71HWbNd9//z2KonDDDTdgMpnsPzExMQwePLjaN/4xMTH2MhzNoEGD6rwOzeHDh0lJSWHmzJkOJTeBgYH85S9/YcuWLbWWQoH1cejfv3+1BgTXX3+9w++bNm0iJyeHm266yeE2WSwWpkyZwvbt2ykuLq53vFWZTCaefvpp+vXrh7e3N3q9Hm9vb44ePVrjY9QQxcXFGAwGDAYDkZGR3Hnnnfz1r3/lqaeeqvUYrSPhhx9+iMlkYvHixUyfPp3AwECnr3P79u1cffXV+Pr62rcHBQVx+eWXO+zr6nOmPps3b6asrKza63Ds2LF07tzZYdsvv/xCv379qj0Hb775ZlRVrbEbWVVvvvkmw4YNs5cQGQwGfv755xofw2nTpmEwGFy6Lc6oqyTPmXK9xYsX4+fnx9/+9jfA+vq59tpr2bBhQ40d3mp7TH/77TfMZrPDvn/5y1+cvRkuy8nJ4dJLL0VVVT777LNqZXfu3C+qqjJ79mw2bNjABx98UGf1RGJiIlOnTiUhIcHeWKIuZrO52vuHJ9V2m5x5DixZsgSLxcKsWbPs22bNmkVxcTGfffZZtf1re98sKChg586dDtsb8zngzOPlzv3y7rvv8tRTTzFv3jyuuOKKGvf56quv2Lp1K1988QX9+vXjkksu8VgHW9GySbAl2rzIyEj8/f05efKkU/tnZ2cD1iDqbHFxcfbLNeHh4dX28/b2rrbd29sboMaWv2eX9Oj1eiIiIuzX5eqYatqvLl9//TXTp08nPj6eZcuWsXnzZrZv386sWbNqHG9tt82V26xJT09HVVWio6PtH/C1ny1btlQrkaxpDoaPj0+1zmI1qe9+tFgs5Obm1nn82Y8VVH/8tDK2a665ptpteu6551BV1eWlBh544AEee+wxrrzySlasWMHWrVvZvn07gwcPduq2u8LPz4/t27ezfft2VqxYwfjx4/nkk0949tln6zzulltuITMzk6effpqdO3e6VEKYm5uLxWJx+v515TlTH+154cx1Z2dn1/r8qXqumrz00kvceeedjBo1iq+++ootW7awfft2pkyZUuNj6Orr2BlV31eqKi4upqKiosb3s6qOHTvGb7/9xmWXXYaqquTl5ZGXl2fvVHn2vD2o/X6tqKio1mq8MW4zWJ9fkydPJjk5mTVr1lQrh42IiKCsrKzGL1tycnJqvF9UVeXWW29l2bJlLF26tNYP2WD9wmnChAno9Xp+/vnneu9ngIkTJzo8t6sGNg2hvYfW9Dyo7bZWZbFYWLp0KXFxcQwfPtz+HJg0aRIBAQE1lhLW9dpq6N8vZ9X3eLl7vyxZsoQ77riD22+/nRdeeKHW6+/fvz8jR47kmmuuYdWqVXTu3Jn77ruvAbdItBYyM0+0eTqdjokTJ/Ljjz9y5swZOnbsWOf+2htuampqtX1TUlKIjIz0+BjT0tKIj4+3/24ymcjOzraPxdUxuTqZfNmyZXTt2pXPPvvM4diqrW4bS2RkJIqisGHDhhqbl9S0zV1V78ezpaSk4OXlRVhYWJ3Hp6WlVdt+9jbt8Vi0aBGjR4+u8VzR0dFOjxusj9GNN95YrT13VlYWoaGhLp2rPl5eXvaGGACTJ09m+PDhLFiwgBkzZtT67X1CQgKTJk1iwYIF9O7dm7Fjxzp9nWFhYSiK4vT968nnjPa8qO26qzZ0iYiIqPX5o42tNsuWLWP8+PG88cYbDttra7Di7tpidRk4cCCffvopaWlpDh+AtUYrta2ZpXnvvfdQVZUvv/ySL7/8strl77//Pk8++SQ6nc6+rbb71dvbu1rmszFuc25uLpMmTeLkyZP8/PPPNc7J0eZq7d27l1GjRjmMMysrq9r9on1wX7JkCYsXL+aGG26o9fpPnTrF+PHjUVWVX3/9td6/QZq33nrL4bnhqb89VW9rv3797NtNJhOHDh3iuuuuq/P4tWvX2isJavrya8uWLRw4cMDh3HW9rs8+R2M8B5x5vLTHeO/evVx66aUOl+3du7fG18aSJUvsDULefPNNp8eu1+sZNmwYn3/+uRu3RrQ2ktkS7cKjjz6KqqrcdtttVFRUVLvcaDSyYsUKAHv5ndZwQbN9+3YOHjxY7/ol7vjoo48cfv/8888xmUz2RTs9NabaMkDa4rVV/1CkpaXV2I3Q07T1ZpKTkxkxYkS1n7MnrDtD+7B99m3t3bs38fHxfPzxxw5NO4qLi/nqq6/sHQprM2HCBPbv38+ff/7psP3jjz92+P3cc88lNDSUAwcO1HibRowYYc/6OUtRlGpBxA8//EBycrJL53GHj48P//vf/ygrK+PJJ5+sc9958+Zx+eWX89hjj7l0HQEBAYwcOZKvv/7aIRNaWFhof21qPP2cGT16NL6+vtVeh5s2bapWnjpx4kQOHDhQrfTpgw8+QFEUJkyYUOv11PQY7tmzx601pNx1xRVXoCgK77//vsP2pUuX4ufnx5QpU2o91mw28/7779O9e3fWrVtX7WfevHmkpqZW6+JY22N6/vnnOwRljUELtE6cOMHq1asZOnRojftNmTIFX1/fausQLl26FEVRuPLKK+3btL8lS5Ys4a233qrW1a+q06dPM378eHujprPLUuvSu3dvh+e1pxaDHzVqFLGxsdVu65dffklRUVG9a20tXrwYLy8vli9fXu058OGHHwLVM5y1vW8GBQW5tXaWK5x9vOLj4xk5ciTLli1zKG/dsmULhw8frna/LF26lFtvvZUbbriBd99916UgsaysjC1bttCjRw/3bpRoVSSzJdqFMWPG8MYbb3DXXXcxfPhw7rzzTvr374/RaGTXrl28/fbbDBgwgMsvv5zevXtz++23s2jRInvXrMTERB577DESEhK4//77PT6+r7/+Gr1ez+TJk9m/fz+PPfYYgwcPZvr06QAeG5P2rfZnn31Gt27d8PX1ZeDAgUydOpWvv/6au+66i2uuuYakpCSeeOIJYmNja5yD4Unnnnsut99+O7fccgt//PEHF1xwAQEBAaSmprJx40YGDhzInXfe6dI5u3fvjp+fHx999BF9+/YlMDCQuLg44uLieP7555kxYwZTp07ljjvuoLy8nBdeeIG8vLx6y+Tmzp3Le++9x2WXXcaTTz5JdHQ0H330EYcOHXLYLzAwkEWLFnHTTTeRk5PDNddcQ1RUFJmZmfz5559kZmZWy27UZ+rUqSxdupQ+ffowaNAgduzYwQsvvOD0t+QNNW7cOC699FKWLFnCI488QteuXWvc76KLLuKiiy5y6zqeeOIJpkyZwuTJk5k3bx5ms5nnnnuOgIAAh7JLTz9nwsLCePDBB3nyySe59dZbufbaa0lKSmL+/PnVyp/uv/9+PvjgAy677DL+85//0LlzZ3744Qdef/117rzzzjrb3E+dOpUnnniCxx9/nHHjxnH48GH+85//0LVrV0wmk+t3WBWnTp1i+/btABw/fhzAnnnq0qWLPVPZv39/Zs+ezeOPP45Op+Occ85h9erVvP322zz55JN1lpD9+OOPpKSk8Nxzz9m/CKpqwIABvPbaayxevJipU6fat+t0OiZPnswDDzyAxWLhueeeo6CggAULFjToNtentLSUiy++mF27dvHyyy9jMpnYsmWL/fIOHTrY28uHh4fz73//m8cee4zw8HAuuugitm/fzvz587n11lsdsjRz5sxh8eLFzJo1i4EDBzqc08fHxx7QZWRkMGHCBFJTU1m8eDEZGRlkZGTY9+3YsaNHX78lJSWsXLkSwD6m9evXk5WVRUBAAJdccglgfTyef/55Zs6cyR133MF1113H0aNHefjhh5k8eXKdAXd2djbffvstF198ca1lkwsXLuSDDz7gmWeesc85jIuLY9q0acyfP5/Y2FiWLVvGmjVreO655+r8gssTnH28wLoA8uTJk7n22mu56667yMjI4JFHHmHAgAEOQdoXX3zB7NmzGTJkCHfccQfbtm1zuM6hQ4fav1gZO3Ys06ZNo2/fvoSEhJCYmMgbb7zB8ePHnW6zL1q5pu3HIUTz2r17t3rTTTepnTp1Ur29vdWAgAB16NCh6v/93/+pGRkZ9v3MZrP63HPPqb169VINBoMaGRmp3nDDDWpSUpLD+caNG6f279+/2vXUtogjts5tGq2b0o4dO9TLL79cDQwMVIOCgtTrrruu2qKJDR2Tqlo7SF100UVqUFCQCjh0aXv22WfVLl26qD4+Pmrfvn3Vd955p8ZuT2ffBlWtvcOd1kHqiy++sG+rrTvce++9p44aNUoNCAhQ/fz81O7du6s33nijQ6fH2m5bTef85JNP1D59+qgGg6FaV7jly5ero0aNUn19fdWAgAB14sSJ6u+//17jfXa2AwcOqJMnT1Z9fX3V8PBwdfbs2eq3335bY+ev9evXq5dddpkaHh6uGgwGNT4+Xr3ssssc7o+a1NSNMDc3V509e7YaFRWl+vv7q+edd566YcMGl7uyubuosaqq6t69e1UvLy/1lltusW+r6flwNme7Eaqqqn733XfqoEGDVG9vb7VTp07qs88+W2vXMWeeM84uamyxWNRnnnlGTUhIUL29vdVBgwapK1asqPH+PXXqlHr99derERERqsFgUHv37q2+8MILDt3tVLV6N8Ly8nL1wQcfVOPj41VfX1912LBh6vLly6uN0Z3FgJcsWWLvdHb2z0033eSwb0VFhfr444/b3wd79erl0CW1NldeeaXq7e3t8F55tr/97W+qXq9X09LS7LfjueeeUxcsWKB27NhR9fb2VocOHar+9NNPDsdpj3FmZma1c7rbjVC7fmfvF1VV1VdeeUXt1auX/fn3+OOPV+sI2blz51rPWfVx1N7/avvx1ILXztzeml4DH3/8sf21FhMTo86ZM0ctLCys8zq0DrO1ddxTVVV98803VUD96quvVFWt/Hv45Zdfqv3791e9vb3VLl26qC+99JLDcTX9vTj7Mne6ETr7eGlWr16tjh492v4ef+ONN1b7e3zTTTfV+dhWfa+bN2+eOnjwYDUkJETV6/VqTEyMetVVVzn9N0e0foqq1rMAjhCi0cyfP58FCxaQmZnZKHPBhBBCCCFE85E5W0IIIYQQQgjRCCTYEkIIIYQQQohGIGWEQgghhBBCCNEIJLMlhBBCCCGEEI1Agi0hhBBCCCGEaAQSbAkhhBBCCCFEI5BFjZ1ksVhISUkhKCjIpVXChRBCCCGEEG2LqqoUFhYSFxeHl1ft+SsJtpyUkpJCQkJCcw9DCCGEEEII0UIkJSXRsWPHWi+XYMtJQUFBAJw8eZLNmzdz0UUXYTAYXDqH0Whk9erVTXpsc1ynHCuPbUs7tiHk8Wmbx7a28cqx8ti2lWNb23jb27GtbbzNeWxBQQEJCQn2GKE2Emw5SSsdDAoKwt/fn+DgYLce0KY+tjmuU46Vx7alHdsQ8vi0zWNb23jlWHls28qxrW287e3Y1jbe5jxWU9/0ImmQIYQQQgghhBCNQIItIYQQQgghhGgEEmwJIYQQQgghRCOQYEsIIYQQQgghGoEEW0IIIYQQQgjRCCTYEkIIIYQQQohGIMGWEEIIIYQQQjQCCbaEEEIIIYQQohFIsCWEEEIIIYQQjUCCLSGEEEIIIYRoBBJsCSGEEEIIIUQjkGBLCCGEEEIIIRqBvrkHIIQQQoiWw2xRsaiq/Xej2YLZYv3XYGjGgQkhRCskwZYQQgjRzlksKuuPZPLxttOsO5SByaKetYeeB7au5aJ+0bx944hmGaMQQrRGEmwJIYQQ7dyiX46xcO2RevdbfSCdwjIjQb6S4hJCCGdIsCWEEEK0c78dzQRg2uA47p7Qg5gQX/tlJqOR1WvW8PKhANILyzmaUcSwTmHNNVQhhGhVpEGGEEII0Y6ZLSoHUgoAuPfCHvSOCSLEz2D/CfYz4K+HHlGBABxNL2zO4QohRKsiwZYQQgjRjp3MKqLUaMbPoKNbh8Ba9+sZFQDAkfSiphqaEEK0ehJsCSGEEO3YvmRrVqtfXDA6L6XW/XraMltHJLMlhBBOk2BLCCGEaMf2JecDMCAuuM79etrLCCWzJYQQzpJgSwghhGjH9qVYg63+8SF17tfDVkaYVlBGfqmx0cclhBBtgQRbQgghRDtlsajst5URDoirO9gK8jUQa+tSeCxDSgmFEMIZEmwJIYQQ7VRSbgmF5Sa8dV70jK69OYamZ3QQAIfTpJRQCCGcIcGWEEII0U5pzTH6xAZh0NX/kaCXh5tk/Lg3lZfWHMFiUT1yPiGEaGlkUWMhhBCinbLP16qnhFDTy5bZOuqhMsLHv9tPRmE5QzuFMqF3lEfOKYQQLYlktoQQQoh2SutEOLCe5hgardTQE2ttWSwqWUXlAKw7lNHg8wkhREvUrMHW/PnzURTF4ScmJsZ+uaqqzJ8/n7i4OPz8/Bg/fjz79+93OEd5eTn33nsvkZGRBAQEMG3aNM6cOeOwT25uLjNnziQkJISQkBBmzpxJXl5eU9xEIYQQokVSVbWy7Xt83W3fNdqcrczC8gZ3JMwrNaJVD/5yKANVlVJCIUTb0+yZrf79+5Oammr/2bt3r/2y559/npdeeonXXnuN7du3ExMTw+TJkyksrCxfmDt3Lt988w2ffvopGzdupKioiKlTp2I2m+37XH/99ezevZtVq1axatUqdu/ezcyZM5v0dgohhBAtSUp+GbklRvReir08sD6BPnqCfKwzELSslLtyiiuPP5NbyrEMabohhGh7mn3Oll6vd8hmaVRV5eWXX+Zf//oXV199NQDvv/8+0dHRfPzxx9xxxx3k5+ezePFiPvzwQyZNmgTAsmXLSEhIYO3atVx88cUcPHiQVatWsWXLFkaNGgXAO++8w5gxYzh8+DC9e/duuhsrhBBCtBBaVqtndBC+Bp3Tx4UHelNYbiKnuILuHdy//pxix8zYz4cy7JkzIYRoK5o9s3X06FHi4uLo2rUrf/vb3zhx4gQAJ0+eJC0tjYsuusi+r4+PD+PGjWPTpk0A7NixA6PR6LBPXFwcAwYMsO+zefNmQkJC7IEWwOjRowkJCbHvU5Py8nIKCgocfoQQQoi2Yr9WQhjnXAmhJjzAG4DsoooGXX/VzBZYSwmFEKKtadbM1qhRo/jggw/o1asX6enpPPnkk4wdO5b9+/eTlpYGQHR0tMMx0dHRnDp1CoC0tDS8vb0JCwurto92fFpaGlFR1TscRUVF2fepyTPPPMOCBQsadPuEEEKIlmpfim0xYyebY2gibMFWTnHDgq1s2/F9YoI4lFbIjlO5LFxzBC9FAcBbByENmxYmhBDNrlmDrUsuucT+/4EDBzJmzBi6d+/O+++/z+jRowFQbG+6GlVVq20729n71LR/fed59NFHeeCBB+y/FxQUkJCQUPcNEkIIIVoJV5tjaMLtwVYD52zZMmODO4aiqnA4vZBXfj7qsM/4WC+ubdC1iIYwmS0UV5gJ8TM091CEaLWavYywqoCAAAYOHMjRo0ft87jOzj5lZGTYs10xMTFUVFSQm5tb5z7p6enVriszM7Na1qwqHx8fgoODHX6EEEKItiCjoIyMwnK8FOgb62qw5QNUZqbcpR0fHujN89cM4sYxnZkxqhMzRnXiwj7WipTjBXV/uSoa19+X7WTkU2tJyy9r7qEI0Wq1qGCrvLycgwcPEhsbS9euXYmJiWHNmjX2yysqKli/fj1jx44FYPjw4RgMBod9UlNT2bdvn32fMWPGkJ+fz7Zt2+z7bN26lfz8fPs+QgghRHuy31ZC2L1DIP7erhW5eKqMMLekwn6+wQmh/OeKATx11UCeumogT1w5AIDkEiitMNd1GtGItp7Mptxk4VCazFsXwl3NWkb44IMPcvnll9OpUycyMjJ48sknKSgo4KabbkJRFObOncvTTz9Nz5496dmzJ08//TT+/v5cf/31AISEhDB79mzmzZtHREQE4eHhPPjggwwcONDenbBv375MmTKF2267jbfeeguA22+/nalTp0onQiGEEO1SZQmha/O1oGoZYUMbZFiPD/P3rnZZXIgv0UE+pBeWszcln3N7+jbouoTr8kuNFJaZgMrAWAjhumYNts6cOcN1111HVlYWHTp0YPTo0WzZsoXOnTsD8PDDD1NaWspdd91Fbm4uo0aNYvXq1QQFVbaGXbhwIXq9nunTp1NaWsrEiRNZunQpOl1lG9uPPvqIOXPm2LsWTps2jddee61pb6wQQgjRQuxLsQZb/V3sRAjWsj9oeDdC7XjtfFUpisKQhBB+OpDBrtP5nNuz9rJ/0TiSc0vt/88tlk4lQrirWYOtTz/9tM7LFUVh/vz5zJ8/v9Z9fH19WbRoEYsWLap1n/DwcJYtW+buMIUQQog25VBaIQD93Ai2PFVGqB2vne9sQzuF8tOBDHYn5TXoeoR7zuSW2P8vmS0h3Nei5mwJIYQQom4/7kvjPysOUG5yby6Tqqqk2hoeJIT5u3y8VvaXU1yBqqpuj0ELtsJrC7YSQgHYlZTv9vUI9yXnVWa2GhpYi7pZLPL8bssk2BJCCCFakad+PMx7v5/k290pbh2fV2KkwmQBICrYx+XjI2xlfxVmC0XlJrfGUFxhpsJsHUNEQM1j6B8bhE5RyS6uICmntMZ9ROM5U7WMUDJbjea9jScZtGC1ZHDbMAm2hBBCiFai3AzpBdb1rT7bnuTWOdILrVmt8ABvfPS6evauzt9bj6/B+vHB3YyHtsaWr8ELP++ax+Bj0NExwPr/nadza9xHNJ6qc7Yks9V4fjmUQVG5iZ8PVl+mSLQNEmwJIYQQrURmleWOdpzK5VhGocvn0NZMig52v8NfRAPX2sq2LYhcW1ZL0yXIWl4lwVbTO5NXZc6WNMhoNFlF1tfCkXTXX8uidZBgSwghhGglMsscF/n9/I8zLp8jw5YZi3ajhFBjb//uZkfC+uZraboEWoOtvbZW9aLpOGS2pIyw0WQWasFWUTOPRDQWCbaEEEKIViLT9vk3KsgaKH2984x9/pWz0gpsma0g9zNbDV1rK9vJYCvO3xpsHU4rlCYCTai43ERuSWU2K7cBzVBE7Uxmiz2QPZVdTJmx9qY3qqoy/7v9PPn9AXksWhkJtoQQQohWQsts/W1kJzoE+ZBVVMHvx7JcOke6FmyFNKSM0LbWlpvBVm49bd81HfzAR+9FSYWZ0zklde4rPEfrROhvm09nsqgUutkMRdTO2tHT+n+LCscyas9uHUkvYummRN7deFKyYK2MBFtCCCFEK6EFWz2iAjm/RyQAB1ILXDqHPdjyQBmhu13qtIxYWD3Blk6BXtGBABx08XYK92lrbHWNDMDPYA248mTelsdl2uZraeqat7XpeOWXKj/tT2u0MQnPk2BLCCGEaCW0BhldIwLo1sHaqu9EZrFL59C6GcY0oEFGuK39e7abc7acLSME6BMTBEiw1ZS0+VrxoX6VJaMyb8vjss56/dSVsdp0PNv+fwm2WhcJtoQQQohWoLDMRJHRmtnqEulPtw7WjM/xTNdKiuxzthrUjVCbs1Vez541y3GyjBAqg60DqdKtraloa2zFh/kRFmAAKks/hedozTE0tWW2zBaVLScqg639KQUkSVltqyHBlhBCCNEKnMq2friKCPAmyNdQJbNV5PSEeZPZYm813ZBgK9zWsr2xG2QA9JYywiZ3xjZnq2OYP2H+DWuGImqnvRbjQ/2A2oOtg6mFFJaZCPLRc06XMABWH5B1uVoLCbaEEEKIViAx21ou2CXC3/ZvAIoCBWUmpxtVZBZZJ+TrvRSnskq1CfdQgwxXygiT80rJL5V5Q03hTA1lhO7OzxO10zJbY7tHANb7vaiGRiSbT1qzWqO6hXPJgFhASglbEwm2hBBCiFYg0ZbZ6mwLtnwNOvs34s7O28qwfbiLCvLBy0upZ+/aRTSw9buz62wBhPgZ7LfzkGS3moQ2Z6tjmJ9kthqRltnqFR1EB9tyDkdryG5tOZEDwJjukVzUPxqA7Yk5UtrZSuibewBCCCGEqJ9WRqhltgC6dQjkTG4pJzKLGNk1vN5zaJ0IoxpQQgiVDTJKKsyUGc342jrWOaPcZLZ/e+9MsAXQNzaI5LxSDqUVMqpbhOsDFk6rMFWWmsaFVgZbktnyPO1+7hDkQ6/oQDILy3lpzRG6RlpLhC0WC6cSvfgjJxewZsA6hvkTH+pHcl4pJ7KKGd6ADLVoGhJsCSGEEC1QhcniUFJ0IsuxjBCgW2QAvx3JtF9WHy2z1ZBOhABBPnoMOgWjWSW7uMKeeXKGVgqoKBDsa3DqmL6xwaw9mCHztpqA1o7cW+dFmL+BcFuDDMlseZ5WRhgZ6MOA+BB+P5bNhqNZbDhade08L8BCZKAPvaOtJbWxIb4k55WSml8KhDX5uIVrJNgSQgghWpj0gjKmvPwbuSXV5yh1rhJsda/SJMO582rNMdxfYwtAURTCA7xJLygnp8i1YKvAFmwF+eidLmXsGxsMuL6mmHCdlv3sEOSDoij2tdByZZ0tj9Nav0cGefP3C7oT6udNaUXlFyxmi4VjR4/Ro2cPJvWLsb9e4kL94FQuqXllzTJu4RoJtoQQQogWZtW+tBoDrY4BKj1sLd8Be/t3Z+dspWtzthqY2QII87cGW66Wl2mZrRB/57JaAAPjQwBrR8JykxkfvfNli8I1GQWVpW0A4f6yzlZjMJot9mxhh0AfwgK8uXN8d8d9jEZWlh/h0ok9MBgqXy+xodbXb0p+adMNWLhNgi0hhBCihfnlUAYA/5jShzsu6AZYP3j9+OOPeOsre1tp7d9P55RgNFsw6Orue5XhgQWNNaG2YMntYMvP+WCrY5i1K15OcQUHUwsZkhDq0nUK52UWauuwWYMtLbOVJ8GWR2mBls5Lsc+Lc1ZciDWTLJmt1kG6EQohhBAtSGmFmc22BUwn9o3Cy0ux/yhnVd3FBPvi763DZFHtDTTqku6BBY012gfEvBoycHVxJ9hSFIVBHa3ZrT1n8ly6PuGayo6V1udIZet3IxaLc+u5ifpp87UiArxd7gwaGyKZrdZEgi0hhBCiBdl0PIsKk4X4UD96RgXWua+iKPbOZc7M27I3yAhp2JwtgFA3u9Tll7gebAEM7hgKwO6kPJeOE66xd6y0lRFqGUyzRaWwrPoaUMI9WiOSyEDXX4txtjmSKZLZahUk2BJCCCFaEK2E8MI+UShnp7JqoM3bOllPR8IKs3UBZPDUnC3rh3BXM1vaGFwNtrTSwT8l2GpU9syWrYzQR68j0Mc660TmbXlOVqHj3DhXaJmtrKJyyk1mj45LeJ4EW0IIIUQLoaoqvx7OBGBCnw5OHdMxzDZ/I7/ub7nzbZ+T/b11BPk0fMq2u+svaWWEwS4GW1oZ4fHMYgrKpDNeY9Hm9VUNyMOk/bvHNSSzFR7gjY9t7mZ6frlHxyU8TxpkCCGEEC3EkfQikvNK8dF7MaZbpFPHxNm+5U6tZ/6GFmxFB/s6lTGrT2WDjMafswUQEehDxzA/zuSWsvdMPuf2cO7+Ea6pnLNVGQSE+XuTlFPKfZ/usme5VFWloFDHW4mbmTOxF1MGxDTLeFurrMLKtu+uUhSF2BBfErNLSMkvpVOV5SBEyyPBlhBCCNFCaCVywzuH4eftXHvzGFtnsrT6MltGa4DV0DW2NJUNMhq/G6FmcEIoZ3JL2Z2Ux7k9IjGaLSz5/SRbT+TY97GoFoqyvRhVVE5MmOvX0Z6ZzBayix0bZAD0iQliz5l8zuSeHdArpJYU8t7GkxJsuUjLbHVwI7MFEBviR2J2Sb1fsojmJ8GWEEII0UJo3cU6u/BNdWVnMufKCD3RiRAqS8uaovW7ZkjHUH7Yk8rGo1n0iw1m4doj7DmTX8OeXny5M4V7JvZy+Tras6yiClTV2o48IqAy4/LElQO4elhHTObKboQms4nvft3G14k60gulUYOrGjJnC6qstSVNMlo8CbaEEEKIFiIlzxpsxdqyVc6IqTJZvsJkcViHq6r8CmtmyxNrbEFlN8K8YhcbZDQg2NLmbW0+kW1vjx/sq+fuCT3smbaNRzP5bk8qe5NrCsJEXTJsQVOHQB+HduQ+eh2ju0U47Gs0Gjn+p8rXidasqqqqHilPbS+yGjBnCyA+VJurKZmtlk6CLSGEEKKF0JpcaK2dnRHu7423zosKs4WMwjI6htWcFdMyW57oRAiVZYSF5SanFlS2j6MBwdbwzmFcNiiWI2mFAPSJDebfl/V1yNbFBBv4bk8q+1MLXT5/e5de4NiJsD4htuRXuclCfqnRHoCL+mnNRiIC3bvPYmVh41ZDgi0hhBCihdAyW1rTC2d4eSlEh/iQlFNKWn5dwZZn52yF+BlQFFBVa/t3Z8uhGhJs6XVe/O/6YXXu0y82GIAzuaXklVRIAOACLbMV5eRjafCCUD8DeaVG0gvK5b52ktmi2tvoRwQ0sIywnvLhlqTCZGHBiv2M6BLGVUM7Nvdwmoy0fhdCCCFaAFVV7ZmtWBcyW1DlW+46PnhpmS1PlRHqvBSCfbW1tpybt2U0WyipsK4L5E6w5YwQPwMRPta5RftTChrlOtoqre17hyDnnyNa8J5W0Ho+9De33BLr3DhFqVyvzlVxIdrCxq2njPCXQ+l8tPU0C1YcQFXV+g9oIyTYEkIIIVqAglKTPRCJdSGzVXX/2joSqqrq8QYZUPlB0dn271pWCyDIt/E6BSYEWj/Iybwt12ht313JfmpZsHQJtpyWXWR9MYb5e6N3svz2bFpmK7/USEmFyWNja0y/H7POs8wrMbaqjFxDSbAlhBBCtADJtm+owwO88TU41/ZdE2PvSFjzt9x5pUZMqrWM0Nn5OM4IdXFhYy3YCvLVo/NqvGYKHQMk2HJHRoFWRuhKZsu6b3o7+vDcUFp7/fAA98sug30N9jXPkqu15K9bbnEFtyzZxlWv/06FyeL2GFy16XiW/f/729FrU+ZsCSGEEC2A1lUsLtT1zFNscN2ZLa08LMzfgI/etUCuLlpmy9kywobM13JFQoD13/b0gc4TalrQuD72zJa0f3ealtmKaECwBdAxzI9DaYXcsHgrt53fzSFrrUOl3Fz9mFPZxdy8ZDsns4oBOJpRSP+4kAaNwxlp+WUczyy2/74/pYCL+rePtdkk2BJCCCFaAK2sxpW275qYeuZs2cvD3FzTpzZh9syWa2WEwY1YQgiQYMtsJWaXUFBmbPTrayvsDTJcyH7a52zllzfKmNqi7Aa2fdc8emlf/vn1XpLzSnnyh4PVLr8gxourqvxeZjRz3dtbHEr40gvKmiTYqprVAjiQ2n7mU0oZoRBCCNECpLrRiVBT35ytNBdbejsrLMC1MsKGrLHligADxNsyhPuT28+HuoYwW1SybBkXV+b1aQF8hmS2nJbdwLbvmnG9OvDLg+OYf3k/zu8ZyZhuEYzpFkH/OGtHzuMFjqW6e87kk5JfRniAN0M7hQJ1N9XxpE3HrfO1tOs90I6a10iwJYQQQrQA7nYihMpgK6OwDJO5+hyMysYHnmuOAVXKCJ1c2LipyggB+wfOfVJK6JT0gjLMFhUvxbXytuh6SlhFdVqw1ZA5WxofvY6bz+3Kh7NH8cnto/nk9tG8c+MIAFJLodxYWUv4x6kcAEZ1DbcvkdAUc+1UVWXTMWtm69bzugHWOaq5xc59SdPaSbAlhBBCtABagwxXOxGCtRxJ76VgUSGzqHo5V7q98YFnM1suN8goabpga1C8tTRqW2JOo19XVV/tTObuj3dSWlHDhJkWbLvtfuoXF+xShzwtW5pVVF5joC+q08oIIxpYRlib2BBfwvwNWFSFIxlF9u1/JOYCMKJLuH0JiKbIbCVml5CSX4a3zosL+0TRKdy6FmB7KSWUYEsIIYRoAbQGGfFuZLa8vBR7hqGmD0/utPR2hjZnK8/FOVshbq4t5Iox3cIB2HIiG7Oladb0KTPBf344xA97Uvn5UHqTXKenbDlhLfMa0y3CpeMiArzR2QJ9rQxR1E1rkBHpgcxWTRRFsWeu9qcUAmCxqOw4ZQu2OofZO5g2xfpoe87kATCoYwh+3jp71nl/SvvIOkuwJYQQQjQzi0W1l2G5U0YIlRmx1LzqH57SC1zvMueMynW2WlY3QrCWEQb56iksM3nkQ11phZnc4gqHtcLOtiNbsa+Vdii1sMHX2ZS2nLBmtka7GGzpvBQ6BMrCxq6onLPVOJktgP5xQUBl9uhYZhH5pUb8DDr6xQXbG/E0Rflnpu3LHu29rTLYah+ZLelGKIQQQjSzrOJyjGbrfBl3OwZq31Sn1rDWVmU3Qs/O2Qp1txthEwRbOi+F0d0iWHMgnd+PZTOoY6jb53p/UyJPrzxIuW1NoquGxvPitYPxOmutsE3pld9hH0prPR8k0wrKOJlVjJcC53QNd/n46BBf0grKZGFjJ2UVNXydrfr01zJbtmBLKyEc2ikUg86LmBCti2TjP2Y5xY6t7vvZgq3fjmRyz8c7AVAtKimpXqwu3EN4oA//uKSPfR2x1q5t3AohhBCiFUvJq1xM1pX5MlVpma33NydSWGayr9elqpUf7jxeRhhQuc6WqqooSt0LFTdlZgtgbHdrsLXpeBZ3ju/u8vGqqvLqz8dYuPaIw/ZvdiUTF+rLQxf3sW/bl1zAmeLK23+wFWW2tp60fhAfEB/iVpt87QsCCbZqV2Y0Y1K90HkpFJaZAIhsYDfCuvSzZbYOpRVhNFv4wzYnb0TnMKByuYjCchNF5aZGDWy0YEsrOx4YH4reSyG3xMj3e1Kr7OnFruw0wPpedc+FPRttTE1Jgi0hhBCimWlt32PdWNBYc37PDiz5PZGknFJe+flotct1iurxb9K1D08mi0pRuYmgej6oN32wFQlYmz+UGc1sT8yhd3QQUU50ZSwuN/F/3+7nq51nAJg7qSf3TOjBd3+m8MDnf/K/dccpqTATbrsPfj+WCcC4npGsP5pFcl4p+aXGJrutDbH1pHslhBotqyrBVs3MFrj8f5vxUhTenzUSAL2X0qjrv3UK88dHp1JusnA8s4g/TlU2xwAI9NET6KOnqNxEWn4ZPaICG20sWrAVbgsuOwT58NGtozhYpUGG2WLhwP79+ER15uNtZ/hkWxJ3ju+BzqvuL3BaAwm2hBBCiGamLTIa5+Z8LYALenVg6z8n8vOhDH47kunQDc+iWog2pnv8g4uvQYevwYsyo4W8EmO9wVZTrbOl6RUdSGSgN1lFFVz9+iYOpBYwplsEn9w+us7jTmWXcPtHuziRaS2t+7+p/bj53K4AXD2sI4fTCnnrtxMs+T2x2rG3nteFY5nFJOeVcjitkJFulOU1NS3YcrU5hqay/bssbFyT1FJrRz6ADUetLdDDA7yrlaF6kpeXQkd/OF4ISzYmcjqnBC+lcp0rsAbJxzKKSC9ommCr6pICo7pFMKrK881oNLIyZx8XTu7ND3vTSc4r5bcjmUzoE9Vo42oqLSbYeuaZZ/jnP//Jfffdx8svvwxAUVERjzzyCMuXLyc7O5suXbowZ84c7rzzTvtx5eXlPPjgg3zyySeUlpYyceJEXn/9dTp27GjfJzc3lzlz5vDdd98BMG3aNBYtWkRoaGhT3kQhhBCiRg1Z0LiqiEAfpo9IYPqIBIftRqORlStXNujctQnz9yY1v4zckgoSbC2da9PUmS1FURjTPZIVf6bYGwXsOJVLucmMj15X63H/XXOUE5nFRAf78MrfhlbL+Dw8pQ+xIb4cTq8sFbRYLFRknWZU1zD6xASRnFfKobSCZg+2PticyPOrDmOy1NyW3WzWYbSUovNSGNElzK3r0IItWdi4ZklFlUHV6gPWMrnGnK+l6RiocrxQ4bM/kgCY0DvK4QuRmGBrsNXY7d9zXFhXzNeg45rhHVm88SQfbT0lwZanbN++nbfffptBgwY5bL///vtZt24dy5Yto0uXLqxevZq77rqLuLg4rrjiCgDmzp3LihUr+PTTT4mIiGDevHlMnTqVHTt2oNNZ30ivv/56zpw5w6pVqwC4/fbbmTlzJitWrGjaGyqEEELUwL6gcYj7ma3mEmoLtmYt/QNfw1nzzVSVklIdLxzawPk9O1Bsy7Y1ZWndpL5RrPgzhS4R/uSWGMkvNbI/pYBhnWoPLLSFkBf+dUiNpXU6L8We6dJYA9pTKIpCn9ggfj6U4VAm1Vw+2ZZEUbmpjj2sgcDZH8Rd0ZRrNrVGSVXm8m06Zm2xH9mInQg1CQGVSx5cNzKB/5va3+Hypir/zK4hs1WX60Z2YvHGk/xyKIOUvNIGZfxbgmYPtoqKipgxYwbvvPMOTz75pMNlmzdv5qabbmL8+PGANUh66623+OOPP7jiiivIz89n8eLFfPjhh0yaNAmAZcuWkZCQwNq1a7n44os5ePAgq1atYsuWLYwaNQqAd955hzFjxnD48GF69+7dpLdXCCGEOJu2oHFcA+ZsNZcBccEcTC2wN+GoTiGnvJRPtp22bwn2bbqPH9MGx9Ep3J/eMUHM+WQXaw9msPNUbq3BVqkJztgalmhrFbmqr+24mppkrNqXyvM/HbZ/2w+ACl38vLjkEs+uB1ZhsnAswzqGr+4cQ9RZ3ShNJhPr1q3jwgsn0CkiyO3r6Rhm/TCclFOCxaI2anlca1S1cUqFbeHniEZsjqEZHK6iH9OJ0d0jmTIgttrlsXV0MPUUk9liz2iHORls9YgKZHS3cLacyOGbXcncPaFHo42vKTR7sHX33Xdz2WWXMWnSpGrB1nnnncd3333HrFmziIuL49dff+XIkSO88sorAOzYsQOj0chFF11kPyYuLo4BAwawadMmLr74YjZv3kxISIg90AIYPXo0ISEhbNq0qdZgq7y8nPLyyj8cBQXN/+2UEEKItkn7sNMaM1vPXD2Qm8Z2wVTDwsEmk4lNv/9O7yHn8NKaYxxOLyQy0NvtjovuUBSFobbAaminMNYezGDX6bxa90+1Tq0hJtjX3treVX1irMHW4bRCe/BRWmHmka/38O3ulBqP2V3qxc7TeYzu4bmyqRNZRRjNKkE+eoZ1CqvWLdJoNBLha11IuyEBUnyYHzovhXKThYzCcnvGRIDRbCG5uPr2iIDGz2x56+Bfl/bBYKg5Y9kUc+20ZSEUpbKhjjOmDopjy4kc1h3KkGCrIT799FN27tzJ9u3ba7z81Vdf5bbbbqNjx47o9Xq8vLx49913Oe+88wBIS0vD29ubsDDHb6eio6NJS0uz7xMVVf2NKyoqyr5PTZ555hkWLFjg7k0TQgghnGI0W+zrYLXGchm9zosB8SE1XmY0GkkOggt7d+DCvjF8syvZngVpDlpzgF2nc2vdJ6XEGnT0iXU/09Mlwh8fvRelRjOnc0roEhnAG78e49vdKXgp8Pdx3bl6WDxaCd/C1Yf5YV8aX+1K8WiwpZUx9okNqrctf0MYdF50DPPjVHYJidnFEmxVcTyzGJOqEOijp2tkAHttJapNkdmqj5bZSitovMyWlsEN9TO41KBnfO8OAOw8nUt+iZEQ/5bf1bM2TffV0lmSkpK47777WLZsGb6+Nb8oX331VbZs2cJ3333Hjh07ePHFF7nrrrtYu3Ztnec+e62Pmt5g6lsP5NFHHyU/P9/+k5SU5OQtE0IIIZyXXlCGqoK3zsvpOQ2tkUHnxfQRCfZ27M1hcMdQvBRr98faFnPVgq3eMe4HW3qdF72ircfvTsqjzGjmo63WMsoXpw/m4Sl96BEVRI+oQHpEBTJjlLWhycq9aZRU1DW/yjVaGWNfN8shXdHJ1hzlVHYNaZx2bF+KNeDtFxvk0ICkJbzWKzNbjTdnK7vYvQWcO4b50zMqEIsKG2zLKrRWzRZs7dixg4yMDIYPH45er0ev17N+/XpeffVV9Ho9xcXF/POf/+Sll17i8ssvZ9CgQdxzzz389a9/5b///S8AMTExVFRUkJvr+A1VRkYG0dHR9n3S09OrXX9mZqZ9n5r4+PgQHBzs8COEEEJ4mtZUICbEV+a6NLIAHz29bSV+tWW3tGCrb0zD/u6f39MaVL689ghf7jhDdnEFcSG+XD4ortq+IzqHEumrUlxhZuXe2qtuXGXPbDXwtjijS0QAYG2bLyodsAVbA+KCGdG5sjNlRBM0yKiPltnKKqqgwlRzt8qGyi22lhG6031Ry26tOyTBllsmTpzI3r172b17t/1nxIgRzJgxg927d2M2mzEajXh5OQ5Rp9NhsbUvHT58OAaDgTVr1tgvT01NZd++fYwdOxaAMWPGkJ+fz7Zt2+z7bN26lfz8fPs+QgghRHNJ0RY0ltKrJjHMVkq4s4ZgS1VV+5ythpQRAtw5vjtRQT4kZpewYMV+AGaO6VLjfDVFURjVwfrZ5os/PFdJU5nZathtcUbnCC2zJcFWVVpmq39cMMM7V8lstYAywvAAb7xtz8c1B9LZciKbrSdzSCqyvhY8IcfNzBZYO2QCrD+SiaWGOaGtRbPN2QoKCmLAgAEO2wICAoiIiLBvHzduHA899BB+fn507tyZ9evX88EHH/DSSy8BEBISwuzZs5k3bx4RERGEh4fz4IMPMnDgQHt3wr59+zJlyhRuu+023nrrLcDa1XDq1KnSiVAIIUSzS8lr+ILGwnlDO4Xx0dbT7DhVPdhKyS+jzKxg0Cl0i2zYIq9Bvgb+PbUfcz7ZhdGs4mvw4rqRCbXuP7KDyo9nrAsM/5GYw4guDVufK7OwnKyichSlYSWRzupsy2wltvEyQrNFrXXukdFsYeXeVPvi3QAH06wBb/+4YGJCfOkfF8yJzGK6RQY0yXjroigKMSG+nM4p4e6Pd1a5RE9ot2RuGNO11mOdlW1fY8v1TN6ILuEEeOvIKirnQGpBrXNDW7pm70ZYl08//ZRHH32UGTNmkJOTQ+fOnXnqqaf4+9//bt9n4cKF6PV6pk+fbl/UeOnSpfY1tgA++ugj5syZY+9aOG3aNF577bUmvz1CCCHE2bROhK2x7XtrNKprOIoCO0/nse5whv3bc4BDtg/G3SID8NY3vPjn8kGxfLb9NL8fy+aqofF1djcM9YGrh8bz5c5kHvpyDyvnnI+fd+0LL9fnUJo1o9IlIgB/78b/uNfFltk6nV1S77z41urhr/byze5UdF4KcaG+/OeKAQ7Pn292JvPwV3uqHeejU+lqu38+v2MMJRVmtztdetod47rx/qZEtMRRfkkFmUUVbDuZ65FgK8fFNbaq8tZ7MbZHJGsOpLPuUIYEW57w66+/OvweExPDkiVL6jzG19eXRYsWsWjRolr3CQ8PZ9myZZ4YohBCCOFRWmarNbZ9b40Swv25ZWxX3vv9JP/4cg+r77/A/sH3SHoRAL2jPZMJUhSFRdcN4+udZ5h+Tu1ZLc2jU3qx8Vg2J7OKef6nQzx+ef96j6mNNl+rKUoIwXq/KgoUlpvIKa5oEXOSPO2nAxmANbuVlFPKLUu2c/eE7syb3BsvL4VNx7MA6/psXSKtwZXFohJdkWKfjxngoyfAp+V8/J4xqjMzRnW2/75qbzJ//2g3RzKK3D6nyWxhb2ouQzqG2oMtZ9fYOtvkvtGsOZDON7uTuefCHq0yiG85j7YQQgjRDklmq+k9PKU3vx7J4ERmMTe+t81e0qWtv9U7pmElhFWFB3hz6/ndnNo32M/Ac9cM4qb3trHk90T6xQZz7Yj6g7SqNhzN5I/EXDYes37wb2ijD2f5GnTEBPuSml9GYnZJmwu2TBYoqTADsPr+C/hw8yk+3HKK/607TpeIAK4dkcAO2zzARy/tw/k9rc0djEYjK1cmN9u4XdUr2vrcP55ZhNFsweDGmnj/XXOUxb+f4qmrBjQoswVwycAYHv9uPycyi9l5OpfhnRtWXtscmq1BhhBCCCGqNsiQzFZT8TXoePHawXgpsOdMPst3p7B8dwqncqzNHQZ3bL5ypXG9OjDrXGv51kNf7mHxxpNOt4PPLzFy6/t/8MrPR+1z0vo0Qdt3jdYk43RO25u3VWx7CHReCj06BPLElQO4c3x3AH7an0ZGQRlJOaUoCgxJCG2+gTZQfIgfPl4qRrPqVht/kwW+3GkNLn8+mGEPttxpkAHWuY+XDYoF4LPtrXMZJslsCSGEEM2ktMJMbol1Mn2cBFtNaminMD66dTT7U/Lt28xmM6nHDzKyynpIzeGxqX1RFFi88SRPfH+AJ74/QEywLwv/OoQx3SNqPW7FnhTKTRaig33oHxdCiJ+BC3o13bpmXSIC2HIih8SstteRsMjW8yLUz2AvCbx8UBxv/Hqc349ls/lENmAtQQ3ybcUL8HopxPjDqSLrHMYeUa6Voe7LVcgvtUam2xNz8LHNfXQ32AKYPiKBL3ec4fs9qTx+ef8WVYbpjNY1WiGEEKIN0UoIA7x1BPvJn+SmNqZ7hEPwYjQaWZl/oNnnhSiKwr8v60tEoDdv/3aCvBIjaQVlLFixn5Vzzq91Pbavdp4B4NbzunHbBc6VLnpSp4i2u7Bxicl6n4f6VwZSfWODiA72Ib2gnDd+PQ7AsM7NG6h7Qqy/yqkihSNphTDIus1sUflwcyJdOwQyrleHWo/dmlH53CwsM1Fo+39DWt2f0yWMbpEBnMgq5oc9qU7Nf2xJpIxQCCGEaCbagsaxoX7N/gFftCyKonDX+B7s/r+L2PrPiQT56jmUVsj3e1Nr3P9EZjG7Tueh81K4Ymj1hZObgn1h45w2mNmylRFWzdAoimLvRqh1shzeqW0EW1B5mwAWrjnC/BUHuOm9bSz6+WiN63BlFpZzKM/6PnZ2a/uwBnRfVBTFPnfx7Q0nMJobZwHmxiJfowkhhBDNJFkWNBZOiA725bbzu/HSmiO8vOYIlw6IqbY48vLdKQBc0DOSqKDmeT615YWNi7UywrOChgl9ovi0ylyi4W0is2X990i6NdhadziD19Yds1/+4poj/HI4g7gQP3wMXihYA6yknGIsKAxJCGFin2heXHMEsGbufQ3uL2MAcP3ITryz4QTHMop4b+NJ7hjXvUHna0oSbAkhhBDNJFVb0Fjma4l6zDqvK0t+P8mJrGIuWvgbPrYPr6qqUligI9d0GoCrh3VstjFqCxvnFFdQUGYkuBXPXTpbiS2zFebveJvO7RGJQadgNKtEBHjbA87WLM6W2TqVU8KxjCLu/2w3ADNHd6ZXdCCPf7efXafz2EVejcf/ZWg8Pat0wQxvQAmhJsTfwCOX9OHhL/fwys9HmTYkrtU0FZJgSwghhGgmaQXWYCtGMluiHoE+eu69sCf/+f4AJ7LOnhOlAGYiA32Y3C+6OYYHWMcYGehNVlEFp7NLWu0itDUpss3ZOnu9qEAfPSO7hvP7sWyGdQ5rE+XAQQYIDzCQU2xk5uKt5JUYGdQxhH9P7YuPXseobhHsPZNPYZmRclNlSZ/ZYibp2CGuHR4PXjq8dV5UmC2EB3hmGYBrhnXks+1J7DiVy7gXfsVQZe5ivJ8XF09RaYnhvQRbQgghRDPJtbVFjvTAN7+i7bvl3C4MTgihuNxs32Yym9i+bTvnjDyHAfFhDS7XaqjOEQFkFVWQmF3cpoItW9PQGuce3TSmCztP5XHt8ObLKnpa7+ggNp/IITW/jGBfPf+7fhg+eutzq1d0EL1qWPjbaDSysuAgOi8Fg0HHoI4h/HEql3B/z4RAXl4KT145gKte/50yo4WKKpcdqfDiRFYx/eJb3nupBFtCCCFEM8ktsX5cOHseiBA1URSl2qKuRqORwiMq5/eIxGBo/u/1O4f7s+NUbpubt2VvkFHDa/Wi/jEcfGJKE4+ocfWMCmTziRwAXpo+hIRw18sjR3UL549TuR6dQ9g3Npit/5xEXkllqHXHB39wKL2IU9kl9ItveXPmJNgSQgghmokWbDWkU5cQLYk2b6uttX+vqfV7W3ZRvyg+33GGu8b3YJKbpam3nNuVMqOF60Z28ujYQvwMhPhVPg49ogI5lF7EyRb6nJNgSwghhGgm2oLG7eUDnGj7ukRaMyCJbSyzpXUjPHvOVls1qms4+xdMQVfLmm7OiAz04bGp/Tw4qpp1sTUlaamLabsUbB0+fJhPPvmEDRs2kJiYSElJCR06dGDo0KFcfPHF/OUvf8HHxzOT4IQQQoi2TFVVeylMeDv5ACfavraa2Sq2dyNsP6/VhgRaTamLbU2vlhrgO7Wo8a5du5g8eTKDBw/mt99+45xzzmHu3Lk88cQT3HDDDaiqyr/+9S/i4uJ47rnnKC8vb+xxCyGEEK1acYUZo9naYrk9fYATbVtn29ye9IJySivM9ezdOhjNFkrNtm6EkoVuceyZrRYabDmV2bryyit56KGH+OyzzwgPD691v82bN7Nw4UJefPFF/vnPf3pskEIIIURbo3Ui9NF74efdvB3khPCUUH8Dwb56CspMnM4poXdM9a51rU1+qdH+/6pzhUTLoAVbGYXlFJebCPBpWbOknBrN0aNH8fau/1u3MWPGMGbMGCoqKurdVwghhGjPpDmGaIsURaFLZAB7zuSTmF3cJoItbW5liJ8evc6pojDRhEL8DATqVYpMCiezWt6SA049Y+oLtPLy8lzaXwghhGjvtA9w7WXCvWg/OtlKCU+30LIuV+VpjWz85LXaUnXws/57stqC383P5fD8ueee47PPPrP/Pn36dCIiIoiPj+fPP//06OCEEEKItirPntmSsiTRtnSJ0BoWOP/Bt8xo5oWfDrHuUEZjDctt9ix0gLxWW6oOvtb5r4ltIdh66623SEhIAGDNmjWsWbOGH3/8kUsuuYSHHnrI4wMUQggh2qKcYikjFG1TZ9scGlcWNn5u1SH+t+449326i5IKU2MNzS2VmS0JtloqLdhqiZktl2eQpaam2oOt77//nunTp3PRRRfRpUsXRo0a5fEBCiGEEG2RrLEl2iqt/fvJrGK2nMgmKacy6DKbzezJUCjdmYyPt57R3SI4nlHMkt8TASgoM/HNrmRmjOrcHEOvkb3kV16rLVaUVkbYApcccDnYCgsLIykpiYSEBFatWsWTTz4JWNcLMZvbRotPIYQQorHJGluirdK6wyXnlfK3t7fUsIeOj4/vB0BRwFdv7cbZMcyPM7mlvL8pketHdkJRWsY6T9LMpuVrU5mtq6++muuvv56ePXuSnZ3NJZdcAsDu3bvp0aOHxwcohBBCtEWVmS35ACfalg5BPnSLDOBEVjFRQT70iQ1GZ4ubLKpKZkYGHaKiyC81sTspj1KjmW6RAXx6+2jG//dXjqQXsfl4NmN7RDbvDbHJK5UsdEsX6Wv9N6/ESG5xRYtqPORysLVw4UK6dOlCUlISzz//PIGBgYC1vPCuu+7y+ACFEEKItii3WBpkiLZJURS+u/c8cosr6Bjm55ChMhqNrFy5kksvHYbBYOBMbgm/HcliXO8ORAX7cs3wjnyw+RRLNiW2mGArt1iCrZbORwexIb6k5pdxMK2Asd1bxnMH3Ai2DAYDDz74YLXtc+fO9cR4hBBCiHZBSpNEWxbooyfQicVlO4b5c/2oTvbfbxjdmQ82n+LXwxkYzRYMLWBdKy2zJa/Vlm1klzC+/TOV345ktahgy61n8PHjx7n33nuZNGkSkydPZs6cOZw4ccLTYxNCCCHarDxZZ0uIanp0CMTPoMNoVh0aazQnyUK3DuN6WQOsXw+3rOUDXA62fvrpJ/r168e2bdsYNGgQAwYMYOvWrfTr1481a9Y0xhiFEEKINidX1tkSohovL4WukdZuhsczW0azg8rMlrxWW7LzekTgpcChtEJS80ubezh2LpcRPvLII9x///08++yz1bb/4x//YPLkyR4bnBBCCNEWlRvNlFRYO/hKgwwhHHWPCuRAagEnMouA6Ca97szCcq783++kVPmwrlob3clrtYUL8/dmSEIoO0/n8evhTK4b2an+g5qAy5mtgwcPMnv27GrbZ82axYEDBzwyKCGEEKIty7V9U67zUgj2dfl7TyHatG72zFZRk1/39sQckvNKUVXsPwCxfioRUvLb4o3vHQXAukMtp5TQ5Xf4Dh06sHv3bnr27Omwfffu3URFRXlsYEIIIURblVdlkdSWspaQEC1F9yhrp+sTzVBGmFlYDsDEPlE8+5dBAJhMRras/xmdl7xWW7oJvaN4ac0Rfj+WRYXJgre++RusuBxs3Xbbbdx+++2cOHGCsWPHoigKGzdu5LnnnmPevHmNMUYhhBCiTcmTNbaEqFX3Ds2X2dKCrY5hfnQI8gHAaPRC4qzWoX9cMJGBPmQVlfNHYk6LWD7A5WDrscceIygoiBdffJFHH30UgLi4OObPn8+cOXM8PkAhhBCirZHmGELUTmuQkVtiJKe4gvAmLN/LKCwDsAdaonXx8lK4oGckX+9KZuOxrBYRbLmcW1MUhfvvv58zZ86Qn59Pfn4+Z86c4b777pNSCCGEEMIJuSWybo8QtfH31hMf6gdga5LheVlF5Xy14wxlRrPDdi2zJcFW6zWmewQAm45nN/NIrBo0KzcoKMhT4xBCCCHajTwJtoSoU7cOASTnlXIis5gRXcI9fv77P9vNhqNZrDmQzuszhuFlqxPMLJJgq7XTsll7zuRRUGYk2NdaQZBRUMY/v9lrf/8FUFWVjorCpY04HpczW+np6cycOZO4uDj0ej06nc7hRwghhBB109btCQ2QMkIhatK9g7VJRmPM29p5OpcNR7MAWLU/jZfWHLFfZs9sBfp6/HpF04gP9aNLhD8WFbafzLFvf/3X46w9mMEfp3LtPztO57HilFe1DKcnuZzZuvnmmzl9+jSPPfYYsbGxUjoohBBC1OJwWiHHMio/LJrNJnZlKxw0FgCS2RKiNt06NN7Cxot+PgpAr+hAjqQX8dq6YwztFMqE3lFkFVnnU0pmq3Ub0z2SxOzT/H4sm4l9oykqN/HljjMAPHJJH7pEWJ9fj3y1h7xSI0czihjWpXECbJeDrY0bN7JhwwaGDBnSCMMRQggh2oYzuSVc/tpGKkyWsy7RAbkATTrxX4jWRMtseXrO1r7kAtYdzsRLgbdnjuDtDSf4eOtpftyXxpCEUMwWFUWBiEB5bbZm5/aI4JNtp9l03JrB/GrHGYrKTXTrEMDt53ezl41+sPkkm47ncCitkGFdGqeZhsvBVkJCAqq2wpsQQgghavTObyeoMFmICfalc4Q/YJ0fkJOTQ3h4OB2CfJncN7qZRylEy6Rltk5kFdP73z/atwfqdAw9t4xOke6V4C7+PRGAK4bE0yUygHO7R/Lx1tOczComw1ZCGO7vjUHX/OszCfeN7mZtknEorZCsonLe35QIwM1ju9gDLYC+MUFsOp7DwdTCRhuLy8HWyy+/zCOPPMJbb71Fly5dGmFIQojWZPX+NN7+7QQvTR9CJ9sHSiFaO1VVmff5nxh0Xjz7l4Eul8xnFZXz6fYkAF6cPphzbRO2jUYjK1eu5NJLz8FgkPlaQtQmJtiXvrHBHEwtoLxKdrjcpLD5RDadIt1r0nYozfqh+uph8QB0ibT+3TqZVSydCNuQyEAf+sQEcSitkCkvbyCrqJwgHz1XD+vosF/f2GAADqa1oGDrr3/9KyUlJXTv3h1/f/9qfyxycnJqOVII0RZ9uj2JP07l8uO+VO4Y1725hyOER6Tkl/H1rmQArh/VicEJoS4dv+T3k5SbLAzuGMJYWxtiIYTzFEVhxT3nkm4LgACe+n4/K/elk1FQXseRddO6DcYEW+fnaHN3coorOGqbXynBVttw2cBYe2YL4PrRnQj0cQx9+sVYg/aDaYVYLKpD1stT3MpsCSGEJs+2OGtqflkzj0QIz0kvqHw+f7Mr2aVgK6OwjA82nQLgrgk9pJGUEG7S67zs620BdA63ZqEybE0sXGW0QH6pCYCoIGuwFeCjJybYl7SCMnvnug6BEmy1Bfdc2INLBsaSX2oEVAZ1DK22T9dIf/SKSnG5maTcEjrbgm9PcjnYuummmzw+CCFE66W1sE7JK23mkQjhORlVgq0Vf6bwr8v6OjWHI7/EyI2Lt1FYbqJPTJDMyRLCg7SMU9XXpysKbcsreeu9CPar/AjcNTKAtIIytiXagq1gCbbaAkVR6BEVWOc+ep0Xsf6QVAwHUgpaRrBVVWlpKUaj0WFbcHBwgwYkhGhd8m2LA6bkS7Al2o60Kpna7OIKNhzN5MI+NQdOu07n8vqvx1FVOJ1TzJH0IjoE+fDmDcMbpSRFiPYqSgu2Ct0rIyywJcQ6BPo4ZJy7RAaw+UQ2OcUV9stF+9ExQCWpWOFAagGXDIz1+PldbrVSXFzMPffcQ1RUFIGBgYSFhTn8uOuZZ55BURTmzp3rsP3gwYNMmzaNkJAQgoKCGD16NKdPn7ZfXl5ezr333ktkZCQBAQFMmzaNM2fOOJwjNzeXmTNnEhISQkhICDNnziQvL8/tsQohrFRVrZLZkjJC0XZo80S0z2Nf7UymzGimzGim3GjGaIFyo5kvd5zhr29tYc2BdNYeTOdIehEhfgaWzR5Fl0jPf0MqRHumBVuZbgZb+RXWF3TUWZmrbme9VmXOVvsSH2Dtsn4gpaBRzu9yZuvhhx9m3bp1vP7669x4443873//Izk5mbfeeotnn33WrUFs376dt99+m0GDBjlsP378OOeddx6zZ89mwYIFhISEcPDgQXx9Kxcdmzt3LitWrODTTz8lIiKCefPmMXXqVHbs2IFOpwPg+uuv58yZM6xatQqA22+/nZkzZ7JixQq3xiuEsCoqN2G2WN+kcoorKK0w4+eta+ZRCdFw2pytSwbEsHJvGj/sSeWHPalV9tDz4Naf7b9N6hvNhX2iKCo3cnH/mEYpRRGivdOCpPTCclRVdXk+pFZGeHbmqqsEW+1avL8t2EptIcHWihUr+OCDDxg/fjyzZs3i/PPPp0ePHnTu3JmPPvqIGTNmuHS+oqIiZsyYwTvvvMOTTz7pcNm//vUvLr30Up5//nn7tm7dutn/n5+fz+LFi/nwww+ZNGkSAMuWLSMhIYG1a9dy8cUXc/DgQVatWsWWLVsYNWoUAO+88w5jxozh8OHD9O7du8ZxlZeXU15e+c1JQUHjPABCtGZ5JY5lxKn5pXTrUHd9tBCtgdbtbFLfaPJLjfx+LLvG/bx1Xvx9fHfmTuwpJYNCNLJIW5BkNKvklRgJc3FR8IJaMltdOzgGW1ESbLUrcQHWKobU/DKOZRTSI8q9ZQVq43IZYU5ODl27dgWs87O0Vu/nnXcev/32m8sDuPvuu7nsssvswZLGYrHwww8/0KtXLy6++GKioqIYNWoUy5cvt++zY8cOjEYjF110kX1bXFwcAwYMYNOmTQBs3ryZkJAQe6AFMHr0aEJCQuz71OSZZ56xlx2GhISQkJDg8m0Toq3LL3UMtqSUULQVabbMVkywL8tmj2LfgovtP7v+fSHPjTSx698XsnfBRTwwuZcEWkI0AR+9FwF6axbCnXlbBbY/WVonQk1CmD+6Kq/hDoGOl4u2zVcHE3t3AODVn495/PwuB1vdunUjMTERgH79+vH5558D1oxXaGioS+f69NNP2blzJ88880y1yzIyMigqKuLZZ59lypQprF69mquuuoqrr76a9evXA5CWloa3t3e1uWLR0dGkpaXZ94mKiqp2/qioKPs+NXn00UfJz8+3/yQlJbl024RoD3JLHNvvSkdC0VJZLCqJWcWcyCziRGYRJ7OKya+je7RWRhgV7IuiKAT66B1+fHUQ6KPHRy9ls0I0pWBbMivdjY6EWrB1dpmgt96LjmF+9v9X7VQo2od7L7SuE7piTwpH0z27wLHLz6ZbbrmFP//8k3HjxvHoo49y2WWXsWjRIkwmEy+99JLT50lKSuK+++5j9erVDnOwNBaLdbXwK664gvvvvx+AIUOGsGnTJt58803GjRtX67nPruOtqaa3vlpfHx8ffHwkjSxEXc4uI0yWYEu0UPd/vptvd6ectVWPf5cUrjmns8PWkgoThWXWtXiipQW0EC1KiEElFcW9YEsrI6yhTLBrZACnskuqdSoU7UO/2GAu7h/NT/vTeeXno7x2/TCPndvlzNb999/PnDlzAJgwYQKHDh3ik08+YefOndx3331On2fHjh1kZGQwfPhw9Ho9er2e9evX8+qrr6LX64mIiECv19OvXz+H4/r27WvvRhgTE0NFRQW5ubkO+2RkZBAdHW3fJz09vdr1Z2Zm2vcRQrgnr7T6nC0hWiJtzlWgj55gXz2+Buufv9+OVp+Lpc3X8vfWEegj33AL0ZJomS23yght2eyzywihskmGNMdov+6b2AuAH/amcsSD2S2Xg62zderUiauvvprBgwe7dNzEiRPZu3cvu3fvtv+MGDGCGTNmsHv3bnx8fDjnnHM4fPiww3FHjhyhc2frt5DDhw/HYDCwZs0a++Wpqans27ePsWPHAjBmzBjy8/PZtm2bfZ+tW7eSn59v30cI4Z58Wxmhj976ViJztkRLVFRuIqvI+sFs06MXsmf+xbz6V+vfrMM1/EHVvjGPtpUQCiFajhAt2HIxs2WxqJXdCGsIqHpHW5siaOWEov3pFxfMlP4xqCq88vNRj53Xqa/sXn31VW6//XZ8fX159dVX69xXy3rVJygoiAEDBjhsCwgIICIiwr79oYce4q9//SsXXHABEyZMYNWqVaxYsYJff/0VgJCQEGbPns28efOIiIggPDycBx98kIEDB9obbvTt25cpU6Zw22238dZbbwHW1u9Tp06ttROhEMI5Whlhn5gg/jyTL3O2RIt0OrsEgDB/A8G+BsD6nAU4nllMhcmCt77yu8c0e7Al33AL0dIEG6wNMtILXMts5ZZUYEFBUSAysHoXwyuHxlNcYWZyX6l6as/um9STVfvTWLk3lcNphfSOaXhnQqeCrYULFzJjxgx8fX1ZuHBhrfspiuJ0sOWMq666ijfffJNnnnmGOXPm0Lt3b7766ivOO+88h7Hp9XqmT59OaWkpEydOZOnSpfY1tgA++ugj5syZY+9aOG3aNF577TWPjVOI9korI+wXF8yfZ/JJzit1a+0TIRrT6ZxiAIe1r2KCffDTqZSa4XhmEX1jg+2XaWWE0cHSkUyIlsae2Sp0LbOVUWitxAj390avq17Y5WvQMfu8rg0en2jd+sYGc+lA6/qKr/58lP/NaPjcLaeCrZMnT9b4f0/TMlZVzZo1i1mzZtV6jK+vL4sWLWLRokW17hMeHs6yZcs8MUQhRBVaZksrvyg3WcgtMRLu4tonQjSmRFtmq3OEv32boijE+cPxQjiUVuAQbFUtIxRCtCzB3u5ltrRS4g41ZLWEqOq+ib34cV8aP+xNZdU/V9a6n6W8xKnzNWjOlqqqqKrakFMIIVqx/FLrN4VRwb72GngpJRQtzSkt2Ar3d9geF2D9+3Uo1XHeVrpt4r0sbCpEy6NltjILy136DKo11JAGGKI+vWOCmD7cur6u2aLW+eMMt9osLV68mIULF3L0qHXyWM+ePZk7dy633nqrO6cTQrRSWmYr1M9AXKgfmYXlJOeVMiA+pJlHJkSlmsoIAeL8rX8oD6adFWzlS2ZLiJYq2DrtkgqzhbwSI2FOVlJkSrAlXPDsXwYy7+Je1BXPFxYU0PPl+s/lcrD12GOPsXDhQu69917GjBkDwObNm7n//vtJTEzkySefdPWUQohWSpuzFeJvID7Ulz+TJLMlmld+qRF/bx2GKnMyErOqlxFCZbB1KLXAYXt6oQRbQrRUei9rs5vcEiPphWXOB1tF1kqMDoESbIn6KYpS4xIBVflR4dS5XA623njjDd555x2uu+46+7Zp06YxaNAg7r33Xgm2hGgnVFUlz9b6Pczfm/hQa7tcrWRLiKaWXVTOBc+vY1DHUD65fTQAFSaLff23TmcFW7H+oCjW8qLsonIiAn1QVdU+ZytGgi0hWqSoIB9yS4xkFJTTJ8a5YyozWzJnSzQtl+dsmc1mRowYUW378OHDMZlMHhmUEKLlK6kwYzRbMwOh/gb6x1lLB/88k9eMoxLt2d7kfIorzPxxKgeLrZb+TG4JFtW6QPHZ32j76KBTmDUAO2wrJSwoM1FmtAAQJa3fhWiRtFLAQ2kFte5TYbKQVVRu/0m1fYkiczFFU3M5s3XDDTfwxhtv8NJLLzlsf/vtt5kxY4bHBiaEaNnybSWE3jov/Aw6BieEArA/pQCj2eJQxiVEU0jKsWZVjWaVrKJyooJ97ZnWTuH+NS5J0DsmkFM5JRxMK2Rsj0h7VivYV4+vQVdtfyFE8+sfG8zGY9k8vfIQR9KLeOyyfoT4G+yXZxeVc8krG+xNMaqKlDJC0cScCrYeeOAB+/8VReHdd99l9erVjB5tLdPYsmULSUlJ3HjjjY0zSiFEi1N1vpaiKHSJ8CfYV09BmYnDaYXSJEM0uaolrCn5ZbZgS2uO4V/jMX2ig1h9IMM+byvZNucwzlYWK4Roee4a35USo4VlW0/x5Y4z7Dqdy9JbRpJg6zi6eOPJGgOtaD+VfrENX6RWCFc4FWzt2rXL4ffhw4cDcPz4cQA6dOhAhw4d2L9/v4eHJ4RoqbTMVqif9dtERVEYnBDKhqNZ7E7Kk2BLNLlTOVWCrbxShiSE2red3YlQ0yXS3+HYM7Z/E8JrDs6EEM3P31vPE1cO4Mqh8dzz8U6OZxZz9RubWHLzOSSE+/Ph5lMAvDVzOBf3t07qMhqNrFy5kgAftxpxC+E2p55x69ata+xxCCFaGXvb9yqlG4M7WoOtP5PyuGF05+YamminTmc7BltQZY2tWjJbCWHWDJZWgngm13pcxzDJbAnR0g3vHMbXd43lliXbOZRWyPS3NnNej0gKy030jg5ict/o5h6iEA1b1FgI0X7ll1ob4oT4VXZ20uZt7TmT3xxDEu2YqqqcrpLZSrYHW7YywvCaM1taBiutoIwyo7lKsCWZLSFag9gQPz7/+xjO7RFBSYWZ1QfSAbhrQne8vKrP0xSiqbmcSy0rK2PRokWsW7eOjIwMLBaLw+U7d+702OCEEC2XvYzQIbNlLR08klFIUbmJQCnXEE0kq6iCUqPZ/ntqXhlGs8UegNWW2Qr3NxDgraO4wkxyXilncq37S2ZLiNYj2NfAkptH8sjXe/h6ZzLdOgQwdVBccw9LCMCNYGvWrFmsWbOGa665hpEjR9bY3UkI0fZpDTLCqgRbUcG+xIb4kppfxr7kfEZ3i2iu4Yl2pmpWCyAlv5QTmcUYzSqBPvpagydFUUgI9+dQWiFJOSUkSRmhEK2St96LF68dzDXDOtIzOgidZLVEC+FysPXDDz+wcuVKzj333MYYjxCilajMbDkuEDm4Yyip+Wn8mZQnwZZoMqdzrEFSmL+B3BIjKXml9jV4+sQE1fnFYCdbsHUorZCcYutC3VJGKETroygKY3tENvcwhHDg8pyt+Ph4goKkbaYQ7Z3WICPEz+CwfaCtlPBgau2LTQrhaUm28r9RXa0BflZRBX8mWecO9o6p+29WJ9u8rc3HswHrGltnP6+FEEIId7gcbL344ov84x//4NSpU40xHiFEK5FXw5wtgOhgXwBybMGYEE1By2wNSgjBz7YY8a+HMwDoExtc57GdbPO5tifmAJLVEkII4TkulxGOGDGCsrIyunXrhr+/PwaD4wetnJwcjw1OCNFyFZTWnNkK9tU7XC5EU7A3wggPIC7Ul+OZxZzIsnYi7FtPZivBFlyVVFgbbMh8LSGEEJ7icrB13XXXkZyczNNPP010dLQ0yBCincqvJdjSfpdgSzQlLbPVOcKfuFA/jmcW2y/rVV+wddYCxpLZEkII4SkuB1ubNm1i8+bNDB48uDHGI4RoJQrLtHW2zgq2bGWF+RJsiSZSboZsW2OLhHB/4kMrM1Mdw/wI9q17/tXZmayEcMlsCSGE8AyX52z16dOH0tLSxhiLEKKVMFug2FZydfYHWe33gjIjqqo2+dhE+5NVZv031N9AiJ+B2JDKYKlPTN3ztQB8DTpibHMNQTJbQgghPMflYOvZZ59l3rx5/Prrr2RnZ1NQUODwI4Ro+0or144lyNcxQa5luoxm1WGRWSEaS3a5tZxd6yoYF1oZOPWpp4RQ06lKKaHM2RJCCOEpLpcRTpkyBYCJEyc6bFdVFUVRMJvlw5UQbV2JtYKQQB89ep3jdzb+3jr0Xgomi0p+qRF/b5ffZoRwSW659V+tfLBqGWGfWOeCrYRwf7bZuhHGS7AlhBDCQ1z+FLRu3brGGIcQohXRMls1rUWkKArBfgZyiisoKDURG9LEgxPtTp4tsxVnC7JiQ10rI4TKzFaIn6HeOV5CCCGEs1wOtsaNG9cY4xBCtCKlJuuH27NLCDUhtmBLmmS0bqqqYlFB59Wyu87mWntjEBtiLR+MD/UjKsgHg86LLhHOzb/qbNtPmmMIIYTwJLfqe/Ly8li8eDEHDx5EURT69evHrFmzCAmRr7CFaA9KbWWENWW2oHKtLQm2WrdHv97Ld3+m8P2959GtQ2BzD6dWeRWOmS1vvRdr543DS1GqlbnWZmLfKC4bGMvlg+MabZxCCCHaH5cbZPzxxx90796dhQsXkpOTQ1ZWFi+99BLdu3dn586djTFGIUQLU2IrIwyuLdiStbZavfVHMvl0exIlFWZ+3JfW3MOpkzZnS8tsgbUrZqCP898nBvka+N+MYUwZEOPp4QkhhGjHXM5s3X///UybNo133nkHvd56uMlk4tZbb2Xu3Ln89ttvHh+kEKJlqS+zpW2XzFbrVG6y8Pi3++y/b7c1jmiJTGYL+bYywqqNMYQQQoiWwOVg648//nAItAD0ej0PP/wwI0aM8OjghBAtU4ltzlZtjQSCJdhqsQrLjLy4+ggFpUb0OgWdlxcGnYKXomCxWEhM9OKbT3aTmF2Cn0FHqdHMjsRczBa1Rc7dyiyqQEXBoFOIDPRp7uEIIYQQDlwOtoKDgzl9+jR9+vRx2J6UlERQkHMtdoUQrZuzma2CMs8HW/mlRgxGCPGXjnHu+GZXMks3JdaxhxekZQHw9NUDeGz5fgrLTRxMLWBAfMubl5uSVwpAdJAPXi0wGBRCCNG+uRxs/fWvf2X27Nn897//ZezYsSiKwsaNG3nooYe47rrrGmOMQogWptQ+Z6v2boTg+cxWuRkuefV3DDovVj8wzqU5OcJqf7J18flxvTowsms4JrOKyWLBoqpYzBaOHT9Oj+7d6dohiCuHxPPNrhR+O5LJ9sScFhlspeaXARBTZb6WEEII0VK4/Enlv//9L4qicOONN2IyWb/eNhgM3HnnnTz77LMeH6AQouUpqbcbodYgw+TR600qspaNAXy05RR3jOvu0fO3BwfTrMHWX89J4NKBsQ6XGY1GVhqPcunknhgM1sdwZJcwe7B1y7ldm3y89UktsAZbcSEyX0sIIUTL43I3Qm9vb1555RVyc3PZvXs3u3btIicnh4ULF+LjI/XyQrQHpfXM2QpppG6Ep4oqy8Te2XCCMqPZo+dv68wWlcNphQD0jXVusd9zuoQDsO1kLqqqNtrY3JWaZw22YiWzJYQQogVyOdjS+Pv7M3DgQAYNGoS/v3OLRgoh2gat9Xtt86Yaq4ywarCVVVTBJ9tOe/T8bd3JrGLKTRb8DDo6hTv3vj04IRRvnRdZReWczCpu5BG6TisjjA2RL/uEEEK0PE6VEV599dVOn/Drr792ezBCiNZBqw6svRuh9a3F0w0ytGDr8sFxrPgzhbfWn2DGqM54693+3qhdOWQrIewdE+R0Z0Ffg47BCSFsT8zlj8TcFre4sVZGGCtt34UQQrRATgVbISEtb1K0EKJ5qKraLOtspReUkVeh4KXAf6b1Z/PxbNIKyth4LJML+0R77HrasoOp1mDL2RJCzcD4ULYn5nLIVoLYktgzW8FSRiiEEKLlcSrYWrJkSWOPQwjRSpRUmLFgm7NVTzfCkgozRrMFg67hmac9Z6yBQs+oQMICvLlsYAzvbz7FD3vSJNhy0sFUbb6Wa8t09Iy2ZrOOZRY1eAxF5SZW/JlCSUXlfDvVYkYpc/1cZUYzOcXWgF7mbAkhhGiJ3O6bnJmZyeHDh1EUhV69etGhQwdPjksI0UIVlNm6kOoU/Ay6GvcJqlJeWFBqJMIDi83+eSYfgMEdrZn2SwfG8v7mU6w5kEaFaaCUEjrhkJuZrR5RtmArveGZrdfXHeP1X49X2x7rr2Omiw04tKyWt5dKSC2BvxBCCNGcXP50UlxczKxZs4iNjeWCCy7g/PPPJy4ujtmzZ1NSUtIYYxRCtCBah8EgXz2KUvO8H52XQpBtDSxPlRLuSXYMtkZ0CadDkA8FZSZ+P5blketoy/JKKkixBSe9Y1zLbPWwzdNKyS+jqLxh7fzXH8kEYEy3CKYNjmPa4Dh89F6klij2gNpZqbYFjcN8qPW5KIQQQjQnl78KfOCBB1i/fj0rVqzg3HPPBWDjxo3MmTOHefPm8cYbb3h8kEKIliPf1vQipJbmGJpgPwOF5Sa3gi1VVfnjVC7FVT7Ya8HWIFuwpfNSuGRADB9sPsUPe1OZ0CfK5etpT7T5Vh3D/GptbFKbsABvIgN9yCoq53hGEYMTQt0aQ25xBQds2bVXrxtKhyBrxnPuJztZ/mcqX+xI5pxutVdJfLrtNE/+cJAKswUAi8WaCQv1bnkt6YUQQghwI9j66quv+PLLLxk/frx926WXXoqfnx/Tp0+XYEuINq7Q1h0jqJ6yrWA/A8l5pfayQ1cs3niSJ384WG27t5dKz6jKbniXDozlg82nWL0/jYqrpJSwLgdS3Csh1PSICiCrqJyjDQi2tpzIRlWhV3SgPdACuHZEPMv/TOWHvWk8Ps3EmdxS9DqF7md1Pvxyx5kaM2u9QyTYEkII0TK5HGyVlJQQHV19MnpUVJSUEQrRDjib2dLm0Lia2UovKGPhmiOA9UO5PYBSoZd3rkPL8nO6hNszLn+cymFs90iXrqs9WXswHYBhncLcOr5nVBBbTuRwLMP9JhmbjmcDVHuczukcRgdflcwyMzPe3crupDz8vXWse3A80bYug6qqctg2Z+yjW0fRNTIAAEU1s+23n90ekxBCCNGYXP4aeMyYMTz++OOUlVW2jiotLWXBggWMGTPGo4MTQrQ8WqaqvlI07XJXg61nfzxEcYWZoZ1CWXXfBXx/7/l8f+/5fHPnaMbFOmYwdF4KwzuHApWd9lqT9zclsnxXcqNfT1p+GZtPWAOdywfHunUOe5OMDOfu57UH0nn7t+OoVZpebDpunVs3pnuEw76KojA6yloauDspD7B2snxr/Qn7Pqn5ZRSWmdB7KZzTJZy4UD/iQv2I9EDzFSGEEKKxuBxsvfLKK2zatImOHTsyceJEJk2aREJCAps2beKVV15xeyDPPPMMiqIwd+7cGi+/4447UBSFl19+2WF7eXk59957L5GRkQQEBDBt2jTOnDnjsE9ubi4zZ84kJCSEkJAQZs6cSV5enttjFaI90xpk1Nb2XaO1fy9wIdjacSqHb3YloyiwYFp/vJxYeLd3jLUsTuu011qk5pfy+Hf7mfvZbnuA4QpVVe1zlurz3Z/JqCqc0yWMjmH+Ll8XYC/fdDaz9Y+v9vD0ykNsOGoNsNLyyzieWYyiwOiuEdX2HxWlEhviS7fIAOZN7gXAx9tOkVVUDmDPanXrECDlokIIIVoNl/9iDRgwgKNHj/LMM88wZMgQBg0axLPPPsvRo0fp37+/W4PYvn07b7/9NoMGDarx8uXLl7N161bi4uKqXTZ37ly++eYbPv30UzZu3EhRURFTp07FbK5cw+X6669n9+7drFq1ilWrVrF7925mzpzp1liFaO+czWy5E2yt+DMVgKuGxDOoY6hTx/S1ddY77IG25E0pvaDc/v/Hv93ndOAEkJxXypD/rOGBz3c7tf83u1IAuGJIvEtjrKqHba2t0zkllBnNde5bVG4iu7gCgJV7rY/p5hPWoGtAXAgh/tWfO0EG+OX+8/jlwfHcc2EPBncMocxo4d0NJwE4bGvw0SvatU6KQgghRHNy6+tBPz8/brvtNl588UVeeuklbr31Vvz8/NwaQFFRETNmzOCdd94hLKz6XILk5GTuuecePvroIwwGxz/Q+fn5LF68mBdffJFJkyYxdOhQli1bxt69e1m7di0ABw8eZNWqVbz77ruMGTOGMWPG8M477/D9999z+PBht8YsRHvmbGYrWAu2ypwPtrQsxoD4EKeP6WNr+HA4rRCzCwFLc8sprgy2/jyTz0dbT5FXUmH7MVJeRzzz/Z8p5JcaWb47hZNZxXVez5H0Qg6mFmDQKVw20L0SQoAOgT4E++qxqNR7ncm5pfb/r9qfhtFsYe2BDADGdq+e1dLobYtfK4rCvRf2BODDzYnklxo5Ygu2ekuwJYQQohVxOtjasWMHEyZMoKCgeqlOfn4+EyZM4M8//3R5AHfffTeXXXYZkyZNqnaZxWJh5syZPPTQQzVmzXbs2IHRaOSiiy6yb4uLi2PAgAFs2rQJgM2bNxMSEsKoUaPs+4wePZqQkBD7PjUpLy+noKDA4UcI4Xpmy5U5W7kl1mxIWIDzrck7hfvjZ9BRbrKQmF13ENCSZBdZb6tBZy2VfOzb/Qz5zxqG/GcN5zyzjoe36Rn5zDqmv7mZE5mOpXvrDmfY///JttN1Xs8XfyQBMK5XFGEB3m6PV1EUetoCnaP1lBIm51U2S8orMfLuhpOs3GfNcF0+uHqFQk0m9o2iR1QgxRVmVu9Ps2cuXV0jTAghhGhOTgdbL774IhdeeCHBwdXbBoeEhDB58mReeOEFl678008/ZefOnTzzzDM1Xv7cc8+h1+uZM2dOjZenpaXh7e1dLSMWHR1NWlqafZ+oqOrr70RFRdn3qckzzzxjn+MVEhJCQkKCszdLiDZNC55CnJyz9cuhDMa/sI7xL6xj0sKNfHmi9red3GLrucP8nQ8KdF4KvWwlblqpWWugldlNGRDLyC7hNe6TW2JkW2IO7248ad9WUGbkj8Rc++9f/JFUa1nfmdwSPth8CoC/ndPw9zBtceOj9ZRsVs1sATy36hCqCtMGxzmdtVQUhWm2wOy7P1PsAZ4EW0IIIVoTp4OtrVu3csUVV9R6+eWXX15npuhsSUlJ3HfffSxbtgxfX99ql+/YsYNXXnmFpUuXoij1T5KvSlVVh2NqOv7sfc726KOPkp+fb/9JSkpyaQxCtFWFtsxWUD2ZLe1DcZnRQmJ2CYnZJZzKKWFDulet87jytMyWC8EWQJ9W2CQjxxZsRQf58Pnfx3DsqUvsPwfnT+LZc0w8f/UAAFbvT7OXSG48moXJotI1MoC4EF9yS4ys2lfzF0fP/HiIcpOF0d3Cmdi34Ys+D7AtKL09MafO/c7kWYOtPlUCI4NO4aGLe7t0fZfayh43HM2iwmTB1+BFgpsNPoQQQojm4PQ6W8nJyQQF1f6NYmBgIKmpqU5f8Y4dO8jIyGD48OH2bWazmd9++43XXnuN5557joyMDDp16uRw+bx583j55ZdJTEwkJiaGiooKcnNzHbJbGRkZjB07FoCYmBjS09OrXX9mZmaN64VpfHx88PGRlsKifSsuN3HTe9s4UWWOjhYQ1ZfZ6hsbzIaHJ5BeULlMxM1LtlNUbiKjsJyI4OofmnPcDbZire9NB1tTZstWRhgeaL2t2nwlANXihZ8eJg+K4akfD5NVVMH2xBxGd4tg3SFrCeHEPlEE+RpYuPYIjy3fxys/H7Ueq6qYy3T8WraPH/ak4qXA/03t7/KXVjXR5lvtPJ1HmdGMr0FX435aZuuqofG8s+EEWUUV3DC6MwnhrgVKPaIC6RMTxKEqzTGc6VAphBBCtBROZ7Y6dOhQZ0OJQ4cOERnp/IKiEydOZO/evezevdv+M2LECGbMmMHu3bu5+eab2bNnj8PlcXFxPPTQQ/z0008ADB8+HIPBwJo1a+znTU1NZd++ffZga8yYMeTn57Nt2zb7Plu3biU/P9++jxCiZp//kcQfp3LJKa6w/1hU8NOpdHbig3NCuD8juoTbf2KCrV9gZBSWV9u3zGimzGhda8mVOVtQmUVrTWWEWoOMiDrmURl0XkzuZ/1SaNW+NCwWlXWHMwGY0CeKv41MwN9bR2G5iZNZxZzMKiYxu4SkYsXegfBvIzvRL656+bc7ukUGEB3sQ4XJws5TubXul2LLbHUK9+c/Vwzg6mHxzJ3Uy63rnDqosqmHNMcQQgjR2jid2Zo0aRJPPfUUU6ZMqXaZqqo8/fTTNTa5qE1QUBADBgxw2BYQEEBERIR9e0SEY9cqg8FATEwMvXtbS1FCQkKYPXs28+bNIyIigvDwcB588EEGDhxoH0vfvn2ZMmUKt912G2+99RYAt99+O1OnTrWfRwhRncWi8v6mRADmTe7FxQNiADAZTezZ+pu926ArooJ9OJZZXGOwpTXH0HspBPo4/dYEVJYRns4poajc5PLxzUErIwwPqDuDfunAGL7ccYYf96UyrlcHsorKCfDWcU6XcLz1Xvw8bxxnqsyRMplMrFq/BUt4F4qNFh52sXSvLoqiMLZ7JN/sSmbT8WzG9qj5C7ZkW7AVH+bHoI6h9nJAd1w6MJb/rj4CyHwtIYQQrY/Tn0j+/e9/M3z4cEaNGsW8efPo3bs3iqJw8OBBXnzxRY4cOcKSJUsac6w1WrhwIXq9nunTp1NaWsrEiRNZunQpOl1lectHH33EnDlz7F0Lp02bxmuvvdbkYxWiNVl3OIPE7BKCffXMOq8rAbYAxmg0ctTNWCYqsPbMlhZ8hPp7u1zyFh7gTXSwD+kF5RxOK2R45+rLSLQ02fZgq+6SyXN7RBLkoye9oJxblm4H4KL+MfaFfWND/IgNqVx6w2g0khGpcumlfastl+EJY7pH2IKtLKB6IFdhstgf3/hQ95YEqapbh0CGdgpl1+k8hrWCx1UIIYSoyumPTN27d2ft2rXcfPPN/O1vf7N/GFJVlX79+rFmzRp69OjRoMH8+uuvdV6emJhYbZuvry+LFi1i0aJFtR4XHh7OsmXLGjQ2Idqb9363dsD728hO9kCroaLqKCPMK7E2zQh3sYRQ0ys6iPSCco5ltI5gSwsu6yojBPDR65jYN4rlu61lgdMGx7HgCvcWkPcEbd7Wn2fya8wiphaUoarga/CqN5B01lszh3Mys5hhnVr+4yqEEEJU5dInqBEjRrBv3z52797N0aNHUVWVXr16MWTIkEYanhCiORxJL+T3Y9l4KXDjmM4eO29UkC3YKqi9jDDUxeYYmo5h1ixKan5ZPXs2v9IKMyUV1nbtWoOMuvx9fHeyiyv4y7COXDk0vrGHV6eOYf50CvfndE4J2xNzmNDbscuhNl8rPtTPI005AKKCfIkKqt61VgghhGjp3Pq6esiQIRJgCdGGfbbdutTBpL7RdPRgq217sFXTnK1irROhe5mtmGBrsFW1+2FLlW1rjuGt8yLIiaxhn5hgPpw9qt79msqYbhGczilh7qe7q2S2VHr6eXFxjPX+j/NACaEQQgjR2rX8WeRCiCZVYbLwza5kAP420rOLedcZbNnLCN3LbMWEWM/dGjJbOVXma3kq+9OUpgyM4bM/ksgvNdoXuQZIzvPCx9YtUcs0CiGEEO2ZBFtCCAe/HEonp7iCqCAfLujZwaPnrjpn6+yFxRtaRhgdbC0zS2sFwZazzTFaqgm9o/j1wfEOgdbCNYf59UgWPx2wrgPmieYYQgghRGsnwZYQwoFWQviX4R0dFtr1BK0bYbnJQkGpiZAqJYMNLSPUOvKltYIywhzbgsYRTszXaqm6RAY4/P73C7ry65Es++/xktkSQgghnF/UWAjR9qXll7H+iLUMbPoIz5YQAvgYdPjrVQDSCx2DIq2MMMzNzFaMLbOVV2KkzGhuwCgbX04rz2zVZFinUBICVPvv8aGem+snhBBCtFYuB1urVq1i48aN9t//97//MWTIEK6//npyc3M9OjghRNP65VAGFhWGdw6j61mZC08JtiWuzu5ImFeiZbbcC0CC/fT4Gazr67X0UsLWXkZYE0VRGB9rsf8umS0hhBDCjWDroYceoqCgAIC9e/cyb948Lr30Uk6cOMEDDzzg8QEKIZpOWr61bXff2KBGu44Qb1tm66xyvxwt2HJznS1FUYgJsc3bauGlhDm2boSRtrLKtmJIhMqQhBBGdA6zZxqFEEKI9szlOVsnT56kX79+AHz11VdMnTqVp59+mp07d3LppZd6fIBCiKaTWdT4QUCILZlzdhlhXnHDygjBWkp4MqvY7fbvSTklfPdnCkZzZYZGr0BIhdtDqlF2UdvLbAHoveDz20bi7d22bpcQQgjhLpeDLW9vb0pKSgBYu3YtN954IwDh4eH2jJcQonXKLLQGAY0ZbNVURmg0WygsNwENDLZsmS13278/8f0BVh9Ir7Z9ZAcv/ub2qKpri2WEmtbYyl4IIYRoLC4HW+eddx4PPPAA5557Ltu2beOzzz4D4MiRI3Ts2NHjAxRCNJ0sW2arQ1BjZrasZYQZVTJbWtt3RYFgP/fKCMH19u8LVuwnu6iCl/86BC8vhRNZxQBc1C+aqGAfckuM/LAnlUN5Cqqq1nM252kNMiLaYLAlhBBCiEouz9l67bXX0Ov1fPnll7zxxhvEx8cD8OOPPzJlyhSPD1AI0XSymqCMMFgrI6yS2cqzdSIM9TOg83I/MxIb4nywteNULkt+T+S7P1M4klGIqqqk5FnnrD16aV+evHIgL00fjK/BiwKjwtGMIrfHdba22I1QCCGEENW5nNnq1KkT33//fbXtCxcu9MiAhBDNQ1VVMgttma1GnbNVvUFG5RpbDQs+7JktJ+ZsvbvhhP3/Z3JKiQn2paTC2jJeC9p89DrO6RzGhmPZ/H48h/4dwxs0PoByk5kiW8lkREDbapAhhBBCCEduLWpsNptZvnw5Bw8eRFEU+vbtyxVXXIFOp/P0+IQQTaSo3ES5ydoYIjKo8TIuVedsqaqKoij2MsJQNxc01jib2TqVXcyq/Wn235NyS4gNtR4bGeiNr6HyvWxs9wg2HMtm0/Fsbh/Xo0Hjg8qslt5LIdhP1pUXQggh2jKX/9IfO3aMSy+9lOTkZHr37o2qqhw5coSEhAR++OEHunfv3hjjFEI0sixbh7wAbx3+3o0XBGjdCCvMFvJKjIQFeNsXNG5oWZ3WICOzqByT2YJeV3Ol9OKNJ6k6BSspp5SOYdYALS7UcX2oc7tHALAtMZcKkwVvfcPWgq/aiVCaSQghhBBtm8ufGubMmUP37t1JSkpi586d7Nq1i9OnT9O1a1fmzJnTGGMUQjQB+3ytRmyOAdb24PG2LNIHm08BVMlsNSzYigz0QeelYLao9uDxbKUVZr744wwAlwyIAayZrVTbGmNxIY7BVu/oQAL1KiUVZnadzuVUdjElFSa3x9iWOxEKIYQQwpHLwdb69et5/vnnCQ+vnLsQERHBs88+y/r16z06OCFE09HmazXFQrvzJvcEYNEvRzmQUlBlzlbDygh1XgpRtmCxtnlbp3NKKDWaCfbVM31EAmBdXyvZ1hzj7MyWl5dCrxBrGuzmJdsZ98Kv3PbBH26PUVvQOCJQgi0hhBCirXM52PLx8aGwsLDa9qKiIlnIUohWzN72vQmCrakDY7i4fzQmi8o9n+xk/ZFMAMI8kO2pbP9eWuPlyXnWdQI7hvmTEG4NrM7klpKcqwVbvtWO6RtmDbZKjdYGGpuOZ1NYZnRrfJVlhNIcQwghhGjrXA62pk6dyu23387WrVtRVRVVVdmyZQt///vfmTZtWmOMUQjRBLK0zFYjNsfQKIrCk1cOJMzfwInMYo6kW9uqn13C5w6tScYzPx7imjc2cc0bm5j+1mbW2hYr1oKq+DA/Oob5A9bmIAdSrYuyn53ZAhgRqTL/8r68PmMY8aF+qCr8mZTv1vhkjS0hhBCi/XB5Fvyrr77KTTfdxJgxYzAYrCU/JpOJadOm8corr3h8gEKIppFpy7g0RRkhWBdO/mDWKH7an4aKSniAD1Nsc6gaom9sMD/uS+NUdgmnskvs28uMZib1iyY5z1peGB/qh69BR4cgHzILyzmRaV3QuKZgy0uBGSMTMBgM/LQ/jeTdpew4lct5PSNdHp+ssSWEEEK0Hy4HW6GhoXz77bccO3aMgwcPoqoq/fr1o0ePhrdEFkI0n6acs6UZ2DGEgR1DPHrOv4/rzqCOIZTZSv4yCsv5v2/3czKzGFVV7XOz4m1BVUKYn/22Q81lhFUN7xzGt7tT2Hk6163xaQ0yZM6WEEII0fa53d+5R48e9OjRA7PZzN69e8nNzSUsLMyTYxNCNCH7nK1G7kbY2Lz1XozvHWX/vdxk5vHv9lNYbiKrqILkXGu2Kz7MGmx1DPNn5+k867E6LyLrmUs1rJP1fW7n6VwsFhUvL9fat0sZoRBCCNF+uDxna+7cuSxevBiwLm48btw4hg0bRkJCAr/++qunxyeEaCL21u9NmNlqCj56HR1tgdXJrOJqXQe1JhkAsaG+9QZPfWKC8DPoKCwzcTyzyOXxZNvuZ2mQIYQQQrR9LgdbX375JYMHDwZgxYoVnDhxgkOHDjF37lz+9a9/eXyAQojGp6pqk3YjbGpdIwMBOJxeSIatZLCyjNDfvp8zDTr0Oi8GJ1hLH3eccr2UUNbZEkIIIdoPl4OtrKwsYmKsk9hXrlzJ9OnT6dWrF7Nnz2bv3r0eH6AQovEVlZsoM1qApulG2NS6RQYAsPl4FqoKPnovIm1zphLCqwRbNTTHqMnwzpWlhK6oMFkoLLMuiCxlhEIIIUTb53KwFR0dzYEDBzCbzaxatYpJkyYBUFJSgk6n8/gAhRCNL8vWiTDAW4e/t9tTOVusrrZg6/dj2YA1q6Uo1nLBqpmt+HqaY2i0eVuuZrZyS6z3s85LIcSvYQs4CyGEEKLlc/lT1S233ML06dOJjY1FURQmT54MwNatW+nTp4/HByiEaHz2+VqtvDlGbbRgK7/UuhBx1QxWbKgvXgpYVOczW0NtwdbxzGIKyowE+zoXOGkLGof5e7vcWEMIIYQQrY/Lwdb8+fMZMGAASUlJXHvttfj4WD+c6XQ6HnnkEY8PUAjR+LKaoe17U9KCLU18laDKoPMiLtSPM7ml9g6F9QkP8CYuxJeU/DIOpRYysmu4U8dJJ0IhhBCifXGrXuiaa64BoKyszL7tpptu8syIhBBNLtPeibBtBgFxoX54672oMFnnpZ0dVD08pQ+bjmUxqmuE0+fsFxdCSn4Z+1PynQ62sou1ToRt834WQgghhCOX52yZzWaeeOIJ4uPjCQwM5MSJEwA89thj9pbwQojWRctstfY1tmqj81LoElF7I4xpg+N49i+D8NY7/5bYLy4YgP0pBU4fo2W2wttoUCuEEEIIRy4HW0899RRLly7l+eefx9u78gPDwIEDeffddz06OCFE00jOs2apY4KdaxDRGlUtJYx3cm5WXfq7EWxpc7akjFAIIYRoH1wOtj744APefvttZsyY4dB9cNCgQRw6dMijgxNCNI2knBIAOkUE1LNn66WttQXYFzluCC3YOpZRaC9PrI+ssSWEEEK0Ly4HW8nJyfTo0aPadovFgtFo9MighBBN67QWbFVZc6qt0dbaUhSI9kAGLz7UjxA/A0azypH0QqeOybHN2ZLMlhBCCNE+uBxs9e/fnw0bNlTb/sUXXzB06FCPDEoI0XTKjGbSC61lhAkeyPi0VD2irZmtuBA/l+Zm1UZRFPrFWrNbB5wsJbR3I2yjXR+FEEII4cjlboSPP/44M2fOJDk5GYvFwtdff83hw4f54IMP+P777xtjjEKIRpScV4qqWhc0bsvlbUMTQvnHlD4MiA/22Dn7xwWz+UQ2+1PygYR695cyQiGEEKJ9cfnr3csvv5zPPvuMlStXoigK//d//8fBgwdZsWKFfYFjIUTroZUQJoT7oyhtd6FdRVG4c3x3zu/ZwWPn7G8L3A6kupjZkmBLCCGEaBdcymypqsqxY8dISEjg559/Rq93a5kuIUQLktQO5ms1ln6xIYC1jNBiUfHyqj1YNZkt5JVY57VKZksIIYRoH5zObCUmJv5/e3ceFlXZ/gH8Owwwwzpsssnijgui4ootam6YZLuZiprtmmaLqfWWWpbZW1pplplKr2urpf4St9QyN0RRVMINBRUEYdi3Yeb5/THM6AjigDMMA9/PdXFdztm4z2FGzs39PPdB165d0b59e3Tu3Blt2rTB0aNHzRkbEdWD1GwmW3XVupkT7G1tUFSu1lcIbyenWFvVkkgAN0cmW0RERE2B0cnWjBkzUFpaitWrV+Onn36Cn58fXnzxRXPGRkT1IE15Yxgh1Y6t1AZtvbWNN/7NqLkjoW4IobujPaQ1VMCIiIio8TB6HODff/+N9evXo1+/fgCAXr16ITg4GCUlJXBwaLwdzIgau9ScEgCsbNVViK8LTl3NR3JGASJDfW+7XQ4faExERNTkGF3ZysjIQPv27fWvAwIC4ODggGvXrpklMCIyPyGEfs4WK1t1097XBQCQfK3mJhlZhdpnbHG+FhERUdNhdLIlkUhgY2O4uY2NDYQQJg+KiOqHsliFwrIKAEBAI37GljmF+Go7Et5pGGFCWi4AoG3l876IiIio8TM62RJCoF27dvDw8NB/FRYWolu3bgbL6mr+/PmQSCSYNm0aAEClUmHGjBno3LkznJyc4O/vj3HjxuHq1asG+5WVlWHKlCnw8vKCk5MTRowYgcuXLxtso1QqER0dDYVCAYVCgejoaOTm5tY5VqLGIk2pHULo6yqH3E5q4Wisk66ydfF6EUpV6ttudzglBwDQu6VnvcRFRERElmf0nK1Vq1aZLYi4uDh8++23CAsL0y8rLi7G0aNH8e6776JLly5QKpWYNm0aRowYgSNHjui3mzZtGjZv3owNGzbA09MTb7zxBqKiohAfHw+pVHvzOHr0aFy+fBmxsbEAgBdeeAHR0dHYvHmz2c6JyBpcrky2Aj1Y1aorbxcZ3BztkFuswrnMQoQ2V1TZJq9EpX8WV++Wdf+jFBEREVkXo5Ot8ePHmyWAwsJCjBkzBsuXL8e8efP0yxUKBXbs2GGw7eLFi9GrVy+kpqYiKCgIeXl5WLFiBVavXo1BgwYBANasWYPAwEDs3LkTQ4cORVJSEmJjY3Hw4EH07t0bALB8+XJEREQgOTkZISEhZjkvooZECIGSm6ouKlUFytTA5etFADhf625IJBKE+LjgUEoO/s0oqDbZir+UAyGAll5O8HaVWyBKIiIisgSzPJVYCAGJxLjWxpMnT8bw4cMxaNAgg2SrOnl5eZBIJHBzcwMAxMfHQ6VSYciQIfpt/P39ERoaiv3792Po0KE4cOAAFAqFPtECgD59+kChUGD//v23TbbKyspQVlamf52fX/Pkd6KGSqMRePyb/TiWmnvLGlsA5wGwE+Hdau+rTbbOXKt+3tahC7ohhKxqERERNSVGzdnq0KED1q1bh/Ly8hq3O3v2LF5++WUsWLDAqG++YcMGHD16FPPnz7/jtqWlpZg5cyZGjx4NV1fthPSMjAzY29vD3d3dYFsfHx9kZGTot/H29q5yPG9vb/021Zk/f75+jpdCoUBgYKBR50TU0FxWllSTaN3gaC9Fv3bN6i+gRuhOTTIOVs7X6sVki4iIqEkxqrL11VdfYcaMGZg8eTKGDBmCHj16wN/fH3K5HEqlEqdPn8a+fftw+vRpvPLKK5g0adIdj5mWloZXX30V27dvh1xe87AalUqFUaNGQaPRYOnSpXc89q2VteqqbHeqvs2aNQuvv/66/nV+fj4TLrJKyZXVlva+Ltg46R4A2s/Utm3bMHToUDjI7WEnNbpXDlUjRNf+PaNqBbyorAInr+QBAHq3YnMMIiKipsSoZOuBBx5AXFwc9u/fjx9++AHr1q3DxYsXUVJSAi8vL3Tr1g3jxo3D2LFj9UP87iQ+Ph6ZmZno3r27fplarcZff/2FJUuWoKysDFKpFCqVCiNHjkRKSgr+/PNPfVULAHx9fVFeXg6lUmlQ3crMzETfvn3121T3LLCsrCz4+PjcNj6ZTAaZTGbUuRA1ZGduSrYc7LVNY2wlGthLAQd7KRMtE9AlW9fyyzB38ylIJRJoNBpcuGiDnZtOQ60RCHB3QHM3NiIhIiJqSmo1Z6tv3776JOZuDRw4EImJiQbLnnnmGbRv3x4zZswwSLTOnj2L3bt3w9PT8K/C3bt3h52dHXbs2IGRI0cCANLT03Hy5El88sknAICIiAjk5eXh8OHD6NWrFwDg0KFDyMvLM9m5EDVkZyuTrbY+LhaOpPFyltmilZcTLlwvwqp/Lt60xgZI1w5XjmBVi4iIqMkxS4MMY7i4uCA0NNRgmZOTEzw9PREaGoqKigo88cQTOHr0KLZs2QK1Wq2fY+Xh4QF7e3soFAo8++yzeOONN+Dp6QkPDw+8+eab6Ny5s747YYcOHRAZGYnnn38ey5YtA6Bt/R4VFcVOhNQkJF8rBACEMNkyq0VPdcXWkxkQ0D7oXaPW4MKFC2jVqhUcZHYY0zvIwhESERFRfbNYsnUnly9fxqZNmwAAXbt2NVi3e/du9O/fHwCwaNEi2NraYuTIkSgpKcHAgQMRExOjf8YWAKxduxZTp07Vdy0cMWIElixZUi/nQWRJFWoNzmdpk612TLbMqkugG7oEuulfq1Qq/PHHOTw4tB3s7OwsFxgRERFZTINKtvbs2aP/d4sWLSCEuOM+crkcixcvxuLFi2+7jYeHB9asWWOKEImsyqWcYpRXaOBgJ0WAO+cLEREREdUnzownasRuzNdyho2Ncc++IyIiIiLTYLJF1IglZ2iHELb15hBCIiIiovrGZIuoETuTqa1shfg6WzgSIiIioqbH6DlbNzecqIlara5zMERkWmz7TkRERGQ5RidbQggEBwdj/Pjx6NatmzljIiITKK/Q4EJWEQC2fSciIiKyBKOTrUOHDmHlypX44osv0LJlS0ycOBFjxoyBu7u7OeMjojo6dTUPFRoBF7kt/BRyS4dDRERE1OQYPWerZ8+e+Prrr5Geno7XX38dGzduREBAAEaNGoUdO3aYM0YiqoP/O5EOAOgf4g2JhJ0IiYiIiOpbrRtkyOVyjB07Frt27cLJkyeRmZmJyMhI5OTkmCM+IqoDjUbg/xK1yVZUmJ+FoyEiIiJqmur0UOPLly8jJiYGMTExKCkpwfTp0+Hq6mrq2Iiojo6mKpGeVwoXmS36tWtm6XCIiIiImiSjk63y8nJs3LgRK1aswN9//41hw4bh888/x4MPPggbG3aQJ2pItlQOIRzc0QdyO+M6iRIRERGRaRmdbPn5+cHFxQXjx4/H0qVL4e3tDQAoLCw02I4VLiLz+CT2X/x45DIAAQAQAigrl+L9E3vgYC/FgsfDcE8bL6hvHkLYhUMIiYiIiCzF6GRLqVRCqVTigw8+wLx586qsF0JAIpHwOVtEZiCEwKp/LqJEdevnS4JCVTlQBGyIS8M9bbxwNFWJrIIyKBzscG8bDiEkIiIishSjk63du3ebMw4iqsH1wnKUqNSQSIAtU+6FrY0NKipU+Ovvv6Fo2QVv/3YKiZdzAQDxl5QAgL6tPWFvyyG+RERERJZidLLVr18/c8ZBRDVIUxYDAHxd5ejkrwAAqFQqnHMEIjo0w9u/ARezi5FXokJCai4AoGugm2WCJSIiIiIAdWj9fjtHjx5FVFSUqQ5HRDdJy9EmW4EejlXWuTvaI8DdAQBw6koeEtJyATDZIiIiIrK0WrV+37FjB7Zv3w47Ozs899xzaNWqFf7991/MnDkTmzdvxuDBg80VJzVAQggs3XMeJy7n4lp+GUpvmk8khIC0zAb9B1VAYWdnwSgbh9RsbbIVVE2yBQBhAQpcVpZgZ1ImMvJLYSMBQpsr6jNEIiIiIrqF0cnW999/j2eeeQYeHh7IycnBd999h4ULF2LSpEl4/PHHcfz4cYSGhpozVmpgDl7IwX+3JdewhQ32JF/Hw+GB9RZTY5WaU3OyFdpcgT8SM/BzfBoAoJ2PC5xkdXqMHhERERGZiNF3Y4sWLcJHH32EmTNn4scff8SoUaOwaNEiHDt2DK1btzZnjNRA7T2TBQCIaOWJCfe0gPNNN/cx/6RgR1ImTl7Nx8Phloqw8dDN2Qr0cKh2fVhzNwBAfmkFAKBbkFt9hEVERERENTA62Tp//jyeeuopAMATTzwBqVSKhQsXMtFqwv4+q022nuoZiKGdfA3WXcwqwI6kTJy6mm+J0BqdtJwSADVVtgyfb9clwM3cIRERERHRHRjdIKOoqAhOTk7anWxsIJfLERjI4WFN1fXCMn0idU8bryrrO/lrb/5PXs2HEKJeY2tsyis0uJqnTbaqa5ABAG6O9gaJWFdWtoiIiIgsrlaTOrZt2waFQjvpXqPRYNeuXTh58qTBNiNGjDBddNRg/XPuOgCgo58rmrnIqqxv6+0MqUQgv7QCqTnFCPZ0qu8QG40ruSUQApDb2aCZc9VrrdO5uQKpOcVwspeirbdLPUZIRERERNWpVbI1fvx4g9cvvviiwWuJRAK1Wg2yLmeuFcBGArS5zQ16eYUGf527hkvZxcgvVSEqzA9/ndEmW/e1q1rVAgB7Wxv4OwJpRUDilTwmW3ch7abmGBKJ5LbbdQ5Q4P8S09E5QAGpze23IyIiIqL6YXSypdFozBkHWUhucTke/eofSG0kOPj2QDjaV31LfPPXBSzefUH/etneC7CVam/m+7VtdttjBzkLpBVJkHglD1Fh/qYPvonQdSIMdK9+CKHO072CcCGrEKN6BdVHWERERER0B0bP2Zo4cSIKCgrMGQtZwP7z2SgqVyO/tAJHL+VWu83uZG0Vq08rD3QPdkeJSo2C0grI7WzQvYX7bY8d6KSdq5V4Oc/kcTclNT3Q+GYKBzt88kQXhAfd/mdCRERERPXH6GTr+++/R0lJiTljIQvQdRQEgIMXsqusL64ATqdrG2F8/lQ3/PhiBF7qp+1AOaSjL2S20tseO9BZm2ydvJLHJhl34U7P2CIiIiKihsnoYYS8WW58hBD6uVcAcKCaZOtCvgQaAbT0coKvQg4AmDmsPcZFBMOrhmYNAODrANhJJWyScZd0z9hiskVERERkXYyubAGocXI+WZ+L2cW4klsCXS+F42m5KC6vMNjmbL52ZZ9WngbL/d0cYG9b89vH1gbo4KttunGCQwnrLDW7MtnyZLJFREREZE1qlWy1a9cOHh4eNX6R9dANIezV0gPN3RxQoRGIv6Q02OZsnjbZimjtWWV/Y4QFaB8VcCw1t+6BNmGZBaXIL62ARHLnBhlERERE1LDUqvX73Llz9c/ZIuunb9/ethnOZxXi16NXcPBCNu6r7DCYW6zCVW1RBX1a1S2R7hqgwJpDaTiWprzzxlSF7sHRrbyc4GB/+/lxRERERNTw1CrZGjVqFLy9vc0VC9UjlVqjb4hxf9tm8HaR4dejV3Dg/I15W4cv5kBAgtbNnODtIq/T9+ka5AYAOHUlH2UVashspSgur4CDnZTDUo1wujLZ6uTPP3IQERERWRujky3eGDcuJ6/kobCsAm6Odujk7wo3RzsAwNHUXHR9fzsAoFSlfUB1n5Z1Hx4a5O4ATyd7ZBeV4+SVfCiLyvH86iPo4OuKSQNaY1ioHx/AW4NTV7Vz3Tr5u1o4EiIiIiKqLaPnbLEbYeOSeEV7E98lwA02NhIEejiiS+X8qtxiFXKLVShVaR9kHdnJp87fRyKRoFtldetYqhLL/joPIbTt5F9ZdwyzN528uxNp5HTDCDsy2SIiIiKyOkZXtjQajTnjoHqme9CwroEFAPz0Ul/9M50AoKKiAnH/7K3zfC2dbkHu2JmUiY3HruDU1XzYSICRPQKxIS4NBy/k3NWxG7P8UhUuVXYi5DBCIiIiIutTq26E1HjoKluhzW/cxNvb2qCNt7P+q3UzJ7ja3/33Cg9yB3CjSjMgxBsv99c+GDktpxgaDaum1UmqvF5+Cjk8nEzwgyAiIiKiesVkqwkqKVfjbGYhAMPKlrl0CVTg5mlZI3sGwt/NATYSoKxCg6zCMrPHYI1O6ZtjcAghERERkTVistUEnU7Ph1oj4OUsg69r3boM1oajvS3a+2oTBi9nGR5o7w07qQ383RwAaKtbVNWN+VocQkhERERkjZhsNUEnK4cQdm7uWm9dJu9po30o8sgeAbCTat92QR7ah/SmMtmq1ul0VraIiIiIrFmtnrNFjcOJyuYYnQPc6u17vjqoHboEumFIR1/9siAPR+w/n81k6yYl5WpMWX8MWYVlOHOtAACTLSIiIiJrxWSrCbpR2aq/4WnOMltEhfkbLAtkZauKQynZ2Jl0Tf/aXyFH88rhlkRERERkXZhsNTHF5RU4m6mtmNRHc4ya6IYRNqY5WxVqDezs6r5/brEKANDe1wUv92+NroFufKA4ERERkZXinK0m5lhqLjQCaOYig089NMeoSWOrbOWVA30W7MH0n47X/Rgl2mSrpZcTHu7aHMGeTqYKj4iIiIjqGZOtJqS8QoMPtpwGAAwIaWbhaG5Utq7ll6FUpbZwNHcvrUiCvJIKbDmRjgp13R4Crku2FA53UR4jIiIiogahwSRb8+fPh0QiwbRp0/TLhBCYM2cO/P394eDggP79++PUqVMG+5WVlWHKlCnw8vKCk5MTRowYgcuXLxtso1QqER0dDYVCAYVCgejoaOTm5tbDWTUsS/ecw78ZBfBwsseMyPaWDgfujnZwlmlHsl5Wllg4mrtXpM2TUKJSI7myuUVtMdkiIiIiajwaRLIVFxeHb7/9FmFhYQbLP/nkEyxcuBBLlixBXFwcfH19MXjwYBQU3LiRnTZtGjZu3IgNGzZg3759KCwsRFRUFNTqG5WS0aNHIyEhAbGxsYiNjUVCQgKio6Pr7fwagrOZhVjy5zkAwJwRneDpLLNwRIBEItEPJWwM87aKKm78+1hqbp2OoUu2XJlsEREREVk9iydbhYWFGDNmDJYvXw53d3f9ciEEPv/8c7zzzjt47LHHEBoaiu+//x7FxcVYt24dACAvLw8rVqzAZ599hkGDBqFbt25Ys2YNEhMTsXPnTgBAUlISYmNj8d133yEiIgIRERFYvnw5tmzZguTk5NvGVVZWhvz8fIMva7blRAYqNAL92jXDQ2F+lg5HL8hD22mvMczbKlLdaGSRkJZbp2Pks7JFRERE1GhYPNmaPHkyhg8fjkGDBhksT0lJQUZGBoYMGaJfJpPJ0K9fP+zfvx8AEB8fD5VKZbCNv78/QkND9dscOHAACoUCvXv31m/Tp08fKBQK/TbVmT9/vn7YoUKhQGBgoEnO11KOVz5ba3BHnwbV3a4xPdjYsLKlrNMxOIyQiIiIqPGwaLK1YcMGHD16FPPnz6+yLiMjAwDg4+NjsNzHx0e/LiMjA/b29gYVseq28fb2rnJ8b29v/TbVmTVrFvLy8vRfaWlptTu5BkQjgMTKZ2t1DXSzbDC30CVbKdeLkF+qQn6pCgWlKmiEhQOrg0LVjX+fzyrSJ061wWSLiIiIqPGw2HO20tLS8Oqrr2L79u2Qy2/fgvzWKowQ4o6VmVu3qW77Ox1HJpNBJrP8vCZTyCoF8ksrILO1QYivi6XDMRBQmWz9+W8mwuZs1y/3kksxcLAadnfz0Kp6VlRh+H46npaL+9vVrusjhxESERERNR4Wq2zFx8cjMzMT3bt3h62tLWxtbbF37158+eWXsLW11Ve0bq0+ZWZm6tf5+vqivLwcSqWyxm2uXbtW5ftnZWVVqZpZs/WHUzF/a1K1LcdTC7VJQOfmCthJLT5y1EB4kDuauzlUWX69VIITldU4a6EbRuhb+fyyujTJYGWLiIiIqPGw2J33wIEDkZiYiISEBP1Xjx49MGbMGCQkJKBVq1bw9fXFjh079PuUl5dj79696Nu3LwCge/fusLOzM9gmPT0dJ0+e1G8TERGBvLw8HD58WL/NoUOHkJeXp9/G2qXlFOOdjYlYtvcCtp2qmlheKtAmW10a2BBCQJtU/P3WAJyZN0z/NbiDdthn4hXrakqiG0Y4oL02/oS02s3bUqk1KCrXdtFkskVERERk/Sw2jNDFxQWhoaEGy5ycnODp6alfPm3aNHz00Udo27Yt2rZti48++giOjo4YPXo0AEChUODZZ5/FG2+8AU9PT3h4eODNN99E586d9Q03OnTogMjISDz//PNYtmwZAOCFF15AVFQUQkJC6vGMzWf53xf0c5xW/pOC4bd0G7xUWdlqaPO1dGxsJLC3uTEEr0uAAjuSMnHSipIttUaguLKy9UB7b6w/nIqEtFyjhr3q5Jfe6LDhIrfYR5OIiIiITKRB39G99dZbKCkpwaRJk6BUKtG7d29s374dLi435h0tWrQItra2GDlyJEpKSjBw4EDExMRAKpXqt1m7di2mTp2q71o4YsQILFmypN7PxxyyC8vw4xFt8w6JBIi/pERCWq4+sSpTqXGlstFfQ022bhXa3BUAkHhVO4ywQq1BXomqQTwb7HbyS1UQ0CZVvVt5AACUxSooi1XwcLI37hiVQwidZbawbWDDPYmIiIio9hpUsrVnzx6D1xKJBHPmzMGcOXNuu49cLsfixYuxePHi227j4eGBNWvWmCjKhuV/By6hVKVB5+YKtPV2xq/HrmDVPyn4YlQ3AMDpjAKohQQeTnYIcK86N6ohCvXXJlupOSXILS7Hf7clY0NcGr4aHY7IUF8LR1e93OIbiZKr3A5+CjnS80qRcr3I6GSL87WIiIiIGpcGlWxR7ZSq1PjfgYsAgJf6tUawpyN+PXYFm49fxfHKh+oWlmmHpnUJUDSo52vVROFgBy+ZwPUyCf4+ex0/xV+GWiPwzsZE9Gzh3iArXMrKZMvdUZsotfRyQnpeKS5eL0L3YPeadtXTDSN0ZbJFRERE1ChwrJIV23z8KpTFKjR3c0BkqC9CmytwX1svaARwMbsYF7OLcb2wHABwf1svC0dbO0HO2klo/92WjPIKbYfF7KJyzNl82pJh3ZaySHud3Z20iVILLycAwMXsIqOPcaOyxb+BEBERETUGvKuzYmsOpQIARvcOgrSywcS30T2QlJEPIbTJSkWFGvGH9mNMr0CLxVkXgc4CR7OB1BzthLMnugdgY2XV7rHw5hgQUvVB1ZaUo69saYcMtvTUJlsXrhufbPEZW0RERESNC5MtK3XySh6Op+XCTirBUz1vJFIO9lKEB90YtqZSqZBxsvoHOzdkQU5C/297qQ3eebADnGW2iNl/ERuPXmlwyZayWFvZ8nC8pbJVi2Qrr0Q7jJDJFhEREVHjwGGEVmrNwUsAgMhQP3g1wDlMdyvAWdtdEQCGdPKBu5O9vqX9X2ezoNaIGvauf8pbK1s3JVu6KuOd5JeyskVERETUmDDZskJ5xSr8nnAVADC2d5CFozEPufRGV8LRlefYLdANrnJb5BarkFDZAKSh0FW23CorW0EejrCRAEXlamQVlBl1DFa2iIiIiBoXJltWaMU/KShRqdHe1wW9WnpYOhyz+fKpLlj/fB/0ba1t7mErtcF97ZoBAPYmZ1oytCqURYaVLXtbGzSvbLWfYuRQQrZ+JyIiImpcmGxZmdxiFVbuSwEAvDqwrdXNxaqNAHcHRLT2NFjWvzLZ2p2cZYmQbktX2dK1fgeAll7OAIzvSKgbRsjW70RERESNA5MtK7Pyn4soLKtABz9XDO3UMB/wa079QrTJVuKVPKOH59UH3UONda3fAaClpyMA4zsSchghERERUePCZMuKFKmA7w9q272/NqgtbGwab1Xrdrxd5Ahtrp3L9deZhlPdurVBBlD7joQFbJBBRERE1Kgw2bIiFwokKC5Xo1UzJwzu6GPpcCxG1/Z984mrFo5Eq0KtQV5louRhMIxQl2wVG3UcVraIiIiIGhcmW1akUHs/jxaeTo16rtadPB4eAIkE2JOchfNZhZYOB3klKui6u9+cKOmSrZTsoju2qtcIoLCMyRYRERFRY8Jky4oUa+/F4dbEb8ZbeDlhYHttdev7/RctGwxuNMdwlArYSm98pJq7OcBFZovyCo3+uWi3U1nUAsAGGURERESNBZMtK1Jcoa1mKRx5Mz7xnpYAgJ/jL+tbpltKTmXb91t/LLZSG7w5NAQAMH9rEi7UUIXTJdJO9lLYSfmxJCIiImoMbC0dABmvqPKG/OYmDE1VRGtPhPi4IPlaASbGxMHXVQ4A0Gg0SM+wQWz+cXQLdscL97c2+fc+dTUPi3edQ1mFGgCQXaStbDlX82mK7hOM7acz8M+5bLzx03Gsf74P5HbSKtsVaw/FqhYRERFRI8Jky4rohxGysgWJRIJn72uJt34+gfhLylvW2iAh+xq2nrqGR7o2h3dlImYqK/5OQeypjCrLm8mrzsuysZHgv090wdBFf+FYai7GrTyM5dE9qlQnS3RVSyZbRERERI0Gky0roku2eEOu9UR4AOR2UigrK0sAoFarcerUKcSm26OoTI2c4nKTJ1tX80oAaKtWYQEKAIAEAqpLx6rd3t/NAcvGdceL/4vH4ZQcPPjl3wjycNSvF0KDSxnaoYOsbBERERE1Hky2rEhRZfXDjcMIAWirRiO6+BssU6lU+CPnJI7ky1BUVoyC0orb7F13mfnahykPD/NDn1aeN77vleqTLQDo29oLP70cgQkr43AltwRXcktu2UL7s21V2cGQiIiIiKwfky0rUqyfs8Xqx524yLVvbd2Dgk3pWn4pAMCnlhWz9r6u2Dbtfvxz/rpBK3i1Wo1jx46hV49w9Gvva9JYiYiIiMhymGxZkRut31nZupMbyZZpK1uFZRUoKtd2s/B2kdV6f4WjHR7s7GewTKVSQZImMKSjD+zs+JEkIiIiaizYY9pKlKnUKNew9buxnGXapCXfxG3hdVUtF5ktnGRMjIiIiIjo9phsWYncyqRBaiOBq5w3+Xeiq2zlm7iypUu2vF1rX9UiIiIioqaFyZaV0D2411VuC4lEYuFoGj5XMw0j1DXHqO18LSIiIiJqephsWQllsTbZcmNrcKOYq0FGXZtjEBEREVHTw2TLSugqW3ygsXFc5NrrZOrK1rXKyhaHERIRERHRnTDZshK6ZIsPNDaOvkGGmSpbvqxsEREREdEdMNmyEhxGWDvmav3OYYREREREZCwmW1aCwwhrx9Vcc7YKdMkWhxESERERUc2YbFkJDiOsHXNUtoQQN+ZsubCyRUREREQ1Y7JlJfTDCFnZMoo5kq28EhXKKzQA2CCDiIiIiO6MyZaVYGWrdlwqG2QUllVArREmOaauquXuaAeZrdQkxyQiIiKixovJlpXIY2WrVpzlN65ToYmqW2yOQURERES1wWTLSihL2I2wNmS2NpDZat/epmr/rku2vJlsEREREZERmGxZCQ4jrD1TP9g4s0A7jNDHhfO1iIiIiOjOmGxZgVKVGqUqbWMGdw4jNJqp279zGCERERER1QaTLSuQWzlfywYCzpWNH+jOdB0J801U2crI4zO2iIiIiMh4TLasQG5JOQDAwRaQSCQWjsZ6uDrohhGaprKVpiwBAPi7OZjkeERERETUuDHZsgK6ypYTi1q1Yspnbak1AheyCgEArZs53/XxiIiIiKjx4+17A5VZUAqVWvt8qJTrRQAAR/60asVFZrrK1tXcEpRVaGAvtUGgh+NdH4+IiIiIGj/evjdAX+0+h/9uS66y3NHWNA/nbSpMWdk6l6mtarX0coLUhkM5iYiIiOjOmGw1QHvPZAEAbG0ksKm8sbeX2qCbZ5klw7I6utbvpnjOli7ZauPNIYREREREZByLztn6+uuvERYWBldXV7i6uiIiIgJbt27Vry8sLMQrr7yCgIAAODg4oEOHDvj6668NjlFWVoYpU6bAy8sLTk5OGDFiBC5fvmywjVKpRHR0NBQKBRQKBaKjo5Gbm1sfp1gnaTnFAICfXorAmXnDcGbeMBz7zwPo5c3KVm24OpiuG6Eu2WrNZIuIiIiIjGTRZCsgIAAff/wxjhw5giNHjuCBBx7Aww8/jFOnTgEAXnvtNcTGxmLNmjVISkrCa6+9hilTpuD333/XH2PatGnYuHEjNmzYgH379qGwsBBRUVFQq9X6bUaPHo2EhATExsYiNjYWCQkJiI6OrvfzNUapSo2Myuc5BXFu0F0x5UONz2exskVEREREtWPRYYQPPfSQwesPP/wQX3/9NQ4ePIhOnTrhwIEDGD9+PPr37w8AeOGFF7Bs2TIcOXIEDz/8MPLy8rBixQqsXr0agwYNAgCsWbMGgYGB2LlzJ4YOHYqkpCTExsbi4MGD6N27NwBg+fLliIiIQHJyMkJCQur1nO/ksrIYQgDOMlt4ONlbOhyr5mKihxoLIXBO34nQ6a7jIiIiIqKmocG0fler1diwYQOKiooQEREBALj33nuxadMmXLlyBUII7N69G2fOnMHQoUMBAPHx8VCpVBgyZIj+OP7+/ggNDcX+/fsBAAcOHIBCodAnWgDQp08fKBQK/TbVKSsrQ35+vsFXfUitHEIY5OHIZ2rdpbo0yKjQAE9/dxjPrDqMojLtftlF5cgtVkEiYdt3IiIiIjKexZOtxMREODs7QyaT4aWXXsLGjRvRsWNHAMCXX36Jjh07IiAgAPb29oiMjMTSpUtx7733AgAyMjJgb28Pd3d3g2P6+PggIyNDv423t3eV7+vt7a3fpjrz58/Xz/FSKBQIDAw01SnX6FL2jWSL7o6rrkFGifGVrbN5Ehy5lIvdyVl47vsjKFWp9fO1AtwdILeTmiVWIiIiImp8LJ5shYSEICEhAQcPHsTLL7+M8ePH4/Tp0wC0ydbBgwexadMmxMfH47PPPsOkSZOwc+fOGo8phDCoClVXIbp1m1vNmjULeXl5+q+0tLQ6nmHt6CpbwZ5Mtu6W601zttYeuoSHFu/DvrPXa9zndO6N98SBC9l4aU08ktK1Vc02rGoRERERUS1YvPW7vb092rRpAwDo0aMH4uLi8MUXX+Dzzz/H22+/jY0bN2L48OEAgLCwMCQkJODTTz/FoEGD4Ovri/LyciiVSoPqVmZmJvr27QsA8PX1xbVr16p836ysLPj4+Nw2LplMBplMZspTNUqqrrLFZOuu6YYRlqjUmLv5NMorNBi/6jDmjOiE6D7BVbYXQuCUUptsvXh/K3x/4CL2JGfh4IVsABxCSERERES1Y/HK1q2EECgrK4NKpYJKpYKNjWGIUqkUGo0GANC9e3fY2dlhx44d+vXp6ek4efKkPtmKiIhAXl4eDh8+rN/m0KFDyMvL02/TkFzK4TBCU3GW3/hbQnmFBs4yW6g1Au/+dhLrD6dW2f5idjGyyySwk0owdWBbLB/XA/ZSG5SqtO83diIkIiIiotqwaGXr7bffxrBhwxAYGIiCggJs2LABe/bsQWxsLFxdXdGvXz9Mnz4dDg4OCA4Oxt69e/G///0PCxcuBAAoFAo8++yzeOONN+Dp6QkPDw+8+eab6Ny5s747YYcOHRAZGYnnn38ey5YtA6DtahgVFVWnToQXrxehQmO6a3AzjUbon7EV7MGud3fLTmoDBzspSlTaxwDEPNMTu5Mz8dXu85j/RxIGd/SBl/ON6uWeM9ohhj1buMNJZov72jbD0jHheGlNPCo0Au39XC1yHkRERERknSyabF27dg3R0dFIT0+HQqFAWFgYYmNjMXjwYADAhg0bMGvWLIwZMwY5OTkIDg7Ghx9+iJdeekl/jEWLFsHW1hYjR45ESUkJBg4ciJiYGEilNxoZrF27FlOnTtV3LRwxYgSWLFlSp5gf/eYQgj0cMCLqLk78NjILylBWoYHURgJ/N7npv0ET5OpgixKVGgPbe6NHCw90C3LHnuQsnLqajwVb/8V/n+yi33ZvZbLVv10z/bJBHX2w7vk+OJtZgC4BinqPn4iIiIisl0WTrRUrVtS43tfXF6tWrapxG7lcjsWLF2Px4sW33cbDwwNr1qypU4zVSSuS4EpuCVo0szPZMQHgUnYRAKC5mwNspQ1uhKdVCg9yx76z1/FWZHsAgNRGgg8eCcVjS/fjp/jLKKvQwFaqnad1+GIOAKB/Oy+DY/Rq6YFeLT3qN3AiIiIisnoWb5BhbUL9XXE6uwJxF5Vo0cy0w8rYidD0vhodjqLyCrjIbyTG4UHuGNUzEBvi0rDp+FWD7b3lAi14/YmIiIjIBJhs1VJ4sBtOZ19H3EUlnuxZtaPd3UhlcwyTs7GRGCRaOrMf6oRO/q76+VwAoNFoYJNxmg+TJiIiIiKTYLJVS+FBblhzVJtsmYqyqByn0/NxOEU7jI3Jlvk52EsRHdHCYJlKpcIff5y2TEBERERE1Ogw2aqlbgEKSCCQkl2MzIJSeLvUvZHFgfPZWPbXeew7ex0VGqFf3tKLnQiJiIiIiKwdk61acnWwg78jcKUYiEtRYniYX52Oc72wDONXHUZ5ZR/5ll5OCHB3QAc/V/QP8TZlyEREREREZAFMtuqgtavAlWIJDqdk1znZ2n7qGsorNGjr7Yxl0d3RqhkfmEtERERE1Jiwv3gdtHbVDvk7VDnHqi62nkwHADwa3pyJFhERERFRI8Rkqw5auWiTrX8zCrD6wMVa768sLsf+89kAgGGhdauMERERERFRw8ZhhHXgag88e08wVvxzCe/+fgqnrubDRW6LwrKKGvcTQkBRIEFRUhbUGoEOfq5shkFERERE1Egx2aqjGUPbwVlujy92ncWGuLRa7CmFe+oZAMDwzr7mCY6IiIiIiCyOyVYdSSQSvDa4Hdr5uOCvM1lQONrBRWaLmp6Heym7CD/FX4GyWAUAGNaZQwiJiIiIiBorJlt3aXiYn9EdCVUqFdyLUvFLmgM6NVegNRtjEBERERE1Wky26lknd4HXn+4Hmb2dpUMhIiIiIiIzYjdCC5DaSCCpabwhERERERFZPSZbREREREREZsBki4iIiIiIyAyYbBEREREREZkBky0iIiIiIiIzYLJFRERERERkBky2iIiIiIiIzIDJFhERERERkRkw2SIiIiIiIjIDJltERERERERmwGSLiIiIiIjIDJhsERERERERmQGTLSIiIiIiIjNgskVERERERGQGTLaIiIiIiIjMwNbSAVgLIQQAoKCgAMXFxcjPz4ednV2tjqFSqep9X0t8T+7Ln21D2/du8OfTOPe1tni5L3+2jWVfa4u3qe1rbfFact/8/HwAN3KE22GyZaTs7GwAQMuWLS0cCRERERERNQTZ2dlQKBS3Xc9ky0geHh4AgNTUVAwaNAhxcXF1Ok7Pnj3rZd/8/HwEBgYiLS0NAwcObPDxmmvfm6+Dq6trvX3f+tr3TvvVdP7Wdq53s29t3wem+r4N7ToZcx0aWsym2re6c2/I8Zpr3127dtXps2Bt58vPQM0sfY/QEK5TY78/uHXf2pxvQ4jXXPua8ndBXl4egoKC9DnC7TDZMpKNjXZ6m0KhgFQqrdMNG4B639fV1dWq4jXXvq6urkYdryHFbMr9qjt/azvXu90XMP59YKrv21CvU03XoaHGbKp9bz53a4jXXPvW9rPQEGI25X5N+TNwM0vdIzSk69RY7w9ut68x59uQ4jXXvqb6XQDcyBFuu77OR27CJk+ebFX7Wlu83Ldhf09r3fdu8OfTOPe1tni5b8P+nty3YX9P7tuwv6e17msMibjTrC4CoC07KhQK5OXl3VX2W1+sLV5zaerXoamfvw6vg1ZTvg5N+dxv1tSvQ1M/fx1eh6Z3DZra+d6OKa+DscdiZctIMpkMs2fPhkwms3QoRrG2eM2lqV+Hpn7+OrwOWk35OjTlc79ZU78OTf38dXgdmt41aGrnezumvA7GHouVLSIiIiIiIjNgZYuIiIiIiMgMmGwRERERERGZAZMtIiIiIiIiM2CyRdQESSQS/Pbbb5YOg4iIqMHg70YyByZbVmLChAmQSCR46aWXqqybNGkSJBIJJkyYUP+BWdiECRPwyCOPWDoMi2uq12H//v2QSqWIjIy0dCgWk5mZiRdffBFBQUGQyWTw9fXF0KFDceDAAUuHVq/S0tLw7LPPwt/fH/b29ggODsarr76K7Oxso/bfs2cPJBIJcnNzzRuoGeh+P3z88ccGy3/77TdIJBILRVV/dOcvkUhgZ2cHHx8fDB48GCtXroRGo7F0ePWuqf4+0Ln5/XDz17lz5ywdmsnx3rCqhnhfwGTLigQGBmLDhg0oKSnRLystLcX69esRFBRkwciILGPlypWYMmUK9u3bh9TUVEuHYxGPP/44jh8/ju+//x5nzpzBpk2b0L9/f+Tk5Fg6tHpz4cIF9OjRA2fOnMH69etx7tw5fPPNN9i1axciIiKaxLWQy+VYsGABlEqlpUOxiMjISKSnp+PixYvYunUrBgwYgFdffRVRUVGoqKiwdHhUz3Tvh5u/WrZsaemwzMKc94Yqlepuw6t3prwvUKvVJvmDDZMtKxIeHo6goCD8+uuv+mW//vorAgMD0a1bN/2y2NhY3HvvvXBzc4OnpyeioqJw/vx5/foHHngAr7zyisGxs7OzIZPJ8Oeff5r/RMykRYsW+Pzzzw2Wde3aFXPmzNG/lkgk+O677/Doo4/C0dERbdu2xaZNm+o3UDMz5jo0BkVFRfjxxx/x8ssvIyoqCjExMfp1MTExcHNzM9i+ur/yz5s3D97e3nBxccFzzz2HmTNnomvXruYP3kRyc3Oxb98+LFiwAAMGDEBwcDB69eqFWbNmYfjw4QCAvLw8vPDCC/D29oarqyseeOABHD9+XH+MOXPmoGvXrli2bBkCAwPh6OiIJ5980qoqPJMnT4a9vT22b9+Ofv36ISgoCMOGDcPOnTtx5coVvPPOOwCAsrIyvPXWWwgMDIRMJkPbtm2xYsUKXLx4EQMGDAAAuLu7W+VfgwcNGgRfX1/Mnz//ttv88ssv6NSpE2QyGVq0aIHPPvtMv27WrFno06dPlX3CwsIwe/Zss8RsSrqqbvPmzREeHo63334bv//+O7Zu3ar/v+FOnwUA2LRpE3r06AG5XA4vLy889thjFjgb07nT/cDFixchkUjw66+/YsCAAXB0dESXLl2svjKuez/c/CWVSrF582Z0794dcrkcrVq1wty5c6sk4+np6Rg2bBgcHBzQsmVL/PTTTxY6C+OY6t5Q91748ccf0b9/f8jlcqxZs6Zez+Vu1XRfoBu98H//93/o0qUL5HI5evfujcTERP02unuHLVu2oGPHjpDJZLh06dJdx8Vky8o888wzWLVqlf71ypUrMXHiRINtioqK8PrrryMuLg67du2CjY0NHn30UX12/txzz2HdunUoKyvT77N27Vr4+/vrbzgas7lz52LkyJE4ceIEHnzwQYwZM6ZJ/OW7sfnhhx8QEhKCkJAQjB07FqtWrUJtHhu4du1afPjhh1iwYAHi4+MRFBSEr7/+2owRm56zszOcnZ3x22+/GXyedYQQGD58ODIyMvDHH38gPj4e4eHhGDhwoMF7/ty5c/jxxx+xefNmxMbGIiEhAZMnT67PU6mznJwcbNu2DZMmTYKDg4PBOl9fX4wZMwY//PADhBAYN24cNmzYgC+//BJJSUn45ptv4OzsjMDAQPzyyy8AgOTkZKSnp+OLL76wxOnUmVQqxUcffYTFixfj8uXLVdbHx8dj5MiRGDVqFBITEzFnzhy8++67+puRMWPG4NChQwY3X6dOnUJiYiLGjBlTX6dhUg888AC6dOmCX3/91ajPwv/93//hsccew/Dhw3Hs2DHs2rULPXr0sPBZ3J073Q/ovPPOO3jzzTeRkJCAdu3a4emnn250FcFt27Zh7NixmDp1Kk6fPo1ly5YhJiYGH374ocF27777rn7EwNixY/H0008jKSnJQlEbxxT3hjozZszA1KlTkZSUhKFDh9ZL/KZizH3B9OnT8emnnyIuLg7e3t4YMWKEQQWvuLgY8+fPx3fffYdTp07B29v77gMTZBXGjx8vHn74YZGVlSVkMplISUkRFy9eFHK5XGRlZYmHH35YjB8/vtp9MzMzBQCRmJgohBCitLRUeHh4iB9++EG/TdeuXcWcOXPq41RMSnddhBAiODhYLFq0yGB9ly5dxOzZs/WvAYj//Oc/+teFhYVCIpGIrVu31kO05lOX67Bx48Z6i88c+vbtKz7//HMhhBAqlUp4eXmJHTt2CCGEWLVqlVAoFAbbb9y4Udz8X17v3r3F5MmTDba55557RJcuXcwat6n9/PPPwt3dXcjlctG3b18xa9Yscfz4cSGEELt27RKurq6itLTUYJ/WrVuLZcuWCSGEmD17tpBKpSItLU2/fuvWrcLGxkakp6fX34nU0cGDB2t8Py9cuFAAEIcOHRIA9O+RW+3evVsAEEql0nzBmsnNn/8+ffqIiRMnCiEM3/OjR48WgwcPNthv+vTpomPHjvrXYWFh4v3339e/njVrlujZs6eZo797N5//rZ566inRoUMHoz4LERERYsyYMeYO1+xquh633g+kpKQIAOK7777Tb3Pq1CkBQCQlJdVHuCY3fvx4IZVKhZOTk/7riSeeEPfdd5/46KOPDLZdvXq18PPz078GIF566SWDbXr37i1efvnleom9tkx5b6h7L+h+r1qjmu4LdP/Hb9iwQb99dna2cHBw0N8Pr1q1SgAQCQkJJo2LlS0r4+XlheHDh+P777/HqlWrMHz4cHh5eRlsc/78eYwePRqtWrWCq6urfpyybuyqTCbD2LFjsXLlSgBAQkICjh8/bnXDZuoqLCxM/28nJye4uLggMzPTghFRbSUnJ+Pw4cMYNWoUAMDW1hZPPfWU/j1t7DF69eplsOzW19bg8ccfx9WrV7Fp0yYMHToUe/bsQXh4OGJiYhAfH4/CwkJ4enrqq2DOzs5ISUkxqGAEBQUhICBA/zoiIgIajQbJycmWOCWTEpV/1UxJSYFUKkW/fv0sHJF5LViwAN9//z1Onz5tsDwpKQn33HOPwbJ77rkHZ8+ehVqtBqCtbq1duxaA9rqtX7/eaqtaOkIISCQSoz4LCQkJGDhwoIUjNq073Q/o3Px70c/PDwCs+vfigAEDkJCQoP/68ssvER8fj/fff9/g5//8888jPT0dxcXF+n0jIiIMjhUREdHgK1umuDfUsdZqrrH3BTf/fD08PBASEmLw87W3tzf4PJiCrUmPRvVi4sSJ+jlXX331VZX1Dz30EAIDA7F8+XL4+/tDo9EgNDQU5eXl+m2ee+45dO3aFZcvX8bKlSsxcOBABAcH19s5mIONjU2VcnF1kzvt7OwMXkskkkbVscrY62DNVqxYgYqKCjRv3ly/TAgBOzs7KJVKo6/BrXO4bt3HWsjlcgwePBiDBw/Ge++9h+eeew6zZ8/GpEmT4Ofnhz179lTZ59Y5bTfTXRdr6GTXpk0bSCQSnD59utoObP/++y/c3d3h6OhY/8FZwP3334+hQ4fi7bffNvgDmi7puNmt7/fRo0dj5syZOHr0KEpKSpCWlqa/cbFWSUlJaNmyJTQazR0/C7cOQ20MjLkfAAx/L+reJ9b8e9HJyQlt2rQxWKbRaDB37txq5+HJ5fIaj2cN/xea4t4Q0F47a3Sn+4Ka3PzzdXBwMPnPm8mWFYqMjNR/OG4dT5udnY2kpCQsW7YM9913HwBg3759VY7RuXNn9OjRA8uXL8e6deuwePFi8wduZs2aNUN6err+dX5+PlJSUiwYkWU09utQUVGB//3vf/jss88wZMgQg3WPP/441q5di9atW6OgoABFRUX6XxwJCQkG24aEhODw4cOIjo7WLzty5IjZ468PHTt2xG+//Ybw8HBkZGTA1tYWLVq0uO32qampuHr1Kvz9/QEABw4cgI2NDdq1a1dPEdedp6cnBg8ejKVLl+K1114zuGHOyMjA2rVrMW7cOHTu3BkajQZ79+7FoEGDqhzH3t4eAPRVHmv28ccfo2vXrgY/v44dO1b5XbB//360a9cOUqkUABAQEID7778fa9euRUlJCQYNGgQfH596jd2U/vzzTyQmJuK1115DQEDAHT8LYWFh2LVrF5555pn6DdRMjL0faCrCw8ORnJxcJQm71cGDBzFu3DiD1zc3mmioTHFvaK2MuS8IDQ0FoP156ro0KpVKnDlzBu3btzdrfEy2rJBUKtWXPHW/JHXc3d3h6emJb7/9Fn5+fkhNTcXMmTOrPc5zzz2HV155BY6Ojnj00UfNHre5PfDAA4iJicFDDz0Ed3d3vPvuu1WuT1PQ2K/Dli1boFQq8eyzz0KhUBise+KJJ7BixQrs2rULjo6OePvttzFlyhQcPnzYoCsRAEyZMgXPP/88evTogb59++KHH37AiRMn0KpVq3o8m7uTnZ2NJ598EhMnTkRYWBhcXFxw5MgRfPLJJ3j44YcxaNAgRERE4JFHHsGCBQsQEhKCq1ev4o8//sAjjzyiHy4il8sxfvx4fPrpp8jPz8fUqVMxcuRI+Pr6WvgMjbNkyRL07dsXQ4cOxbx589CyZUucOnUK06dPR/PmzfHhhx/Cw8MD48ePx8SJE/Hll1+iS5cuuHTpEjIzMzFy5EgEBwdDIpFgy5YtePDBB+Hg4ABnZ2dLn1qddO7cGWPGjDH4I9obb7yBnj174oMPPsBTTz2FAwcOYMmSJVi6dKnBvmPGjMGcOXNQXl6ORYsW1XfodVZWVoaMjAyo1Wpcu3YNsbGxmD9/PqKiojBu3DjY2Njc8bMwe/ZsDBw4EK1bt8aoUaNQUVGBrVu34q233rL06dVJbe4HmoL33nsPUVFRCAwMxJNPPgkbGxucOHECiYmJmDdvnn67n376CT169MC9996LtWvX4vDhw1ixYoUFIzeOqe4NrZEx9wW6/8/ef/99eHp6wsfHB++88w68vLzM/1w6k84AI7OpacKrEMJgEuSOHTtEhw4dhEwmE2FhYWLPnj3VTiAvKCgQjo6OYtKkSeYL3Myio6PF448/LoQQIi8vT4wcOVK4urqKwMBAERMTY1RjCIVCIVatWlV/QZuBKa6DtYiKihIPPvhgtevi4+MFABEfHy82btwo2rRpI+RyuYiKihLffvutuPW/vPfff194eXkJZ2dnMXHiRDF16lTRp0+f+jgNkygtLRUzZ84U4eHhQqFQCEdHRxESEiL+85//iOLiYiGEEPn5+WLKlCnC399f2NnZicDAQDFmzBiRmpoqhNA2yOjSpYtYunSp8Pf3F3K5XDz22GMiJyfHkqdWaxcvXhQTJkwQvr6++vOcMmWKuH79un6bkpIS8dprrwk/Pz9hb28v2rRpI1auXKlf//777wtfX18hkUhuO6m8Iaru98PFixeFTCYzeM///PPPomPHjsLOzk4EBQWJ//73v1WOpVQqhUwmE46OjqKgoMDcoZvE+PHjBQABQNja2opmzZqJQYMGiZUrVwq1Wq3f7k6fBSGE+OWXX0TXrl2Fvb298PLyEo899pglTumu3Pz74E73A7qmCMeOHdPvr1QqBQCxe/fu+g/eBGq6X4qNjRV9+/YVDg4OwtXVVfTq1Ut8++23+vUAxFdffSUGDx4sZDKZCA4OFuvXr6+nyGvPlPeG1b0XrIUx9wWfffaZACA2b94sOnXqJOzt7UXPnj0NmmFU11zLFCRCWOkkBbpraWlpaNGiBeLi4hAeHm7pcOokMjISbdq0wZIlSywdikXxOpjG4MGD4evri9WrV1s6lHozZ84c/Pbbb1WGWRKRdeLvA6Kq9uzZgwEDBkCpVNY4Z9kcOIywCVKpVEhPT8fMmTPRp08fq0y0lEol9u/fjz179uCll16ydDgWw+tQd8XFxfjmm28wdOhQSKVSrF+/Hjt37sSOHTssHRoRUa3x9wFRw8Rkqwn6559/MGDAALRr1w4///yzpcOpk4kTJyIuLg5vvPEGHn74YUuHYzG8DnUnkUjwxx9/YN68eSgrK0NISAh++eWXapsnEBE1dPx9QNQwcRghERERERGRGfChxkRERERERGbAZIuIiIiIiMgMmGwRERERWZH58+ejZ8+ecHFxgbe3Nx555BEkJycbbCOEwJw5c+Dv7w8HBwf0798fp06d0q/PycnBlClTEBISAkdHRwQFBWHq1KnIy8szOM6IESMQFBQEuVwOPz8/REdH4+rVq/VynkSNAZMtIiIiIiuyd+9eTJ48GQcPHsSOHTtQUVGBIUOGoKioSL/NJ598goULF2LJkiWIi4uDr68vBg8ejIKCAgDA1atXcfXqVXz66adITExETEwMYmNj8eyzzxp8rwEDBuDHH39EcnIyfvnlF5w/fx5PPPFEvZ4vkTVjgwwiIiIiK5aVlQVvb2/s3bsX999/P4QQ8Pf3x7Rp0zBjxgwAQFlZGXx8fLBgwQK8+OKL1R7np59+wtixY1FUVARb2+obVm/atAmPPPIIysrKYGdnZ7ZzImosWNkiIiIismK6oX8eHh4AgJSUFGRkZGDIkCH6bWQyGfr164f9+/fXeBxXV9fbJlo5OTlYu3Yt+vbty0SLyEhMtoiIiIislBACr7/+Ou69916EhoYCADIyMgAAPj4+Btv6+Pjo190qOzsbH3zwQbVVrxkzZsDJyQmenp5ITU3F77//buKzIGq8mGwRERERWalXXnkFJ06cwPr166usk0gkBq+FEFWWAUB+fj6GDx+Ojh07Yvbs2VXWT58+HceOHcP27dshlUoxbtw4cBYKkXGqrxMTERERUYM2ZcoUbNq0CX/99RcCAgL0y319fQFoK1x+fn765ZmZmVWqXQUFBYiMjISzszM2btxY7fBALy8veHl5oV27dujQoQMCAwNx8OBBREREmOnMiBoPVraIiIiIrIgQAq+88gp+/fVX/Pnnn2jZsqXB+pYtW8LX1xc7duzQLysvL8fevXvRt29f/bL8/HwMGTIE9vb22LRpE+RyuVHfG9A23CCiO2Nli4iIiMiKTJ48GevWrcPvv/8OFxcX/TwshUIBBwcHSCQSTJs2DR999BHatm2Ltm3b4qOPPoKjoyNGjx4NQFvRGjJkCIqLi7FmzRrk5+cjPz8fANCsWTNIpVIcPnwYhw8fxr333gt3d3dcuHAB7733Hlq3bs2qFpGR2PqdiIiIyIpUN+8KAFatWoUJEyYA0Fag5s6di2XLlkGpVKJ379746quv9E009uzZgwEDBlR7nJSUFLRo0QKJiYl49dVXcfz4cRQVFcHPzw+RkZH4z3/+g+bNm5vl3IgaGyZbREREREREZsA5W0RERERERGbAZIuIiIiIiMgMmGwRERERERGZAZMtIiIiIiIiM2CyRUREREREZAZMtoiIiIiIiMyAyRYREREREZEZMNkiIiIiIiIyAyZbREREdRQTEwM3NzdLh0FERA0Uky0iImr0JkyYAIlEUuXr3Llzlg6NiIgaMVtLB0BERFQfIiMjsWrVKoNlzZo1s1A0RETUFLCyRURETYJMJoOvr6/Bl1QqxebNm9G9e3fI5XK0atUKc+fORUVFhX6/3NxcvPDCC/Dx8YFcLkdoaCi2bNlicOxt27ahQ4cOcHZ2RmRkJNLT0/Xr4uLiMHjwYHh5eUGhUKBfv344evRovZ03ERFZDpMtIiJqsrZt24axY8di6tSpOH36NJYtW4aYmBh8+OGHAACNRoNhw4Zh//79WLNmDU6fPo2PP/4YUqlUf4zi4mJ8+umnWL16Nf766y+kpqbizTff1K8vKCjA+PHj8ffff+PgwYNo27YtHnzwQRQUFNT7+RIRUf2SCCGEpYMgIiIypwkTJmDNmjWQy+X6ZcOGDcO1a9cwbNgwzJo1S798zZo1eOutt3D16lVs374dw4YNQ1JSEtq1a1fluDExMXjmmWdw7tw5tG7dGgCwdOlSvP/++8jIyKg2FrVaDXd3d6xbtw5RUVEmPlMiImpIOGeLiIiahAEDBuDrr7/Wv3ZyckKbNm0QFxenr2QB2mSotLQUxcXFSEhIQEBAQLWJlo6jo6M+0QIAPz8/ZGZm6l9nZmbivffew59//olr165BrVajuLgYqampJj5DIiJqaJhsERFRk6BLrm6m0Wgwd+5cPPbYY1W2l8vlcHBwuONx7ezsDF5LJBLcPGhkwoQJyMrKwueff47g4GDIZDJERESgvLy8jmdCRETWgskWERE1WeHh4UhOTq6ShOmEhYXh8uXLOHPmTI3VrZr8/fffWLp0KR588EEAQFpaGq5fv17nmImIyHow2SIioibrvffeQ1RUFAIDA/Hkk0/CxsYGJ06cQGJiIubNm4d+/frh/vvvx+OPP46FCxeiTZs2+PfffyGRSBAZGWnU92jTpg1Wr16NHj16ID8/H9OnTzeqYkZERNaP3QiJiKjJGjp0KLZs2YIdO3agZ8+e6NOnDxYuXIjg4GD9Nr/88gt69uyJp59+Gh07dsRbb70FtVpt9PdYuXIllEolunXrhujoaEydOhXe3t7mOB0iImpg2I2QiIiIiIjIDFjZIiIiIiIiMgMmW0RERERERGbAZIuIiIiIiMgMmGwRERERERGZAZMtIiIiIiIiM2CyRUREREREZAZMtoiIiIiIiMyAyRYREREREZEZMNkiIiIiIiIyAyZbREREREREZsBki4iIiIiIyAz+HwExvhuAvmS9AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "timeseries.plot(ax=ax)\n",
    "ax.set_title(\"Comportamiento de la TRM del dolar 10 Abril 2022 - 10 Abril 2023\")\n",
    "ax.set_xlabel(\"Fecha\")\n",
    "ax.set_ylabel(\"TRM (Pesos Colombianos)\")\n",
    "ax.grid(which=\"minor\", axis=\"x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f579714-0af3-42c2-abaa-fcc5c22472fd",
   "metadata": {},
   "source": [
    "#### Partición Train y Test\n",
    "\n",
    "El modelo de pronóstico busca realizar la predicción del valor de la TRM del dolar para el siguiente día, aplicando el algoritmo DeepAR, teniendo en cuenta que el valor a predecir solo consiste en uno (1), que corresponde a la TRM del siguiente día, la cantidad de datos en test será pequeña (2 semanas) y busca hacer una validación walk-forward del modelo, adicionalmente se considera que de acuerdo con las recomendaciones de AWS para utilizar este algortimo se debe utilizar como minimo 300 datos, por tanto este es el tamaño minimo del paquete de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4245f80c-520d-4be1-8b49-29d9ea094542",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# usamos una frecuencia de 1 día para la serie de tiempo\n",
    "freq = \"1D\"\n",
    "\n",
    "# predecimos 1 día\n",
    "prediction_length = 1\n",
    "\n",
    "# también usamos 7 días como duración del contexto, esta es la cantidad de actualizaciones de estado realizadas antes de hacer predicciones\n",
    "context_length = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b1dbe0a7-8ddb-41c5-a3c3-28a9e9a251f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "start_dataset = pd.Timestamp(\"2022-04-10 00:00:00\") # Fecha inicial del dataset\n",
    "end_training = pd.Timestamp(\"2023-03-27 00:00:00\") # Fecha final del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d677c1c-7cd8-40b2-9561-5cf2a7388592",
   "metadata": {},
   "source": [
    "Se crea un archivo json de entrenamiento de acuerdo con la forma en la que lo entiende el algoritmo DeepAR de AWS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e743894e-a0bd-4edc-8e83-0f9fa86c3bc1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "training_data = [{\n",
    "    \"start\": str(start_dataset),\n",
    "    \"target\": timeseries[start_dataset : end_training].tolist()\n",
    "}]\n",
    "print(len(training_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dc5b7f-10c0-4cd3-832c-486f977b383b",
   "metadata": {},
   "source": [
    "Los datos de prueba se usarán para calcular las metricas de prueba, usando el modelo entrenado para pronosticar 1 día a las vez y comparar las predicciones con los valores reales. Para evaluar el rendimiento de nuestro modelo en más de un (1) día, generamos datos de prueba que se extienden a 15 días más allá del rango de entrenamiento. De esta manera realizamos una evaluación continua de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "76817d19-34dd-4944-8413-fafc59a4b0e8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "num_test_windows = 14\n",
    "\n",
    "test_data = [{\n",
    "        \"start\": str(start_dataset),\n",
    "        \"target\": timeseries[start_dataset : end_training + timedelta(days=k * prediction_length)].tolist(),\n",
    "    }\n",
    "    for k in range(1, num_test_windows + 1)]\n",
    "\n",
    "print(len(test_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961b5b22-1c6b-43dc-872e-7620e668f250",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Escritura de archivos json y envío a s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bc4c0240-2322-450f-970a-27a02b490e7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def write_dicts_to_file(path, data):\n",
    "    with open(path, \"wb\") as fp:\n",
    "        for d in data:\n",
    "            fp.write(json.dumps(d).encode(\"utf-8\"))\n",
    "            fp.write(\"\\n\".encode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a800c311-f69e-4f0b-b677-54ba314b40cb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.86 ms, sys: 41 µs, total: 3.9 ms\n",
      "Wall time: 4.78 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "write_dicts_to_file(\"train.json\", training_data)\n",
    "write_dicts_to_file(\"test.json\", test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "924d1f5c-4016-44ae-ac0c-740967653e17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Envío de archivos a S3\n",
    "\n",
    "s3 = boto3.resource(\"s3\")\n",
    "\n",
    "\n",
    "def copy_to_s3(local_file, s3_path, override=False):\n",
    "    assert s3_path.startswith(\"s3://\")\n",
    "    split = s3_path.split(\"/\")\n",
    "    bucket = split[2]\n",
    "    path = \"/\".join(split[3:])\n",
    "    buk = s3.Bucket(bucket)\n",
    "\n",
    "    if len(list(buk.objects.filter(Prefix=path))) > 0:\n",
    "        if not override:\n",
    "            print(\n",
    "                \"File s3://{}/{} already exists.\\nSet override to upload anyway.\\n\".format(\n",
    "                    s3_bucket, s3_path\n",
    "                )\n",
    "            )\n",
    "            return\n",
    "        else:\n",
    "            print(\"Overwriting existing file\")\n",
    "    with open(local_file, \"rb\") as data:\n",
    "        print(\"Uploading file to {}\".format(s3_path))\n",
    "        buk.put_object(Key=path, Body=data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "97d0b94f-1fe3-4047-9b31-220b19c4c7f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading file to s3://pronostico-dolar-04-2023/deepar-dolar-notebook/data/train/train.json\n",
      "Uploading file to s3://pronostico-dolar-04-2023/deepar-dolar-notebook/data/test/test.json\n",
      "CPU times: user 66.8 ms, sys: 0 ns, total: 66.8 ms\n",
      "Wall time: 328 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "copy_to_s3(\"train.json\", s3_data_path + \"/train/train.json\")\n",
    "copy_to_s3(\"test.json\", s3_data_path + \"/test/test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8478f45-8ff1-442a-a8ea-7750a0389b18",
   "metadata": {},
   "source": [
    "### Modelo de entrenamiento "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6431e703-dafd-4e78-97fa-69c77db0afe2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator = sagemaker.estimator.Estimator(\n",
    "    image_uri=image_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m4.xlarge\",\n",
    "    base_job_name=\"deepar-dolar\",\n",
    "    output_path=s3_output_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "5ca5db76-b0ee-40a1-a5ae-200fb7c94c82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    \"time_freq\": freq,\n",
    "    \"epochs\": \"400\",\n",
    "    \"early_stopping_patience\": \"40\",\n",
    "    \"mini_batch_size\": \"64\",\n",
    "    \"learning_rate\": \"5E-4\",\n",
    "    \"context_length\": str(context_length),\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "510210d5-86fb-4c0a-811c-faa34ebc3f2e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "70549070-e582-4736-9c51-b3b28ecfef5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: deepar-dolar-2023-04-09-23-05-44-553\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-09 23:05:44 Starting - Starting the training job...\n",
      "2023-04-09 23:06:14 Starting - Preparing the instances for training.........\n",
      "2023-04-09 23:07:31 Downloading - Downloading input data...\n",
      "2023-04-09 23:07:56 Training - Downloading the training image......\n",
      "2023-04-09 23:08:52 Training - Training image download completed. Training in progress..\u001b[34mArguments: train\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/mxnet/model.py:78: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if num_device is 1 and 'dist' not in kvstore:\u001b[0m\n",
      "\u001b[34m/opt/amazon/lib/python3.8/site-packages/jsonref.py:8: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3, and in 3.10 it will stop working\n",
      "  from collections import Mapping, MutableMapping, Sequence\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Reading default configuration from /opt/amazon/lib/python3.8/site-packages/algorithm/resources/default-input.json: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '', 'embedding_dimension': '10', 'learning_rate': '0.001', 'likelihood': 'student-t', 'mini_batch_size': '128', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]'}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Merging with provided configuration from /opt/ml/input/config/hyperparameters.json: {'context_length': '7', 'early_stopping_patience': '40', 'epochs': '400', 'learning_rate': '5E-4', 'mini_batch_size': '64', 'prediction_length': '1', 'time_freq': '1D'}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Final configuration: {'_kvstore': 'auto', '_num_gpus': 'auto', '_num_kv_servers': 'auto', '_tuning_objective_metric': '', 'cardinality': 'auto', 'dropout_rate': '0.10', 'early_stopping_patience': '40', 'embedding_dimension': '10', 'learning_rate': '5E-4', 'likelihood': 'student-t', 'mini_batch_size': '64', 'num_cells': '40', 'num_dynamic_feat': 'auto', 'num_eval_samples': '100', 'num_layers': '2', 'test_quantiles': '[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', 'context_length': '7', 'epochs': '400', 'prediction_length': '1', 'time_freq': '1D'}\u001b[0m\n",
      "\u001b[34mProcess 7 is a worker.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Using early stopping with patience 40\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] random_seed is None\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] [num_dynamic_feat=auto] `dynamic_feat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Training set statistics:\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Real time series\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] number of time series: 1\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] number of observations: 352\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] mean target length: 352.0\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] min/mean/max target: 3731.31005859375/4480.193892045455/5061.2099609375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] mean abs(target): 4480.193892045455\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Small number of time series. Doing 640 passes over dataset with prob 1.0 per epoch.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Test set statistics:\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Real time series\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] number of time series: 14\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] number of observations: 5033\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] mean target length: 359.5\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] min/mean/max target: 3731.31005859375/4483.3617623683685/5061.2099609375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] mean abs(target): 4483.3617623683685\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] contains missing values: no\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #memory_usage::<batchbuffer> = 1.34765625 mb\u001b[0m\n",
      "\u001b[34m/opt/amazon/python3.8/lib/python3.8/subprocess.py:848: RuntimeWarning: line buffering (buffering=1) isn't supported in binary mode, the default buffer size will be used\n",
      "  self.stdout = io.open(c2pread, 'rb', bufsize)\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] nvidia-smi: took 0.030 seconds to run.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] nvidia-smi identified 0 GPUs.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Create Store: local\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081753.3446953, \"EndTime\": 1681081753.3782465, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 32.2718620300293, \"count\": 1, \"min\": 32.2718620300293, \"max\": 32.2718620300293}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #memory_usage::<model> = 3 mb\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081753.3783655, \"EndTime\": 1681081753.419501, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"initialize.time\": {\"sum\": 74.6314525604248, \"count\": 1, \"min\": 74.6314525604248, \"max\": 74.6314525604248}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Epoch[0] Batch[0] avg_epoch_loss=9.498636\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=9.498636245727539\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Epoch[0] Batch[5] avg_epoch_loss=9.374551\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=9.37455145517985\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Epoch[0] Batch [5]#011Speed: 3584.06 samples/sec#011loss=9.374551\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081753.4195826, \"EndTime\": 1681081753.865932, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"epochs\": {\"sum\": 400.0, \"count\": 1, \"min\": 400, \"max\": 400}, \"update.time\": {\"sum\": 446.16127014160156, \"count\": 1, \"min\": 446.16127014160156, \"max\": 446.16127014160156}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1357.4879520356592 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #progress_metric: host=algo-1, completed 0.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=0, train loss <loss>=9.265973854064942\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:13 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_9723c23c-3267-48ab-b584-d53dac840be5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081753.8660705, \"EndTime\": 1681081753.877398, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.47372817993164, \"count\": 1, \"min\": 10.47372817993164, \"max\": 10.47372817993164}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[1] Batch[0] avg_epoch_loss=9.106247\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=9.106246948242188\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[1] Batch[5] avg_epoch_loss=8.954300\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=8.95430040359497\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[1] Batch [5]#011Speed: 4041.53 samples/sec#011loss=8.954300\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[1] Batch[10] avg_epoch_loss=8.827817\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=8.676037788391113\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[1] Batch [10]#011Speed: 3210.49 samples/sec#011loss=8.676038\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] processed a total of 679 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081753.8774962, \"EndTime\": 1681081754.3129473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 435.37020683288574, \"count\": 1, \"min\": 435.37020683288574, \"max\": 435.37020683288574}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1559.0492508119785 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #progress_metric: host=algo-1, completed 0.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=1, train loss <loss>=8.827817396684127\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_10c76659-3f67-4631-bd25-e5879d5ebb6f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081754.3130548, \"EndTime\": 1681081754.3231797, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.430170059204102, \"count\": 1, \"min\": 9.430170059204102, \"max\": 9.430170059204102}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[2] Batch[0] avg_epoch_loss=8.491798\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=8.491798400878906\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[2] Batch[5] avg_epoch_loss=8.292021\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=8.29202127456665\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[2] Batch [5]#011Speed: 4021.44 samples/sec#011loss=8.292021\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[2] Batch[10] avg_epoch_loss=8.113931\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=2, batch=10 train loss <loss>=7.900222110748291\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Epoch[2] Batch [10]#011Speed: 2927.79 samples/sec#011loss=7.900222\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] processed a total of 691 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081754.3232708, \"EndTime\": 1681081754.7683604, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 445.00207901000977, \"count\": 1, \"min\": 445.00207901000977, \"max\": 445.00207901000977}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1552.2603275088707 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #progress_metric: host=algo-1, completed 0.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=2, train loss <loss>=8.113930745558305\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:14 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_f1af587f-d495-4f2d-b77c-5a0bc860f306-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081754.7684622, \"EndTime\": 1681081754.7788491, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.628772735595703, \"count\": 1, \"min\": 9.628772735595703, \"max\": 9.628772735595703}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[3] Batch[0] avg_epoch_loss=7.598292\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=7.598292350769043\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[3] Batch[5] avg_epoch_loss=7.579649\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=7.579649368921916\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[3] Batch [5]#011Speed: 3989.27 samples/sec#011loss=7.579649\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] processed a total of 620 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081754.778951, \"EndTime\": 1681081755.187882, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 408.846378326416, \"count\": 1, \"min\": 408.846378326416, \"max\": 408.846378326416}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1515.8290965770584 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #progress_metric: host=algo-1, completed 1.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=3, train loss <loss>=7.457720470428467\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_31135130-cabe-4cc4-82d9-1b44c9edfa3b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081755.1879988, \"EndTime\": 1681081755.1984913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.562492370605469, \"count\": 1, \"min\": 9.562492370605469, \"max\": 9.562492370605469}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[4] Batch[0] avg_epoch_loss=7.192521\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=7.192520618438721\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[4] Batch[5] avg_epoch_loss=7.135513\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=7.135512908299764\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[4] Batch [5]#011Speed: 4287.71 samples/sec#011loss=7.135513\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[4] Batch[10] avg_epoch_loss=7.092948\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=4, batch=10 train loss <loss>=7.0418699264526365\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[4] Batch [10]#011Speed: 3997.23 samples/sec#011loss=7.041870\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081755.1985857, \"EndTime\": 1681081755.6155255, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 416.8527126312256, \"count\": 1, \"min\": 416.8527126312256, \"max\": 416.8527126312256}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1556.2666203196572 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #progress_metric: host=algo-1, completed 1.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=4, train loss <loss>=7.092947916551069\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_a6f1c65f-1cb3-4921-8c82-934bc6c7cb8a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081755.615644, \"EndTime\": 1681081755.6257412, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.382486343383789, \"count\": 1, \"min\": 9.382486343383789, \"max\": 9.382486343383789}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[5] Batch[0] avg_epoch_loss=6.840563\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=6.8405632972717285\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[5] Batch[5] avg_epoch_loss=6.952992\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=6.952992121378581\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:15 INFO 139996980188992] Epoch[5] Batch [5]#011Speed: 3864.78 samples/sec#011loss=6.952992\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[5] Batch[10] avg_epoch_loss=6.958259\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=5, batch=10 train loss <loss>=6.964579963684082\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[5] Batch [10]#011Speed: 2602.42 samples/sec#011loss=6.964580\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] processed a total of 672 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081755.6258197, \"EndTime\": 1681081756.0991712, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 473.27256202697754, \"count\": 1, \"min\": 473.27256202697754, \"max\": 473.27256202697754}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1419.4115160497408 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #progress_metric: host=algo-1, completed 1.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=5, train loss <loss>=6.958259322426536\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_4fa5ca16-4207-462b-8759-4ef16543fd63-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081756.0992844, \"EndTime\": 1681081756.1125937, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.711524963378906, \"count\": 1, \"min\": 12.711524963378906, \"max\": 12.711524963378906}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[6] Batch[0] avg_epoch_loss=6.936265\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=6.936264991760254\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[6] Batch[5] avg_epoch_loss=6.918662\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=6.918661912282308\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[6] Batch [5]#011Speed: 3145.43 samples/sec#011loss=6.918662\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[6] Batch[10] avg_epoch_loss=6.947738\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=6, batch=10 train loss <loss>=6.982629585266113\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Epoch[6] Batch [10]#011Speed: 3478.31 samples/sec#011loss=6.982630\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081756.1126683, \"EndTime\": 1681081756.6648488, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 552.0985126495361, \"count\": 1, \"min\": 552.0985126495361, \"max\": 552.0985126495361}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1187.8428823240672 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #progress_metric: host=algo-1, completed 1.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=6, train loss <loss>=6.947738127274946\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:16 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_a7c3c245-987c-4ff7-8214-79340782c416-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081756.664959, \"EndTime\": 1681081756.6756177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.891271591186523, \"count\": 1, \"min\": 9.891271591186523, \"max\": 9.891271591186523}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[7] Batch[0] avg_epoch_loss=6.976394\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=6.976393699645996\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[7] Batch[5] avg_epoch_loss=6.959726\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=6.959725538889567\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[7] Batch [5]#011Speed: 3164.07 samples/sec#011loss=6.959726\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[7] Batch[10] avg_epoch_loss=6.907772\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=6.845427227020264\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[7] Batch [10]#011Speed: 2789.86 samples/sec#011loss=6.845427\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081756.6757064, \"EndTime\": 1681081757.2209413, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 545.1436042785645, \"count\": 1, \"min\": 545.1436042785645, \"max\": 545.1436042785645}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1201.712872627661 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #progress_metric: host=algo-1, completed 2.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=7, train loss <loss>=6.907771760767156\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_781e4714-a4d5-4169-a913-bcbf172d93a7-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081757.221628, \"EndTime\": 1681081757.239289, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 15.726804733276367, \"count\": 1, \"min\": 15.726804733276367, \"max\": 15.726804733276367}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[8] Batch[0] avg_epoch_loss=7.104159\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=7.104159355163574\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[8] Batch[5] avg_epoch_loss=6.930522\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=6.930522282918294\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[8] Batch [5]#011Speed: 3084.17 samples/sec#011loss=6.930522\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[8] Batch[10] avg_epoch_loss=6.832106\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=8, batch=10 train loss <loss>=6.714007186889648\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Epoch[8] Batch [10]#011Speed: 2800.44 samples/sec#011loss=6.714007\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081757.239777, \"EndTime\": 1681081757.811159, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 571.2809562683105, \"count\": 1, \"min\": 571.2809562683105, \"max\": 571.2809562683105}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1156.572969473266 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #progress_metric: host=algo-1, completed 2.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=8, train loss <loss>=6.832106330178001\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:17 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_1abe79fe-e765-471b-bac0-180f24fde320-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081757.8113372, \"EndTime\": 1681081757.8255506, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 13.144493103027344, \"count\": 1, \"min\": 13.144493103027344, \"max\": 13.144493103027344}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[9] Batch[0] avg_epoch_loss=6.778005\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=6.7780046463012695\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[9] Batch[5] avg_epoch_loss=6.725003\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=6.7250025272369385\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[9] Batch [5]#011Speed: 3548.26 samples/sec#011loss=6.725003\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081757.8257494, \"EndTime\": 1681081758.3867285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 560.882568359375, \"count\": 1, \"min\": 560.882568359375, \"max\": 560.882568359375}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1099.7392754743473 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #progress_metric: host=algo-1, completed 2.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=9, train loss <loss>=6.698998069763183\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_8192341e-4a05-45e2-b2c3-810352f119fc-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081758.3868358, \"EndTime\": 1681081758.3996902, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.192249298095703, \"count\": 1, \"min\": 12.192249298095703, \"max\": 12.192249298095703}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[10] Batch[0] avg_epoch_loss=6.601998\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=6.601998329162598\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[10] Batch[5] avg_epoch_loss=6.597144\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=6.59714412689209\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Epoch[10] Batch [5]#011Speed: 3736.10 samples/sec#011loss=6.597144\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081758.3997917, \"EndTime\": 1681081758.8690364, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 469.15435791015625, \"count\": 1, \"min\": 469.15435791015625, \"max\": 469.15435791015625}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1357.2282380432298 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #progress_metric: host=algo-1, completed 2.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=10, train loss <loss>=6.606686210632324\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:18 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_91d4c076-148f-4da6-bc2d-1015eb9138be-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081758.8691545, \"EndTime\": 1681081758.8823612, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.577295303344727, \"count\": 1, \"min\": 12.577295303344727, \"max\": 12.577295303344727}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[11] Batch[0] avg_epoch_loss=6.585688\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=6.58568811416626\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[11] Batch[5] avg_epoch_loss=6.570374\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=6.570374170939128\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[11] Batch [5]#011Speed: 4264.39 samples/sec#011loss=6.570374\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[11] Batch[10] avg_epoch_loss=6.567107\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=6.563186264038086\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[11] Batch [10]#011Speed: 3525.35 samples/sec#011loss=6.563186\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081758.882462, \"EndTime\": 1681081759.3590736, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 476.5200614929199, \"count\": 1, \"min\": 476.5200614929199, \"max\": 476.5200614929199}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1367.784214409147 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #progress_metric: host=algo-1, completed 3.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=11, train loss <loss>=6.5671069405295635\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_0c6a19b8-9cdd-4f18-ab71-4f3453abab36-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081759.3591852, \"EndTime\": 1681081759.3723433, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.56704330444336, \"count\": 1, \"min\": 12.56704330444336, \"max\": 12.56704330444336}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[12] Batch[0] avg_epoch_loss=6.569290\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=6.5692901611328125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[12] Batch[5] avg_epoch_loss=6.541561\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=6.541560570398967\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Epoch[12] Batch [5]#011Speed: 4020.06 samples/sec#011loss=6.541561\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081759.372431, \"EndTime\": 1681081759.82922, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.70366287231445, \"count\": 1, \"min\": 456.70366287231445, \"max\": 456.70366287231445}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1337.357714209375 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #progress_metric: host=algo-1, completed 3.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] #quality_metric: host=algo-1, epoch=12, train loss <loss>=6.533109521865844\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:19 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_a278be02-cb24-4ff7-b2c5-30ea84d06da0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081759.829337, \"EndTime\": 1681081759.8395393, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.275197982788086, \"count\": 1, \"min\": 9.275197982788086, \"max\": 9.275197982788086}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[13] Batch[0] avg_epoch_loss=6.498251\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=6.498251438140869\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[13] Batch[5] avg_epoch_loss=6.458776\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=6.458776156107585\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[13] Batch [5]#011Speed: 4341.30 samples/sec#011loss=6.458776\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[13] Batch[10] avg_epoch_loss=6.484165\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=6.51463098526001\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[13] Batch [10]#011Speed: 3451.97 samples/sec#011loss=6.514631\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] processed a total of 667 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081759.839637, \"EndTime\": 1681081760.2616339, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 421.91171646118164, \"count\": 1, \"min\": 421.91171646118164, \"max\": 421.91171646118164}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1580.3537989546032 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #progress_metric: host=algo-1, completed 3.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=13, train loss <loss>=6.484164714813232\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_3d8aed9b-2419-4b61-a225-5e1ceba0b514-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081760.2617333, \"EndTime\": 1681081760.2744358, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.124300003051758, \"count\": 1, \"min\": 12.124300003051758, \"max\": 12.124300003051758}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[14] Batch[0] avg_epoch_loss=6.466834\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=6.466833591461182\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[14] Batch[5] avg_epoch_loss=6.506225\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=6.506224791208903\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[14] Batch [5]#011Speed: 4385.54 samples/sec#011loss=6.506225\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[14] Batch[10] avg_epoch_loss=6.509823\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=6.514140605926514\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] Epoch[14] Batch [10]#011Speed: 3290.33 samples/sec#011loss=6.514141\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081760.2745113, \"EndTime\": 1681081760.7409706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 466.381311416626, \"count\": 1, \"min\": 466.381311416626, \"max\": 466.381311416626}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1451.1145266613587 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #progress_metric: host=algo-1, completed 3.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] #quality_metric: host=algo-1, epoch=14, train loss <loss>=6.509822888807817\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:20 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[15] Batch[0] avg_epoch_loss=6.557164\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=6.557163715362549\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[15] Batch[5] avg_epoch_loss=6.555086\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=6.555086215337117\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[15] Batch [5]#011Speed: 4280.01 samples/sec#011loss=6.555086\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[15] Batch[10] avg_epoch_loss=6.523711\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=6.4860611915588375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[15] Batch [10]#011Speed: 3584.08 samples/sec#011loss=6.486061\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081760.741079, \"EndTime\": 1681081761.205808, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 464.17999267578125, \"count\": 1, \"min\": 464.17999267578125, \"max\": 464.17999267578125}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1419.2377096023376 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #progress_metric: host=algo-1, completed 4.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=15, train loss <loss>=6.523711204528809\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[16] Batch[0] avg_epoch_loss=6.536527\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=6.536527156829834\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[16] Batch[5] avg_epoch_loss=6.505874\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=6.505873839060466\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[16] Batch [5]#011Speed: 4293.33 samples/sec#011loss=6.505874\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[16] Batch[10] avg_epoch_loss=6.491348\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=16, batch=10 train loss <loss>=6.473917865753174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[16] Batch [10]#011Speed: 4093.15 samples/sec#011loss=6.473918\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081761.2059104, \"EndTime\": 1681081761.6231527, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 416.5174961090088, \"count\": 1, \"min\": 416.5174961090088, \"max\": 416.5174961090088}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1538.35833456354 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #progress_metric: host=algo-1, completed 4.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=16, train loss <loss>=6.4913483966480605\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] Epoch[17] Batch[0] avg_epoch_loss=6.537824\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:21 INFO 139996980188992] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=6.5378241539001465\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[17] Batch[5] avg_epoch_loss=6.481240\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=6.481240431467692\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[17] Batch [5]#011Speed: 4182.71 samples/sec#011loss=6.481240\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[17] Batch[10] avg_epoch_loss=6.450861\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=6.414406585693359\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[17] Batch [10]#011Speed: 3355.96 samples/sec#011loss=6.414407\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081761.6232607, \"EndTime\": 1681081762.1162584, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 492.434024810791, \"count\": 1, \"min\": 492.434024810791, \"max\": 492.434024810791}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1329.6915775896075 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #progress_metric: host=algo-1, completed 4.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=17, train loss <loss>=6.450861410661177\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_80cbb17a-31d1-437d-815f-39ede724ab7a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081762.1163657, \"EndTime\": 1681081762.1261876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.058952331542969, \"count\": 1, \"min\": 9.058952331542969, \"max\": 9.058952331542969}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[18] Batch[0] avg_epoch_loss=6.473938\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=6.47393798828125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[18] Batch[5] avg_epoch_loss=6.419732\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=6.419732093811035\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[18] Batch [5]#011Speed: 4347.33 samples/sec#011loss=6.419732\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[18] Batch[10] avg_epoch_loss=6.405240\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=18, batch=10 train loss <loss>=6.387849617004394\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[18] Batch [10]#011Speed: 4097.53 samples/sec#011loss=6.387850\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081762.126268, \"EndTime\": 1681081762.540001, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 413.65790367126465, \"count\": 1, \"min\": 413.65790367126465, \"max\": 413.65790367126465}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1551.2167342304326 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #progress_metric: host=algo-1, completed 4.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=18, train loss <loss>=6.405240058898926\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_148bf422-f8d8-423a-8aeb-416c70314126-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081762.5401645, \"EndTime\": 1681081762.5529613, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.173652648925781, \"count\": 1, \"min\": 12.173652648925781, \"max\": 12.173652648925781}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[19] Batch[0] avg_epoch_loss=6.601010\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=6.601009845733643\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[19] Batch[5] avg_epoch_loss=6.488742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=6.488742033640544\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:22 INFO 139996980188992] Epoch[19] Batch [5]#011Speed: 4107.04 samples/sec#011loss=6.488742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[19] Batch[10] avg_epoch_loss=6.449449\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=6.402296638488769\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[19] Batch [10]#011Speed: 3763.14 samples/sec#011loss=6.402297\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081762.553042, \"EndTime\": 1681081763.0211062, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 467.98229217529297, \"count\": 1, \"min\": 467.98229217529297, \"max\": 467.98229217529297}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1371.3137236628415 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #progress_metric: host=algo-1, completed 5.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=19, train loss <loss>=6.449448672207919\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[20] Batch[0] avg_epoch_loss=7.061576\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=7.0615763664245605\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[20] Batch[5] avg_epoch_loss=6.616998\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=6.616998116175334\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[20] Batch [5]#011Speed: 3389.88 samples/sec#011loss=6.616998\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[20] Batch[10] avg_epoch_loss=6.585271\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=6.54719877243042\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[20] Batch [10]#011Speed: 2713.69 samples/sec#011loss=6.547199\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] processed a total of 697 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081763.0212352, \"EndTime\": 1681081763.5421822, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 520.2333927154541, \"count\": 1, \"min\": 520.2333927154541, \"max\": 520.2333927154541}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1339.3075877133506 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #progress_metric: host=algo-1, completed 5.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=20, train loss <loss>=6.585271141745827\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[21] Batch[0] avg_epoch_loss=6.472857\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=6.472857475280762\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[21] Batch[5] avg_epoch_loss=6.430668\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=6.430668274561564\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Epoch[21] Batch [5]#011Speed: 3663.17 samples/sec#011loss=6.430668\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081763.542308, \"EndTime\": 1681081763.9503174, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.3479175567627, \"count\": 1, \"min\": 407.3479175567627, \"max\": 407.3479175567627}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1440.4291748671349 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #progress_metric: host=algo-1, completed 5.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] #quality_metric: host=algo-1, epoch=21, train loss <loss>=6.393609285354614\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:23 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_2c1f78b1-a5e2-4c80-b2fc-312eaef6b4f0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081763.9504302, \"EndTime\": 1681081763.960999, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.730339050292969, \"count\": 1, \"min\": 9.730339050292969, \"max\": 9.730339050292969}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[22] Batch[0] avg_epoch_loss=6.418570\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=6.418570041656494\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[22] Batch[5] avg_epoch_loss=6.364465\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=6.36446483929952\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[22] Batch [5]#011Speed: 4363.39 samples/sec#011loss=6.364465\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] processed a total of 595 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081763.9610832, \"EndTime\": 1681081764.357054, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 395.8919048309326, \"count\": 1, \"min\": 395.8919048309326, \"max\": 395.8919048309326}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1502.3266095340375 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #progress_metric: host=algo-1, completed 5.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=22, train loss <loss>=6.399729490280151\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[23] Batch[0] avg_epoch_loss=6.522130\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=6.522130012512207\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[23] Batch[5] avg_epoch_loss=6.421856\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=6.421856005986531\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] Epoch[23] Batch [5]#011Speed: 4292.55 samples/sec#011loss=6.421856\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081764.3571658, \"EndTime\": 1681081764.8000963, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 442.0890808105469, \"count\": 1, \"min\": 442.0890808105469, \"max\": 442.0890808105469}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1442.5762453834009 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #progress_metric: host=algo-1, completed 6.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] #quality_metric: host=algo-1, epoch=23, train loss <loss>=6.4381578922271725\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:24 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[24] Batch[0] avg_epoch_loss=6.342546\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=6.342546463012695\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[24] Batch[5] avg_epoch_loss=6.343134\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=6.343133529027303\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[24] Batch [5]#011Speed: 4258.97 samples/sec#011loss=6.343134\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[24] Batch[10] avg_epoch_loss=6.358975\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=6.377984428405762\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[24] Batch [10]#011Speed: 3607.65 samples/sec#011loss=6.377984\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081764.8002176, \"EndTime\": 1681081765.2736716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 472.7187156677246, \"count\": 1, \"min\": 472.7187156677246, \"max\": 472.7187156677246}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1380.8042751847706 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #progress_metric: host=algo-1, completed 6.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=24, train loss <loss>=6.3589748469266025\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_5c434188-cb5d-4a55-979d-529b36071502-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081765.2738092, \"EndTime\": 1681081765.28445, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.99307632446289, \"count\": 1, \"min\": 9.99307632446289, \"max\": 9.99307632446289}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[25] Batch[0] avg_epoch_loss=6.836410\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=6.836409568786621\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[25] Batch[5] avg_epoch_loss=6.623737\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=6.6237374146779375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[25] Batch [5]#011Speed: 3918.64 samples/sec#011loss=6.623737\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[25] Batch[10] avg_epoch_loss=6.552913\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=6.467922687530518\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[25] Batch [10]#011Speed: 3564.83 samples/sec#011loss=6.467923\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] processed a total of 671 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081765.2845416, \"EndTime\": 1681081765.7081516, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 423.53367805480957, \"count\": 1, \"min\": 423.53367805480957, \"max\": 423.53367805480957}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1583.7199161765334 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #progress_metric: host=algo-1, completed 6.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=25, train loss <loss>=6.552912538701838\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] Epoch[26] Batch[0] avg_epoch_loss=6.352606\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:25 INFO 139996980188992] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=6.352606296539307\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[26] Batch[5] avg_epoch_loss=6.342235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=6.34223477045695\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[26] Batch [5]#011Speed: 3643.80 samples/sec#011loss=6.342235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] processed a total of 596 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081765.7082596, \"EndTime\": 1681081766.1100671, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 401.13306045532227, \"count\": 1, \"min\": 401.13306045532227, \"max\": 401.13306045532227}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1485.190124498338 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #progress_metric: host=algo-1, completed 6.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=26, train loss <loss>=6.374549007415771\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[27] Batch[0] avg_epoch_loss=6.167957\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=6.167956829071045\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[27] Batch[5] avg_epoch_loss=6.395824\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=6.395824114481608\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[27] Batch [5]#011Speed: 4349.23 samples/sec#011loss=6.395824\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081766.1101732, \"EndTime\": 1681081766.494656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 383.79502296447754, \"count\": 1, \"min\": 383.79502296447754, \"max\": 383.79502296447754}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1583.5129946660209 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #progress_metric: host=algo-1, completed 7.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=27, train loss <loss>=6.367514371871948\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[28] Batch[0] avg_epoch_loss=6.267694\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=6.267693996429443\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[28] Batch[5] avg_epoch_loss=6.274458\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=6.274457613627116\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Epoch[28] Batch [5]#011Speed: 4169.37 samples/sec#011loss=6.274458\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] processed a total of 629 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081766.4947672, \"EndTime\": 1681081766.8890715, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 393.35107803344727, \"count\": 1, \"min\": 393.35107803344727, \"max\": 393.35107803344727}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1598.4060974353692 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #progress_metric: host=algo-1, completed 7.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] #quality_metric: host=algo-1, epoch=28, train loss <loss>=6.282630729675293\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:26 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_fb1b9ee9-6c6a-4dfc-a61b-048cc11d612b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081766.8891792, \"EndTime\": 1681081766.899145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.163379669189453, \"count\": 1, \"min\": 9.163379669189453, \"max\": 9.163379669189453}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[29] Batch[0] avg_epoch_loss=6.218673\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=6.218672752380371\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[29] Batch[5] avg_epoch_loss=6.290526\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=6.290526072184245\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[29] Batch [5]#011Speed: 3585.84 samples/sec#011loss=6.290526\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] processed a total of 628 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081766.899227, \"EndTime\": 1681081767.319547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 420.23611068725586, \"count\": 1, \"min\": 420.23611068725586, \"max\": 420.23611068725586}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1493.7971164408693 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #progress_metric: host=algo-1, completed 7.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=29, train loss <loss>=6.264763736724854\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_848b43c0-32f7-4ac5-81aa-6ce07037891d-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081767.3196633, \"EndTime\": 1681081767.3317177, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 11.257171630859375, \"count\": 1, \"min\": 11.257171630859375, \"max\": 11.257171630859375}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[30] Batch[0] avg_epoch_loss=6.271014\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=6.271014213562012\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[30] Batch[5] avg_epoch_loss=6.280094\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=6.280093590418498\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Epoch[30] Batch [5]#011Speed: 3415.40 samples/sec#011loss=6.280094\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081767.3318453, \"EndTime\": 1681081767.7598007, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.8583526611328, \"count\": 1, \"min\": 427.8583526611328, \"max\": 427.8583526611328}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1443.863392005187 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #progress_metric: host=algo-1, completed 7.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] #quality_metric: host=algo-1, epoch=30, train loss <loss>=6.249365377426147\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:27 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_faee4d16-795a-4741-8b28-fc776dc72320-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081767.7599072, \"EndTime\": 1681081767.7691245, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.32056999206543, \"count\": 1, \"min\": 8.32056999206543, \"max\": 8.32056999206543}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[31] Batch[0] avg_epoch_loss=6.292006\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=6.292006492614746\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[31] Batch[5] avg_epoch_loss=6.259799\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=6.259798765182495\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[31] Batch [5]#011Speed: 4252.38 samples/sec#011loss=6.259799\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081767.769281, \"EndTime\": 1681081768.1785913, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.21688079833984, \"count\": 1, \"min\": 409.21688079833984, \"max\": 409.21688079833984}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1502.2421317828102 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #progress_metric: host=algo-1, completed 8.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=31, train loss <loss>=6.220773506164551\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_cd45eff9-fde2-4ce7-8db8-1472148458bb-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081768.1787083, \"EndTime\": 1681081768.1916826, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.31694221496582, \"count\": 1, \"min\": 12.31694221496582, \"max\": 12.31694221496582}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[32] Batch[0] avg_epoch_loss=6.145174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=6.1451735496521\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[32] Batch[5] avg_epoch_loss=6.184051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=6.184050957361857\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[32] Batch [5]#011Speed: 3322.22 samples/sec#011loss=6.184051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[32] Batch[10] avg_epoch_loss=6.216480\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=32, batch=10 train loss <loss>=6.255394458770752\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[32] Batch [10]#011Speed: 3803.25 samples/sec#011loss=6.255394\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081768.191785, \"EndTime\": 1681081768.6872706, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 495.3911304473877, \"count\": 1, \"min\": 495.3911304473877, \"max\": 495.3911304473877}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1297.518431010303 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #progress_metric: host=algo-1, completed 8.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=32, train loss <loss>=6.216479821638628\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_b876c48d-64ae-42bf-aebd-c100ba9d2702-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081768.6873872, \"EndTime\": 1681081768.6974018, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.328365325927734, \"count\": 1, \"min\": 9.328365325927734, \"max\": 9.328365325927734}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] Epoch[33] Batch[0] avg_epoch_loss=6.295197\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:28 INFO 139996980188992] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=6.295197486877441\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[33] Batch[5] avg_epoch_loss=6.344170\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=6.3441697756449384\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[33] Batch [5]#011Speed: 3992.72 samples/sec#011loss=6.344170\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081768.6974988, \"EndTime\": 1681081769.0916278, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 394.0286636352539, \"count\": 1, \"min\": 394.0286636352539, \"max\": 394.0286636352539}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1522.0696808687578 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #progress_metric: host=algo-1, completed 8.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=33, train loss <loss>=6.34192967414856\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[34] Batch[0] avg_epoch_loss=6.483006\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=6.483005523681641\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[34] Batch[5] avg_epoch_loss=6.340875\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=6.3408745129903155\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[34] Batch [5]#011Speed: 4371.56 samples/sec#011loss=6.340875\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[34] Batch[10] avg_epoch_loss=6.291001\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=34, batch=10 train loss <loss>=6.231153297424316\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[34] Batch [10]#011Speed: 3220.85 samples/sec#011loss=6.231153\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081769.091744, \"EndTime\": 1681081769.5743263, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 481.94074630737305, \"count\": 1, \"min\": 481.94074630737305, \"max\": 481.94074630737305}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1412.5601971823185 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #progress_metric: host=algo-1, completed 8.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=34, train loss <loss>=6.291001233187589\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[35] Batch[0] avg_epoch_loss=6.242858\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=6.242858409881592\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[35] Batch[5] avg_epoch_loss=6.211174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=6.2111741701761884\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:29 INFO 139996980188992] Epoch[35] Batch [5]#011Speed: 4297.64 samples/sec#011loss=6.211174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] processed a total of 636 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081769.574435, \"EndTime\": 1681081770.0258474, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.7746696472168, \"count\": 1, \"min\": 450.7746696472168, \"max\": 450.7746696472168}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1410.0065775494347 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #progress_metric: host=algo-1, completed 9.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=35, train loss <loss>=6.197851610183716\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_dec39741-7e0b-4807-a627-07fbfbf7e766-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081770.026044, \"EndTime\": 1681081770.0371344, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 10.30588150024414, \"count\": 1, \"min\": 10.30588150024414, \"max\": 10.30588150024414}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[36] Batch[0] avg_epoch_loss=6.245623\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=36, batch=0 train loss <loss>=6.2456231117248535\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[36] Batch[5] avg_epoch_loss=6.197053\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=36, batch=5 train loss <loss>=6.19705335299174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[36] Batch [5]#011Speed: 3452.66 samples/sec#011loss=6.197053\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] processed a total of 603 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081770.0372882, \"EndTime\": 1681081770.4510968, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 413.7096405029297, \"count\": 1, \"min\": 413.7096405029297, \"max\": 413.7096405029297}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1456.9713521354267 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #progress_metric: host=algo-1, completed 9.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=36, train loss <loss>=6.199859619140625\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[37] Batch[0] avg_epoch_loss=6.153463\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=37, batch=0 train loss <loss>=6.153462886810303\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[37] Batch[5] avg_epoch_loss=6.163265\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=37, batch=5 train loss <loss>=6.163264513015747\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[37] Batch [5]#011Speed: 4389.36 samples/sec#011loss=6.163265\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[37] Batch[10] avg_epoch_loss=6.148560\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=37, batch=10 train loss <loss>=6.130913734436035\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Epoch[37] Batch [10]#011Speed: 2955.43 samples/sec#011loss=6.130914\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] processed a total of 706 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081770.4512057, \"EndTime\": 1681081770.9546998, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 502.77137756347656, \"count\": 1, \"min\": 502.77137756347656, \"max\": 502.77137756347656}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1403.4607244283175 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #progress_metric: host=algo-1, completed 9.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] #quality_metric: host=algo-1, epoch=37, train loss <loss>=6.145790338516235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:30 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_af4595f0-d701-47a0-9ff1-bec69276ab0f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081770.9549131, \"EndTime\": 1681081770.9644148, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.727073669433594, \"count\": 1, \"min\": 8.727073669433594, \"max\": 8.727073669433594}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[38] Batch[0] avg_epoch_loss=6.169025\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=38, batch=0 train loss <loss>=6.16902494430542\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[38] Batch[5] avg_epoch_loss=6.239860\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=38, batch=5 train loss <loss>=6.239859580993652\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[38] Batch [5]#011Speed: 2960.80 samples/sec#011loss=6.239860\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081770.964494, \"EndTime\": 1681081771.3958285, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.25057220458984, \"count\": 1, \"min\": 431.25057220458984, \"max\": 431.25057220458984}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1404.7425667553698 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #progress_metric: host=algo-1, completed 9.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=38, train loss <loss>=6.200725555419922\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[39] Batch[0] avg_epoch_loss=6.160483\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=39, batch=0 train loss <loss>=6.160483360290527\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[39] Batch[5] avg_epoch_loss=6.152347\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=39, batch=5 train loss <loss>=6.15234653155009\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] Epoch[39] Batch [5]#011Speed: 4336.49 samples/sec#011loss=6.152347\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081771.395923, \"EndTime\": 1681081771.787363, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 390.78521728515625, \"count\": 1, \"min\": 390.78521728515625, \"max\": 390.78521728515625}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1624.3131601063124 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #progress_metric: host=algo-1, completed 10.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] #quality_metric: host=algo-1, epoch=39, train loss <loss>=6.167223405838013\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:31 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[40] Batch[0] avg_epoch_loss=6.166845\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=40, batch=0 train loss <loss>=6.166844844818115\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[40] Batch[5] avg_epoch_loss=6.172389\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=40, batch=5 train loss <loss>=6.172388871510823\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[40] Batch [5]#011Speed: 4124.94 samples/sec#011loss=6.172389\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081771.787455, \"EndTime\": 1681081772.1755753, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.40038871765137, \"count\": 1, \"min\": 387.40038871765137, \"max\": 387.40038871765137}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1638.3627095101797 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #progress_metric: host=algo-1, completed 10.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=40, train loss <loss>=6.163529348373413\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[41] Batch[0] avg_epoch_loss=6.158530\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=41, batch=0 train loss <loss>=6.158529758453369\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[41] Batch[5] avg_epoch_loss=6.125743\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=41, batch=5 train loss <loss>=6.125742832819621\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[41] Batch [5]#011Speed: 3987.11 samples/sec#011loss=6.125743\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081772.1756954, \"EndTime\": 1681081772.6039004, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.43921279907227, \"count\": 1, \"min\": 427.43921279907227, \"max\": 427.43921279907227}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1496.683677764861 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #progress_metric: host=algo-1, completed 10.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=41, train loss <loss>=6.129553174972534\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_f72a3b7b-6840-4ef1-975c-6bc7c248a344-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081772.6040173, \"EndTime\": 1681081772.6170921, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.42685317993164, \"count\": 1, \"min\": 12.42685317993164, \"max\": 12.42685317993164}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[42] Batch[0] avg_epoch_loss=6.211407\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=42, batch=0 train loss <loss>=6.211406707763672\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] Epoch[42] Batch[5] avg_epoch_loss=6.102887\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:32 INFO 139996980188992] #quality_metric: host=algo-1, epoch=42, batch=5 train loss <loss>=6.102887074152629\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[42] Batch [5]#011Speed: 4302.07 samples/sec#011loss=6.102887\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[42] Batch[10] avg_epoch_loss=6.097441\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=42, batch=10 train loss <loss>=6.090905380249024\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[42] Batch [10]#011Speed: 3107.73 samples/sec#011loss=6.090905\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] processed a total of 685 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081772.617176, \"EndTime\": 1681081773.1036992, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 486.44065856933594, \"count\": 1, \"min\": 486.44065856933594, \"max\": 486.44065856933594}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1407.7259315514834 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #progress_metric: host=algo-1, completed 10.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=42, train loss <loss>=6.09744084965099\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_dd500d1f-0045-4926-8f65-ab54d166a40a-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081773.1038094, \"EndTime\": 1681081773.116911, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.419462203979492, \"count\": 1, \"min\": 12.419462203979492, \"max\": 12.419462203979492}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[43] Batch[0] avg_epoch_loss=6.042692\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=43, batch=0 train loss <loss>=6.042691707611084\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[43] Batch[5] avg_epoch_loss=6.074522\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=43, batch=5 train loss <loss>=6.074521541595459\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[43] Batch [5]#011Speed: 4375.35 samples/sec#011loss=6.074522\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[43] Batch[10] avg_epoch_loss=6.061691\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=43, batch=10 train loss <loss>=6.046294021606445\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[43] Batch [10]#011Speed: 3712.15 samples/sec#011loss=6.046294\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081773.1169953, \"EndTime\": 1681081773.6023095, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 485.2323532104492, \"count\": 1, \"min\": 485.2323532104492, \"max\": 485.2323532104492}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1345.3021717827874 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #progress_metric: host=algo-1, completed 11.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=43, train loss <loss>=6.061690850691362\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_2c33040b-d9fb-4695-85ae-dc8de39158aa-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081773.602419, \"EndTime\": 1681081773.6157868, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.658119201660156, \"count\": 1, \"min\": 12.658119201660156, \"max\": 12.658119201660156}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] Epoch[44] Batch[0] avg_epoch_loss=6.164922\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:33 INFO 139996980188992] #quality_metric: host=algo-1, epoch=44, batch=0 train loss <loss>=6.16492223739624\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[44] Batch[5] avg_epoch_loss=6.079094\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=44, batch=5 train loss <loss>=6.079094012578328\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[44] Batch [5]#011Speed: 4327.93 samples/sec#011loss=6.079094\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081773.615879, \"EndTime\": 1681081774.067738, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 451.7707824707031, \"count\": 1, \"min\": 451.7707824707031, \"max\": 451.7707824707031}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1374.0897963920856 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #progress_metric: host=algo-1, completed 11.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=44, train loss <loss>=6.07962064743042\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[45] Batch[0] avg_epoch_loss=6.176994\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=45, batch=0 train loss <loss>=6.176994323730469\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[45] Batch[5] avg_epoch_loss=6.085968\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=45, batch=5 train loss <loss>=6.085968017578125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[45] Batch [5]#011Speed: 4403.67 samples/sec#011loss=6.085968\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081774.0678506, \"EndTime\": 1681081774.4599407, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 391.369104385376, \"count\": 1, \"min\": 391.369104385376, \"max\": 391.369104385376}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1626.664260556413 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #progress_metric: host=algo-1, completed 11.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=45, train loss <loss>=6.058763790130615\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_b88952c3-302a-4b10-afbc-17a7b26fa45f-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081774.4601111, \"EndTime\": 1681081774.4730632, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.257099151611328, \"count\": 1, \"min\": 12.257099151611328, \"max\": 12.257099151611328}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[46] Batch[0] avg_epoch_loss=6.070242\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=46, batch=0 train loss <loss>=6.070242404937744\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[46] Batch[5] avg_epoch_loss=6.087391\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=46, batch=5 train loss <loss>=6.087391297022502\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] Epoch[46] Batch [5]#011Speed: 4232.21 samples/sec#011loss=6.087391\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] processed a total of 606 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081774.4731438, \"EndTime\": 1681081774.9220378, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 448.81367683410645, \"count\": 1, \"min\": 448.81367683410645, \"max\": 448.81367683410645}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1349.7121482940008 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #progress_metric: host=algo-1, completed 11.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] #quality_metric: host=algo-1, epoch=46, train loss <loss>=6.115215015411377\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:34 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[47] Batch[0] avg_epoch_loss=5.924692\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=47, batch=0 train loss <loss>=5.924692153930664\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[47] Batch[5] avg_epoch_loss=6.016202\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=47, batch=5 train loss <loss>=6.016202052434285\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[47] Batch [5]#011Speed: 4377.30 samples/sec#011loss=6.016202\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081774.922156, \"EndTime\": 1681081775.3502593, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.19125747680664, \"count\": 1, \"min\": 427.19125747680664, \"max\": 427.19125747680664}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1415.6578303600966 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #progress_metric: host=algo-1, completed 12.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=47, train loss <loss>=6.041518306732177\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_0fc36842-a647-443b-af0b-dd4f6965554c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081775.3503752, \"EndTime\": 1681081775.3634145, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.348651885986328, \"count\": 1, \"min\": 12.348651885986328, \"max\": 12.348651885986328}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[48] Batch[0] avg_epoch_loss=6.011606\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=48, batch=0 train loss <loss>=6.011605739593506\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[48] Batch[5] avg_epoch_loss=6.012450\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=48, batch=5 train loss <loss>=6.012450297673543\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[48] Batch [5]#011Speed: 4333.32 samples/sec#011loss=6.012450\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[48] Batch[10] avg_epoch_loss=6.060034\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=48, batch=10 train loss <loss>=6.1171345710754395\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] Epoch[48] Batch [10]#011Speed: 3016.85 samples/sec#011loss=6.117135\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] processed a total of 693 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081775.3634973, \"EndTime\": 1681081775.8475595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 483.9813709259033, \"count\": 1, \"min\": 483.9813709259033, \"max\": 483.9813709259033}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1431.4015583310434 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #progress_metric: host=algo-1, completed 12.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] #quality_metric: host=algo-1, epoch=48, train loss <loss>=6.060034058310769\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:35 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[49] Batch[0] avg_epoch_loss=6.087235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=49, batch=0 train loss <loss>=6.087235450744629\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[49] Batch[5] avg_epoch_loss=6.001648\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=49, batch=5 train loss <loss>=6.001648346583049\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[49] Batch [5]#011Speed: 3617.43 samples/sec#011loss=6.001648\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[49] Batch[10] avg_epoch_loss=5.970428\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=49, batch=10 train loss <loss>=5.932964229583741\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[49] Batch [10]#011Speed: 3279.38 samples/sec#011loss=5.932964\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081775.847669, \"EndTime\": 1681081776.346927, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 498.68178367614746, \"count\": 1, \"min\": 498.68178367614746, \"max\": 498.68178367614746}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1290.9831430010715 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #progress_metric: host=algo-1, completed 12.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=49, train loss <loss>=5.970428293401545\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_22b9a31e-0f31-4e67-8e46-a0cbb665ef27-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081776.3470376, \"EndTime\": 1681081776.355702, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.034706115722656, \"count\": 1, \"min\": 8.034706115722656, \"max\": 8.034706115722656}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[50] Batch[0] avg_epoch_loss=5.986630\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=50, batch=0 train loss <loss>=5.986629962921143\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[50] Batch[5] avg_epoch_loss=6.054683\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=50, batch=5 train loss <loss>=6.054682652155559\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] Epoch[50] Batch [5]#011Speed: 3778.13 samples/sec#011loss=6.054683\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081776.3557925, \"EndTime\": 1681081776.7781205, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 422.23238945007324, \"count\": 1, \"min\": 422.23238945007324, \"max\": 422.23238945007324}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1500.9461306390845 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #progress_metric: host=algo-1, completed 12.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] #quality_metric: host=algo-1, epoch=50, train loss <loss>=6.042225170135498\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:36 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[51] Batch[0] avg_epoch_loss=6.041741\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=51, batch=0 train loss <loss>=6.041740894317627\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[51] Batch[5] avg_epoch_loss=6.047314\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=51, batch=5 train loss <loss>=6.047313531239827\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[51] Batch [5]#011Speed: 4375.36 samples/sec#011loss=6.047314\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[51] Batch[10] avg_epoch_loss=6.062984\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=51, batch=10 train loss <loss>=6.081788825988769\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[51] Batch [10]#011Speed: 3116.21 samples/sec#011loss=6.081789\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] processed a total of 656 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081776.7782264, \"EndTime\": 1681081777.2224505, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 443.42684745788574, \"count\": 1, \"min\": 443.42684745788574, \"max\": 443.42684745788574}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1478.8981693991127 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #progress_metric: host=algo-1, completed 13.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=51, train loss <loss>=6.062984119762074\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[52] Batch[0] avg_epoch_loss=5.972534\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=52, batch=0 train loss <loss>=5.972533702850342\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[52] Batch[5] avg_epoch_loss=6.019235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=52, batch=5 train loss <loss>=6.019234577814738\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[52] Batch [5]#011Speed: 3829.10 samples/sec#011loss=6.019235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[52] Batch[10] avg_epoch_loss=5.995886\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=52, batch=10 train loss <loss>=5.967866611480713\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[52] Batch [10]#011Speed: 3674.34 samples/sec#011loss=5.967867\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] processed a total of 664 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081777.2225382, \"EndTime\": 1681081777.6621463, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 438.90905380249023, \"count\": 1, \"min\": 438.90905380249023, \"max\": 438.90905380249023}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1512.2315431558793 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #progress_metric: host=algo-1, completed 13.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=52, train loss <loss>=5.995885502208363\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] Epoch[53] Batch[0] avg_epoch_loss=6.028421\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:37 INFO 139996980188992] #quality_metric: host=algo-1, epoch=53, batch=0 train loss <loss>=6.028421401977539\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[53] Batch[5] avg_epoch_loss=6.053681\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=53, batch=5 train loss <loss>=6.053681214650472\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[53] Batch [5]#011Speed: 3525.25 samples/sec#011loss=6.053681\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081777.662267, \"EndTime\": 1681081778.0953748, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.2848320007324, \"count\": 1, \"min\": 432.2848320007324, \"max\": 432.2848320007324}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1403.4894689056118 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #progress_metric: host=algo-1, completed 13.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=53, train loss <loss>=6.062660646438599\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[54] Batch[0] avg_epoch_loss=6.039271\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=54, batch=0 train loss <loss>=6.039270877838135\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[54] Batch[5] avg_epoch_loss=6.069014\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=54, batch=5 train loss <loss>=6.069013595581055\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[54] Batch [5]#011Speed: 3614.32 samples/sec#011loss=6.069014\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081778.095526, \"EndTime\": 1681081778.5242414, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 427.87694931030273, \"count\": 1, \"min\": 427.87694931030273, \"max\": 427.87694931030273}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1495.1531243403174 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #progress_metric: host=algo-1, completed 13.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=54, train loss <loss>=6.0516204833984375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[55] Batch[0] avg_epoch_loss=5.905564\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=55, batch=0 train loss <loss>=5.905563831329346\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[55] Batch[5] avg_epoch_loss=5.994526\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=55, batch=5 train loss <loss>=5.994525909423828\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[55] Batch [5]#011Speed: 4261.04 samples/sec#011loss=5.994526\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[55] Batch[10] avg_epoch_loss=6.017648\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=55, batch=10 train loss <loss>=6.045394420623779\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] Epoch[55] Batch [10]#011Speed: 3496.05 samples/sec#011loss=6.045394\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081778.5243566, \"EndTime\": 1681081778.9920924, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 467.0994281768799, \"count\": 1, \"min\": 467.0994281768799, \"max\": 467.0994281768799}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1401.8002311442663 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #progress_metric: host=algo-1, completed 14.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] #quality_metric: host=algo-1, epoch=55, train loss <loss>=6.017647959969261\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:38 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[56] Batch[0] avg_epoch_loss=6.057095\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=56, batch=0 train loss <loss>=6.057094573974609\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[56] Batch[5] avg_epoch_loss=6.018698\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=56, batch=5 train loss <loss>=6.0186982949574785\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[56] Batch [5]#011Speed: 4404.00 samples/sec#011loss=6.018698\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] processed a total of 622 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081778.9921985, \"EndTime\": 1681081779.4330869, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.25421142578125, \"count\": 1, \"min\": 440.25421142578125, \"max\": 440.25421142578125}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1412.299254942912 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #progress_metric: host=algo-1, completed 14.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=56, train loss <loss>=6.0266155242919925\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[57] Batch[0] avg_epoch_loss=6.051995\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=57, batch=0 train loss <loss>=6.051994800567627\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[57] Batch[5] avg_epoch_loss=5.987482\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=57, batch=5 train loss <loss>=5.987482150395711\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] Epoch[57] Batch [5]#011Speed: 3745.06 samples/sec#011loss=5.987482\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081779.4331944, \"EndTime\": 1681081779.8484116, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.623498916626, \"count\": 1, \"min\": 414.623498916626, \"max\": 414.623498916626}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1528.5461080597925 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #progress_metric: host=algo-1, completed 14.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] #quality_metric: host=algo-1, epoch=57, train loss <loss>=5.984262990951538\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:39 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[58] Batch[0] avg_epoch_loss=5.907417\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=58, batch=0 train loss <loss>=5.907416820526123\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[58] Batch[5] avg_epoch_loss=5.937376\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=58, batch=5 train loss <loss>=5.937376499176025\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[58] Batch [5]#011Speed: 3470.88 samples/sec#011loss=5.937376\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[58] Batch[10] avg_epoch_loss=5.976587\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=58, batch=10 train loss <loss>=6.023640537261963\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[58] Batch [10]#011Speed: 2908.84 samples/sec#011loss=6.023641\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] processed a total of 674 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081779.8485055, \"EndTime\": 1681081780.3122547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 463.0906581878662, \"count\": 1, \"min\": 463.0906581878662, \"max\": 463.0906581878662}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1454.9539091812103 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #progress_metric: host=algo-1, completed 14.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=58, train loss <loss>=5.976587425578725\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[59] Batch[0] avg_epoch_loss=6.099491\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=59, batch=0 train loss <loss>=6.099491119384766\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[59] Batch[5] avg_epoch_loss=6.067274\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=59, batch=5 train loss <loss>=6.067274173100789\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[59] Batch [5]#011Speed: 4220.27 samples/sec#011loss=6.067274\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[59] Batch[10] avg_epoch_loss=6.036018\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=59, batch=10 train loss <loss>=5.998511505126953\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] Epoch[59] Batch [10]#011Speed: 3394.51 samples/sec#011loss=5.998512\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081780.3123562, \"EndTime\": 1681081780.7522056, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 439.16940689086914, \"count\": 1, \"min\": 439.16940689086914, \"max\": 439.16940689086914}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1504.5675041610612 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #progress_metric: host=algo-1, completed 15.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] #quality_metric: host=algo-1, epoch=59, train loss <loss>=6.036018414930864\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:40 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[60] Batch[0] avg_epoch_loss=6.006204\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=60, batch=0 train loss <loss>=6.006203651428223\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[60] Batch[5] avg_epoch_loss=5.915253\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=60, batch=5 train loss <loss>=5.915252844492595\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[60] Batch [5]#011Speed: 4270.53 samples/sec#011loss=5.915253\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[60] Batch[10] avg_epoch_loss=5.928293\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=60, batch=10 train loss <loss>=5.943942260742188\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[60] Batch [10]#011Speed: 3508.73 samples/sec#011loss=5.943942\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081780.7523127, \"EndTime\": 1681081781.223771, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 470.58773040771484, \"count\": 1, \"min\": 470.58773040771484, \"max\": 470.58773040771484}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1380.7443151805028 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #progress_metric: host=algo-1, completed 15.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=60, train loss <loss>=5.928293488242409\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_df1e84e3-c061-4456-b96d-bc984633e4c5-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081781.2238846, \"EndTime\": 1681081781.2336876, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.109258651733398, \"count\": 1, \"min\": 9.109258651733398, \"max\": 9.109258651733398}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[61] Batch[0] avg_epoch_loss=6.031493\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=61, batch=0 train loss <loss>=6.031493186950684\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[61] Batch[5] avg_epoch_loss=5.937782\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=61, batch=5 train loss <loss>=5.937781810760498\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[61] Batch [5]#011Speed: 3409.63 samples/sec#011loss=5.937782\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] processed a total of 600 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081781.233781, \"EndTime\": 1681081781.6595507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.68206787109375, \"count\": 1, \"min\": 425.68206787109375, \"max\": 425.68206787109375}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1408.97312653021 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #progress_metric: host=algo-1, completed 15.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=61, train loss <loss>=5.914037752151489\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_a7b787f2-cd02-47cf-9204-a50f1f26de14-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081781.6596591, \"EndTime\": 1681081781.6729045, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.576580047607422, \"count\": 1, \"min\": 12.576580047607422, \"max\": 12.576580047607422}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] Epoch[62] Batch[0] avg_epoch_loss=5.883321\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:41 INFO 139996980188992] #quality_metric: host=algo-1, epoch=62, batch=0 train loss <loss>=5.8833208084106445\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[62] Batch[5] avg_epoch_loss=5.943574\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=62, batch=5 train loss <loss>=5.9435741901397705\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[62] Batch [5]#011Speed: 4172.96 samples/sec#011loss=5.943574\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] processed a total of 631 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081781.673164, \"EndTime\": 1681081782.123757, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 450.5023956298828, \"count\": 1, \"min\": 450.5023956298828, \"max\": 450.5023956298828}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1400.1642271682897 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #progress_metric: host=algo-1, completed 15.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=62, train loss <loss>=5.961301851272583\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[63] Batch[0] avg_epoch_loss=5.863812\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=63, batch=0 train loss <loss>=5.86381196975708\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[63] Batch[5] avg_epoch_loss=5.933852\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=63, batch=5 train loss <loss>=5.933851877848308\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[63] Batch [5]#011Speed: 4244.83 samples/sec#011loss=5.933852\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[63] Batch[10] avg_epoch_loss=5.944748\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=63, batch=10 train loss <loss>=5.95782241821289\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[63] Batch [10]#011Speed: 3801.90 samples/sec#011loss=5.957822\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081782.1238625, \"EndTime\": 1681081782.5474892, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 422.9397773742676, \"count\": 1, \"min\": 422.9397773742676, \"max\": 422.9397773742676}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1543.4364807887591 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #progress_metric: host=algo-1, completed 16.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=63, train loss <loss>=5.944747578014027\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[64] Batch[0] avg_epoch_loss=6.117992\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=64, batch=0 train loss <loss>=6.117991924285889\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[64] Batch[5] avg_epoch_loss=5.971536\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=64, batch=5 train loss <loss>=5.971535762151082\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] Epoch[64] Batch [5]#011Speed: 3532.94 samples/sec#011loss=5.971536\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] processed a total of 624 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081782.5475805, \"EndTime\": 1681081782.9550724, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 406.9216251373291, \"count\": 1, \"min\": 406.9216251373291, \"max\": 406.9216251373291}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1532.8244872811524 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #progress_metric: host=algo-1, completed 16.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] #quality_metric: host=algo-1, epoch=64, train loss <loss>=5.968723583221435\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:42 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[65] Batch[0] avg_epoch_loss=5.943458\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=65, batch=0 train loss <loss>=5.94345760345459\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[65] Batch[5] avg_epoch_loss=5.986867\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=65, batch=5 train loss <loss>=5.98686671257019\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[65] Batch [5]#011Speed: 3244.97 samples/sec#011loss=5.986867\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[65] Batch[10] avg_epoch_loss=5.941143\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=65, batch=10 train loss <loss>=5.8862754821777346\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[65] Batch [10]#011Speed: 3813.75 samples/sec#011loss=5.886275\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081782.955186, \"EndTime\": 1681081783.459185, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 503.15284729003906, \"count\": 1, \"min\": 503.15284729003906, \"max\": 503.15284729003906}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1281.5200928078202 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #progress_metric: host=algo-1, completed 16.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=65, train loss <loss>=5.941143426028165\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[66] Batch[0] avg_epoch_loss=6.122224\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=66, batch=0 train loss <loss>=6.122223854064941\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[66] Batch[5] avg_epoch_loss=5.986630\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=66, batch=5 train loss <loss>=5.986629883448283\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] Epoch[66] Batch [5]#011Speed: 4212.10 samples/sec#011loss=5.986630\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] processed a total of 608 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081783.4592896, \"EndTime\": 1681081783.916092, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 456.22706413269043, \"count\": 1, \"min\": 456.22706413269043, \"max\": 456.22706413269043}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1332.0251451960087 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #progress_metric: host=algo-1, completed 16.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] #quality_metric: host=algo-1, epoch=66, train loss <loss>=5.947790431976318\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:43 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[67] Batch[0] avg_epoch_loss=5.852597\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=67, batch=0 train loss <loss>=5.852596759796143\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[67] Batch[5] avg_epoch_loss=5.891767\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=67, batch=5 train loss <loss>=5.891767342885335\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[67] Batch [5]#011Speed: 3856.64 samples/sec#011loss=5.891767\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] processed a total of 578 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081783.9162564, \"EndTime\": 1681081784.32989, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 412.86730766296387, \"count\": 1, \"min\": 412.86730766296387, \"max\": 412.86730766296387}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1399.317464486148 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #progress_metric: host=algo-1, completed 17.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=67, train loss <loss>=5.89222469329834\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_19e035b0-3a17-4002-874f-c743ff4bf074-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081784.3300211, \"EndTime\": 1681081784.3408117, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.970664978027344, \"count\": 1, \"min\": 9.970664978027344, \"max\": 9.970664978027344}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[68] Batch[0] avg_epoch_loss=5.976363\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=68, batch=0 train loss <loss>=5.976363182067871\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[68] Batch[5] avg_epoch_loss=5.879707\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=68, batch=5 train loss <loss>=5.879707415898641\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] Epoch[68] Batch [5]#011Speed: 4240.63 samples/sec#011loss=5.879707\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] processed a total of 627 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081784.3409092, \"EndTime\": 1681081784.7505355, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 409.5304012298584, \"count\": 1, \"min\": 409.5304012298584, \"max\": 409.5304012298584}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1529.4823196605375 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #progress_metric: host=algo-1, completed 17.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] #quality_metric: host=algo-1, epoch=68, train loss <loss>=5.900445175170899\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:44 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[69] Batch[0] avg_epoch_loss=5.915494\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=69, batch=0 train loss <loss>=5.915494441986084\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[69] Batch[5] avg_epoch_loss=5.864666\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=69, batch=5 train loss <loss>=5.864665746688843\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[69] Batch [5]#011Speed: 4316.60 samples/sec#011loss=5.864666\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081784.7508914, \"EndTime\": 1681081785.199854, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 448.1320381164551, \"count\": 1, \"min\": 448.1320381164551, \"max\": 448.1320381164551}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1414.2619905906931 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #progress_metric: host=algo-1, completed 17.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=69, train loss <loss>=5.891868019104004\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_5e062b2e-4733-4ecb-aec7-e7f334d11b66-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081785.1999586, \"EndTime\": 1681081785.2126718, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.036800384521484, \"count\": 1, \"min\": 12.036800384521484, \"max\": 12.036800384521484}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[70] Batch[0] avg_epoch_loss=5.910017\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=70, batch=0 train loss <loss>=5.910017490386963\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[70] Batch[5] avg_epoch_loss=5.889006\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=70, batch=5 train loss <loss>=5.889005740483602\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[70] Batch [5]#011Speed: 3935.84 samples/sec#011loss=5.889006\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[70] Batch[10] avg_epoch_loss=5.882707\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=70, batch=10 train loss <loss>=5.8751476287841795\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[70] Batch [10]#011Speed: 3248.27 samples/sec#011loss=5.875148\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] processed a total of 684 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081785.2127552, \"EndTime\": 1681081785.6880703, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.2328395843506, \"count\": 1, \"min\": 475.2328395843506, \"max\": 475.2328395843506}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1438.8411529370344 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #progress_metric: host=algo-1, completed 17.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=70, train loss <loss>=5.882706598802046\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_efd842a9-b5e2-4b2d-9b00-a95109ea83e0-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081785.6881676, \"EndTime\": 1681081785.6979382, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.093046188354492, \"count\": 1, \"min\": 9.093046188354492, \"max\": 9.093046188354492}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] Epoch[71] Batch[0] avg_epoch_loss=5.902742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:45 INFO 139996980188992] #quality_metric: host=algo-1, epoch=71, batch=0 train loss <loss>=5.9027419090271\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[71] Batch[5] avg_epoch_loss=5.835278\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=71, batch=5 train loss <loss>=5.835277716318767\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[71] Batch [5]#011Speed: 4059.28 samples/sec#011loss=5.835278\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] processed a total of 615 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081785.6980228, \"EndTime\": 1681081786.0850227, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 386.91234588623047, \"count\": 1, \"min\": 386.91234588623047, \"max\": 386.91234588623047}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1588.9120113142742 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #progress_metric: host=algo-1, completed 18.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=71, train loss <loss>=5.854572820663452\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_7b7ef184-4e06-4b48-8c40-a1b7da2e473c-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081786.0851111, \"EndTime\": 1681081786.0950396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 9.243488311767578, \"count\": 1, \"min\": 9.243488311767578, \"max\": 9.243488311767578}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[72] Batch[0] avg_epoch_loss=5.819775\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=72, batch=0 train loss <loss>=5.819775104522705\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[72] Batch[5] avg_epoch_loss=5.821745\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=72, batch=5 train loss <loss>=5.821744680404663\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[72] Batch [5]#011Speed: 4378.22 samples/sec#011loss=5.821745\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[72] Batch[10] avg_epoch_loss=5.874052\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=72, batch=10 train loss <loss>=5.936820602416992\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[72] Batch [10]#011Speed: 3729.31 samples/sec#011loss=5.936821\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081786.0951266, \"EndTime\": 1681081786.5056448, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 410.4294776916504, \"count\": 1, \"min\": 410.4294776916504, \"max\": 410.4294776916504}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1590.42135055252 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #progress_metric: host=algo-1, completed 18.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=72, train loss <loss>=5.874051917682994\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[73] Batch[0] avg_epoch_loss=5.959143\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=73, batch=0 train loss <loss>=5.959143161773682\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[73] Batch[5] avg_epoch_loss=5.879128\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=73, batch=5 train loss <loss>=5.879128058751424\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[73] Batch [5]#011Speed: 4382.96 samples/sec#011loss=5.879128\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[73] Batch[10] avg_epoch_loss=5.871936\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=73, batch=10 train loss <loss>=5.863305282592774\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] Epoch[73] Batch [10]#011Speed: 3563.78 samples/sec#011loss=5.863305\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] processed a total of 658 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081786.5057452, \"EndTime\": 1681081786.9235306, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 417.16909408569336, \"count\": 1, \"min\": 417.16909408569336, \"max\": 417.16909408569336}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1576.666389783111 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #progress_metric: host=algo-1, completed 18.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] #quality_metric: host=algo-1, epoch=73, train loss <loss>=5.87193588777022\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:46 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[74] Batch[0] avg_epoch_loss=5.933910\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=74, batch=0 train loss <loss>=5.933910369873047\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[74] Batch[5] avg_epoch_loss=5.851221\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=74, batch=5 train loss <loss>=5.851221481959025\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[74] Batch [5]#011Speed: 4149.96 samples/sec#011loss=5.851221\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081786.9236386, \"EndTime\": 1681081787.3241575, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 399.8448848724365, \"count\": 1, \"min\": 399.8448848724365, \"max\": 399.8448848724365}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1592.4512124107373 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #progress_metric: host=algo-1, completed 18.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=74, train loss <loss>=5.846787691116333\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_baa8b2ec-591a-4d56-bb06-8867ee33e7ce-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081787.3242688, \"EndTime\": 1681081787.33741, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.046575546264648, \"count\": 1, \"min\": 12.046575546264648, \"max\": 12.046575546264648}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[75] Batch[0] avg_epoch_loss=5.621769\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=75, batch=0 train loss <loss>=5.621769428253174\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[75] Batch[5] avg_epoch_loss=5.805673\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=75, batch=5 train loss <loss>=5.805673440297444\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[75] Batch [5]#011Speed: 4403.79 samples/sec#011loss=5.805673\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[75] Batch[10] avg_epoch_loss=5.839675\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=75, batch=10 train loss <loss>=5.880475807189941\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Epoch[75] Batch [10]#011Speed: 3656.85 samples/sec#011loss=5.880476\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081787.337491, \"EndTime\": 1681081787.7982817, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 460.71434020996094, \"count\": 1, \"min\": 460.71434020996094, \"max\": 460.71434020996094}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1429.9145821424045 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #progress_metric: host=algo-1, completed 19.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] #quality_metric: host=algo-1, epoch=75, train loss <loss>=5.839674516157671\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:47 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_c6cb5699-bd52-4592-9000-da4c496bef91-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081787.7983875, \"EndTime\": 1681081787.8112917, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.035608291625977, \"count\": 1, \"min\": 12.035608291625977, \"max\": 12.035608291625977}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[76] Batch[0] avg_epoch_loss=5.898286\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=76, batch=0 train loss <loss>=5.89828634262085\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[76] Batch[5] avg_epoch_loss=5.843920\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=76, batch=5 train loss <loss>=5.843920310338338\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[76] Batch [5]#011Speed: 4400.16 samples/sec#011loss=5.843920\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081787.8113737, \"EndTime\": 1681081788.2479763, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 436.52939796447754, \"count\": 1, \"min\": 436.52939796447754, \"max\": 436.52939796447754}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1442.6864512874326 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #progress_metric: host=algo-1, completed 19.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=76, train loss <loss>=5.847045421600342\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[77] Batch[0] avg_epoch_loss=5.975747\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=77, batch=0 train loss <loss>=5.975747108459473\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[77] Batch[5] avg_epoch_loss=5.758277\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=77, batch=5 train loss <loss>=5.758276621500651\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[77] Batch [5]#011Speed: 4343.55 samples/sec#011loss=5.758277\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[77] Batch[10] avg_epoch_loss=5.779777\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=77, batch=10 train loss <loss>=5.805578422546387\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[77] Batch [10]#011Speed: 3963.41 samples/sec#011loss=5.805578\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] processed a total of 646 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081788.2480867, \"EndTime\": 1681081788.6667125, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 418.00761222839355, \"count\": 1, \"min\": 418.00761222839355, \"max\": 418.00761222839355}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1544.8589476565016 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #progress_metric: host=algo-1, completed 19.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=77, train loss <loss>=5.779777440157804\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_82c64f5b-49a0-40f1-a3ba-66a55bdc41f8-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081788.6668136, \"EndTime\": 1681081788.680254, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.60232925415039, \"count\": 1, \"min\": 12.60232925415039, \"max\": 12.60232925415039}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] Epoch[78] Batch[0] avg_epoch_loss=5.836874\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:48 INFO 139996980188992] #quality_metric: host=algo-1, epoch=78, batch=0 train loss <loss>=5.836874485015869\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[78] Batch[5] avg_epoch_loss=5.885115\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=78, batch=5 train loss <loss>=5.885114590326945\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[78] Batch [5]#011Speed: 4091.07 samples/sec#011loss=5.885115\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[78] Batch[10] avg_epoch_loss=5.856750\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=78, batch=10 train loss <loss>=5.822712993621826\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[78] Batch [10]#011Speed: 3896.30 samples/sec#011loss=5.822713\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081788.6803265, \"EndTime\": 1681081789.149625, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 469.2263603210449, \"count\": 1, \"min\": 469.2263603210449, \"max\": 469.2263603210449}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1382.697079429936 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #progress_metric: host=algo-1, completed 19.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=78, train loss <loss>=5.856750228188255\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[79] Batch[0] avg_epoch_loss=5.923400\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=79, batch=0 train loss <loss>=5.923399925231934\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[79] Batch[5] avg_epoch_loss=5.959464\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=79, batch=5 train loss <loss>=5.9594636758168535\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[79] Batch [5]#011Speed: 4339.61 samples/sec#011loss=5.959464\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[79] Batch[10] avg_epoch_loss=5.929942\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=79, batch=10 train loss <loss>=5.894516563415527\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[79] Batch [10]#011Speed: 3728.25 samples/sec#011loss=5.894517\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081789.149726, \"EndTime\": 1681081789.62444, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.1837978363037, \"count\": 1, \"min\": 474.1837978363037, \"max\": 474.1837978363037}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1374.513943241893 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #progress_metric: host=algo-1, completed 20.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=79, train loss <loss>=5.9299422610889785\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[80] Batch[0] avg_epoch_loss=5.961763\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=80, batch=0 train loss <loss>=5.96176290512085\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[80] Batch[5] avg_epoch_loss=5.916922\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] #quality_metric: host=algo-1, epoch=80, batch=5 train loss <loss>=5.916922489802043\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:49 INFO 139996980188992] Epoch[80] Batch [5]#011Speed: 3573.75 samples/sec#011loss=5.916922\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081789.6245525, \"EndTime\": 1681081790.0366564, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.5180969238281, \"count\": 1, \"min\": 411.5180969238281, \"max\": 411.5180969238281}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1535.1170031515414 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #progress_metric: host=algo-1, completed 20.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=80, train loss <loss>=5.87856879234314\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[81] Batch[0] avg_epoch_loss=5.966968\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=81, batch=0 train loss <loss>=5.966968059539795\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[81] Batch[5] avg_epoch_loss=5.896734\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=81, batch=5 train loss <loss>=5.896734237670898\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[81] Batch [5]#011Speed: 3586.63 samples/sec#011loss=5.896734\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081790.0367768, \"EndTime\": 1681081790.4669108, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.3527603149414, \"count\": 1, \"min\": 429.3527603149414, \"max\": 429.3527603149414}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1476.1117435533633 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #progress_metric: host=algo-1, completed 20.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=81, train loss <loss>=5.890421962738037\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[82] Batch[0] avg_epoch_loss=5.748617\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=82, batch=0 train loss <loss>=5.748616695404053\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[82] Batch[5] avg_epoch_loss=5.869690\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=82, batch=5 train loss <loss>=5.869690418243408\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[82] Batch [5]#011Speed: 3757.25 samples/sec#011loss=5.869690\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[82] Batch[10] avg_epoch_loss=5.870181\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=82, batch=10 train loss <loss>=5.870770072937011\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] Epoch[82] Batch [10]#011Speed: 4096.63 samples/sec#011loss=5.870770\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081790.4670074, \"EndTime\": 1681081790.9004424, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 432.6629638671875, \"count\": 1, \"min\": 432.6629638671875, \"max\": 432.6629638671875}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1485.577921938424 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #progress_metric: host=algo-1, completed 20.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] #quality_metric: host=algo-1, epoch=82, train loss <loss>=5.870181170376864\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:50 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[83] Batch[0] avg_epoch_loss=6.076087\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=83, batch=0 train loss <loss>=6.076087474822998\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[83] Batch[5] avg_epoch_loss=5.853999\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=83, batch=5 train loss <loss>=5.853999296824138\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[83] Batch [5]#011Speed: 4286.52 samples/sec#011loss=5.853999\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[83] Batch[10] avg_epoch_loss=5.823549\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=83, batch=10 train loss <loss>=5.787009239196777\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[83] Batch [10]#011Speed: 3474.20 samples/sec#011loss=5.787009\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] processed a total of 677 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081790.9005466, \"EndTime\": 1681081791.325478, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 424.29518699645996, \"count\": 1, \"min\": 424.29518699645996, \"max\": 424.29518699645996}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1595.0288937596265 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #progress_metric: host=algo-1, completed 21.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=83, train loss <loss>=5.823549270629883\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[84] Batch[0] avg_epoch_loss=5.813051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=84, batch=0 train loss <loss>=5.813051223754883\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[84] Batch[5] avg_epoch_loss=5.788347\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=84, batch=5 train loss <loss>=5.7883468468983965\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] Epoch[84] Batch [5]#011Speed: 3610.83 samples/sec#011loss=5.788347\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] processed a total of 637 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081791.3255777, \"EndTime\": 1681081791.7736716, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.50523567199707, \"count\": 1, \"min\": 447.50523567199707, \"max\": 447.50523567199707}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1422.9580500399975 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #progress_metric: host=algo-1, completed 21.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] #quality_metric: host=algo-1, epoch=84, train loss <loss>=5.799650192260742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:51 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[85] Batch[0] avg_epoch_loss=5.714373\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=85, batch=0 train loss <loss>=5.7143731117248535\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[85] Batch[5] avg_epoch_loss=5.835476\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=85, batch=5 train loss <loss>=5.835476477940877\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[85] Batch [5]#011Speed: 4377.37 samples/sec#011loss=5.835476\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[85] Batch[10] avg_epoch_loss=5.828489\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=85, batch=10 train loss <loss>=5.820105075836182\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[85] Batch [10]#011Speed: 3687.84 samples/sec#011loss=5.820105\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081791.773769, \"EndTime\": 1681081792.200666, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 426.1784553527832, \"count\": 1, \"min\": 426.1784553527832, \"max\": 426.1784553527832}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1536.228182747952 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #progress_metric: host=algo-1, completed 21.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=85, train loss <loss>=5.828489476984197\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[86] Batch[0] avg_epoch_loss=6.483047\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=86, batch=0 train loss <loss>=6.4830474853515625\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[86] Batch[5] avg_epoch_loss=6.024565\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=86, batch=5 train loss <loss>=6.02456529935201\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[86] Batch [5]#011Speed: 4246.72 samples/sec#011loss=6.024565\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[86] Batch[10] avg_epoch_loss=5.965321\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=86, batch=10 train loss <loss>=5.894227695465088\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[86] Batch [10]#011Speed: 3992.10 samples/sec#011loss=5.894228\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] processed a total of 644 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081792.2007997, \"EndTime\": 1681081792.6268208, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.4188537597656, \"count\": 1, \"min\": 425.4188537597656, \"max\": 425.4188537597656}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1513.1517766761917 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #progress_metric: host=algo-1, completed 21.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=86, train loss <loss>=5.965320933948863\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[87] Batch[0] avg_epoch_loss=5.993564\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=87, batch=0 train loss <loss>=5.993563652038574\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[87] Batch[5] avg_epoch_loss=5.938447\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] #quality_metric: host=algo-1, epoch=87, batch=5 train loss <loss>=5.938446680704753\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:52 INFO 139996980188992] Epoch[87] Batch [5]#011Speed: 3692.20 samples/sec#011loss=5.938447\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081792.626949, \"EndTime\": 1681081793.0278468, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 400.30479431152344, \"count\": 1, \"min\": 400.30479431152344, \"max\": 400.30479431152344}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1470.7691487030986 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #progress_metric: host=algo-1, completed 22.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=87, train loss <loss>=5.882926797866821\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[88] Batch[0] avg_epoch_loss=5.923661\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=88, batch=0 train loss <loss>=5.923660755157471\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[88] Batch[5] avg_epoch_loss=5.854497\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=88, batch=5 train loss <loss>=5.854496558507283\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[88] Batch [5]#011Speed: 3824.00 samples/sec#011loss=5.854497\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[88] Batch[10] avg_epoch_loss=5.836443\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=88, batch=10 train loss <loss>=5.814779663085938\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[88] Batch [10]#011Speed: 3683.69 samples/sec#011loss=5.814780\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] processed a total of 648 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081793.0279593, \"EndTime\": 1681081793.504581, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.9693145751953, \"count\": 1, \"min\": 475.9693145751953, \"max\": 475.9693145751953}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1360.944196415159 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #progress_metric: host=algo-1, completed 22.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=88, train loss <loss>=5.8364434242248535\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[89] Batch[0] avg_epoch_loss=5.878276\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=89, batch=0 train loss <loss>=5.8782758712768555\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[89] Batch[5] avg_epoch_loss=5.802396\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=89, batch=5 train loss <loss>=5.802396456400554\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] Epoch[89] Batch [5]#011Speed: 4013.90 samples/sec#011loss=5.802396\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081793.504695, \"EndTime\": 1681081793.912571, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 407.1636199951172, \"count\": 1, \"min\": 407.1636199951172, \"max\": 407.1636199951172}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1500.0314578645832 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #progress_metric: host=algo-1, completed 22.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] #quality_metric: host=algo-1, epoch=89, train loss <loss>=5.792674398422241\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:53 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[90] Batch[0] avg_epoch_loss=5.850226\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=90, batch=0 train loss <loss>=5.850225925445557\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[90] Batch[5] avg_epoch_loss=5.852645\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=90, batch=5 train loss <loss>=5.852644840876262\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[90] Batch [5]#011Speed: 4343.83 samples/sec#011loss=5.852645\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] processed a total of 611 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081793.9126787, \"EndTime\": 1681081794.3551316, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.34790992736816, \"count\": 1, \"min\": 440.34790992736816, \"max\": 440.34790992736816}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1387.018803385497 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #progress_metric: host=algo-1, completed 22.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=90, train loss <loss>=5.810718584060669\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[91] Batch[0] avg_epoch_loss=5.788849\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=91, batch=0 train loss <loss>=5.788848876953125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[91] Batch[5] avg_epoch_loss=5.738206\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=91, batch=5 train loss <loss>=5.738206068674724\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Epoch[91] Batch [5]#011Speed: 3780.98 samples/sec#011loss=5.738206\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] processed a total of 640 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081794.3552382, \"EndTime\": 1681081794.8312225, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.24023056030273, \"count\": 1, \"min\": 475.24023056030273, \"max\": 475.24023056030273}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1346.2287554394952 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #progress_metric: host=algo-1, completed 23.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] #quality_metric: host=algo-1, epoch=91, train loss <loss>=5.749678707122802\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:54 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_b190f931-c42b-485e-b249-ac38feb1f897-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081794.8313262, \"EndTime\": 1681081794.8407547, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.704900741577148, \"count\": 1, \"min\": 8.704900741577148, \"max\": 8.704900741577148}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[92] Batch[0] avg_epoch_loss=5.793288\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=92, batch=0 train loss <loss>=5.793288230895996\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[92] Batch[5] avg_epoch_loss=5.793618\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=92, batch=5 train loss <loss>=5.793617566426595\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[92] Batch [5]#011Speed: 4200.74 samples/sec#011loss=5.793618\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081794.8408384, \"EndTime\": 1681081795.2374682, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 396.5566158294678, \"count\": 1, \"min\": 396.5566158294678, \"max\": 396.5566158294678}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1574.1529164079548 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #progress_metric: host=algo-1, completed 23.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=92, train loss <loss>=5.763747692108154\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[93] Batch[0] avg_epoch_loss=5.782355\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=93, batch=0 train loss <loss>=5.782355308532715\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[93] Batch[5] avg_epoch_loss=5.811879\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=93, batch=5 train loss <loss>=5.811878522237142\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[93] Batch [5]#011Speed: 4284.48 samples/sec#011loss=5.811879\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081795.2379, \"EndTime\": 1681081795.7056236, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 466.94207191467285, \"count\": 1, \"min\": 466.94207191467285, \"max\": 466.94207191467285}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1368.0010290019293 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #progress_metric: host=algo-1, completed 23.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=93, train loss <loss>=5.787262582778931\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] Epoch[94] Batch[0] avg_epoch_loss=5.843110\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:55 INFO 139996980188992] #quality_metric: host=algo-1, epoch=94, batch=0 train loss <loss>=5.843110084533691\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[94] Batch[5] avg_epoch_loss=5.769968\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=94, batch=5 train loss <loss>=5.769967873891194\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[94] Batch [5]#011Speed: 4384.15 samples/sec#011loss=5.769968\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] processed a total of 635 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081795.7057316, \"EndTime\": 1681081796.1127856, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 406.2051773071289, \"count\": 1, \"min\": 406.2051773071289, \"max\": 406.2051773071289}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1562.5991386112996 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #progress_metric: host=algo-1, completed 23.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=94, train loss <loss>=5.759150648117066\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[95] Batch[0] avg_epoch_loss=5.807452\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=95, batch=0 train loss <loss>=5.8074517250061035\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[95] Batch[5] avg_epoch_loss=5.740484\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=95, batch=5 train loss <loss>=5.74048376083374\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[95] Batch [5]#011Speed: 4082.92 samples/sec#011loss=5.740484\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[95] Batch[10] avg_epoch_loss=5.777958\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=95, batch=10 train loss <loss>=5.822927379608155\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[95] Batch [10]#011Speed: 3329.87 samples/sec#011loss=5.822927\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] processed a total of 659 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081796.1128948, \"EndTime\": 1681081796.5387235, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.00805854797363, \"count\": 1, \"min\": 425.00805854797363, \"max\": 425.00805854797363}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1549.9840944240761 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #progress_metric: host=algo-1, completed 24.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=95, train loss <loss>=5.777958133003929\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[96] Batch[0] avg_epoch_loss=6.200703\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=96, batch=0 train loss <loss>=6.200702667236328\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[96] Batch[5] avg_epoch_loss=5.949427\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=96, batch=5 train loss <loss>=5.949427445729573\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[96] Batch [5]#011Speed: 4415.25 samples/sec#011loss=5.949427\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[96] Batch[10] avg_epoch_loss=5.948959\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=96, batch=10 train loss <loss>=5.948396110534668\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] Epoch[96] Batch [10]#011Speed: 3263.59 samples/sec#011loss=5.948396\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] processed a total of 695 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081796.5388258, \"EndTime\": 1681081796.9512656, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 411.73839569091797, \"count\": 1, \"min\": 411.73839569091797, \"max\": 411.73839569091797}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1687.0838228604434 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #progress_metric: host=algo-1, completed 24.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] #quality_metric: host=algo-1, epoch=96, train loss <loss>=5.948958657004616\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:56 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[97] Batch[0] avg_epoch_loss=5.776417\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=97, batch=0 train loss <loss>=5.776417255401611\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[97] Batch[5] avg_epoch_loss=5.837436\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=97, batch=5 train loss <loss>=5.837435642878215\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[97] Batch [5]#011Speed: 4499.89 samples/sec#011loss=5.837436\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] processed a total of 589 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081796.9514258, \"EndTime\": 1681081797.348288, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 396.1658477783203, \"count\": 1, \"min\": 396.1658477783203, \"max\": 396.1658477783203}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1486.093724941725 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #progress_metric: host=algo-1, completed 24.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=97, train loss <loss>=5.87087116241455\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[98] Batch[0] avg_epoch_loss=5.806271\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=98, batch=0 train loss <loss>=5.806271076202393\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[98] Batch[5] avg_epoch_loss=5.729772\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=98, batch=5 train loss <loss>=5.729772090911865\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[98] Batch [5]#011Speed: 3970.59 samples/sec#011loss=5.729772\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[98] Batch[10] avg_epoch_loss=5.742654\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=98, batch=10 train loss <loss>=5.758112525939941\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Epoch[98] Batch [10]#011Speed: 3815.97 samples/sec#011loss=5.758113\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081797.3484077, \"EndTime\": 1681081797.8272579, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 478.1761169433594, \"count\": 1, \"min\": 478.1761169433594, \"max\": 478.1761169433594}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1348.3926803812326 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #progress_metric: host=algo-1, completed 24.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] #quality_metric: host=algo-1, epoch=98, train loss <loss>=5.742654106833718\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:57 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_96cde728-1849-4ee4-83ce-0bb1f5698edf-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081797.8273718, \"EndTime\": 1681081797.8367343, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.748054504394531, \"count\": 1, \"min\": 8.748054504394531, \"max\": 8.748054504394531}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[99] Batch[0] avg_epoch_loss=5.772829\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=99, batch=0 train loss <loss>=5.772829055786133\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[99] Batch[5] avg_epoch_loss=5.732756\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=99, batch=5 train loss <loss>=5.7327555020650225\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[99] Batch [5]#011Speed: 3728.97 samples/sec#011loss=5.732756\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] processed a total of 605 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081797.8368356, \"EndTime\": 1681081798.252053, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 415.13895988464355, \"count\": 1, \"min\": 415.13895988464355, \"max\": 415.13895988464355}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1456.2276843229445 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #progress_metric: host=algo-1, completed 25.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=99, train loss <loss>=5.722214031219482\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_568de881-4e48-4169-8182-354b6082514b-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081798.252322, \"EndTime\": 1681081798.2654567, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 12.092113494873047, \"count\": 1, \"min\": 12.092113494873047, \"max\": 12.092113494873047}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[100] Batch[0] avg_epoch_loss=5.762486\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=100, batch=0 train loss <loss>=5.762486457824707\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[100] Batch[5] avg_epoch_loss=5.794328\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=100, batch=5 train loss <loss>=5.794328371683757\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[100] Batch [5]#011Speed: 4403.24 samples/sec#011loss=5.794328\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[100] Batch[10] avg_epoch_loss=5.752473\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=100, batch=10 train loss <loss>=5.702245998382568\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] Epoch[100] Batch [10]#011Speed: 3115.88 samples/sec#011loss=5.702246\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] processed a total of 669 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081798.2655497, \"EndTime\": 1681081798.7400684, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.43318367004395, \"count\": 1, \"min\": 474.43318367004395, \"max\": 474.43318367004395}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1409.6213178150485 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #progress_metric: host=algo-1, completed 25.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] #quality_metric: host=algo-1, epoch=100, train loss <loss>=5.752472747455943\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:58 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[101] Batch[0] avg_epoch_loss=6.210956\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=101, batch=0 train loss <loss>=6.210955619812012\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[101] Batch[5] avg_epoch_loss=5.874951\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=101, batch=5 train loss <loss>=5.874951124191284\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[101] Batch [5]#011Speed: 4258.65 samples/sec#011loss=5.874951\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[101] Batch[10] avg_epoch_loss=5.855406\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=101, batch=10 train loss <loss>=5.83195161819458\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[101] Batch [10]#011Speed: 3493.62 samples/sec#011loss=5.831952\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] processed a total of 663 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081798.7401779, \"EndTime\": 1681081799.1718779, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 431.0619831085205, \"count\": 1, \"min\": 431.0619831085205, \"max\": 431.0619831085205}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1537.4759644251599 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #progress_metric: host=algo-1, completed 25.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=101, train loss <loss>=5.855405894192782\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[102] Batch[0] avg_epoch_loss=6.067243\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=102, batch=0 train loss <loss>=6.067242622375488\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[102] Batch[5] avg_epoch_loss=5.819685\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=102, batch=5 train loss <loss>=5.819684743881226\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[102] Batch [5]#011Speed: 4110.73 samples/sec#011loss=5.819685\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[102] Batch[10] avg_epoch_loss=5.809015\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=102, batch=10 train loss <loss>=5.796211910247803\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[102] Batch [10]#011Speed: 2833.30 samples/sec#011loss=5.796212\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] processed a total of 686 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081799.171986, \"EndTime\": 1681081799.61669, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 444.08154487609863, \"count\": 1, \"min\": 444.08154487609863, \"max\": 444.08154487609863}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1544.210841436888 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #progress_metric: host=algo-1, completed 25.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=102, train loss <loss>=5.809015274047852\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[103] Batch[0] avg_epoch_loss=5.705469\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=103, batch=0 train loss <loss>=5.705468654632568\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[103] Batch[5] avg_epoch_loss=5.680554\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] #quality_metric: host=algo-1, epoch=103, batch=5 train loss <loss>=5.680554469426473\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:09:59 INFO 139996980188992] Epoch[103] Batch [5]#011Speed: 3742.13 samples/sec#011loss=5.680554\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[103] Batch[10] avg_epoch_loss=5.676871\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=103, batch=10 train loss <loss>=5.672451305389404\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[103] Batch [10]#011Speed: 2674.71 samples/sec#011loss=5.672451\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] processed a total of 673 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081799.6167967, \"EndTime\": 1681081800.089256, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 471.6153144836426, \"count\": 1, \"min\": 471.6153144836426, \"max\": 471.6153144836426}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1426.5121708277134 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #progress_metric: host=algo-1, completed 26.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=103, train loss <loss>=5.676871213045987\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_5a45e5d8-bc96-47e3-946c-a8d167c9bb6e-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081800.0893683, \"EndTime\": 1681081800.0983396, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.274555206298828, \"count\": 1, \"min\": 8.274555206298828, \"max\": 8.274555206298828}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[104] Batch[0] avg_epoch_loss=5.712182\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=104, batch=0 train loss <loss>=5.712181568145752\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[104] Batch[5] avg_epoch_loss=5.687123\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=104, batch=5 train loss <loss>=5.6871231396993\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[104] Batch [5]#011Speed: 3627.75 samples/sec#011loss=5.687123\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] processed a total of 621 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081800.0984468, \"EndTime\": 1681081800.5199246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 421.3895797729492, \"count\": 1, \"min\": 421.3895797729492, \"max\": 421.3895797729492}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1473.1230711890519 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #progress_metric: host=algo-1, completed 26.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=104, train loss <loss>=5.564499092102051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] best epoch loss so far\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/state_24cc4697-4a08-401f-ac27-e30a7bbb6aef-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081800.520038, \"EndTime\": 1681081800.529727, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.serialize.time\": {\"sum\": 8.909940719604492, \"count\": 1, \"min\": 8.909940719604492, \"max\": 8.909940719604492}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[105] Batch[0] avg_epoch_loss=5.745593\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=105, batch=0 train loss <loss>=5.7455925941467285\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[105] Batch[5] avg_epoch_loss=5.672619\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=105, batch=5 train loss <loss>=5.672619183858235\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[105] Batch [5]#011Speed: 3438.55 samples/sec#011loss=5.672619\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[105] Batch[10] avg_epoch_loss=5.666684\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=105, batch=10 train loss <loss>=5.659562873840332\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] Epoch[105] Batch [10]#011Speed: 3840.21 samples/sec#011loss=5.659563\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081800.5298374, \"EndTime\": 1681081800.9688246, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 438.9007091522217, \"count\": 1, \"min\": 438.9007091522217, \"max\": 438.9007091522217}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1480.4082352973119 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #progress_metric: host=algo-1, completed 26.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] #quality_metric: host=algo-1, epoch=105, train loss <loss>=5.666684497486461\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:00 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[106] Batch[0] avg_epoch_loss=5.756239\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=106, batch=0 train loss <loss>=5.75623893737793\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[106] Batch[5] avg_epoch_loss=5.698811\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=106, batch=5 train loss <loss>=5.698811213175456\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[106] Batch [5]#011Speed: 4064.96 samples/sec#011loss=5.698811\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[106] Batch[10] avg_epoch_loss=5.696694\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=106, batch=10 train loss <loss>=5.694153118133545\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[106] Batch [10]#011Speed: 2000.55 samples/sec#011loss=5.694153\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] processed a total of 678 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081800.9689424, \"EndTime\": 1681081801.5063176, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 536.5979671478271, \"count\": 1, \"min\": 536.5979671478271, \"max\": 536.5979671478271}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1263.172298976532 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #progress_metric: host=algo-1, completed 26.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=106, train loss <loss>=5.6966938972473145\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[107] Batch[0] avg_epoch_loss=5.692836\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=107, batch=0 train loss <loss>=5.692835807800293\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[107] Batch[5] avg_epoch_loss=5.614740\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=107, batch=5 train loss <loss>=5.614739894866943\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] Epoch[107] Batch [5]#011Speed: 3579.43 samples/sec#011loss=5.614740\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] processed a total of 630 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081801.5064123, \"EndTime\": 1681081801.9754694, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 468.58882904052734, \"count\": 1, \"min\": 468.58882904052734, \"max\": 468.58882904052734}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1343.740284572277 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #progress_metric: host=algo-1, completed 27.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] #quality_metric: host=algo-1, epoch=107, train loss <loss>=5.6345714092254635\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:01 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[108] Batch[0] avg_epoch_loss=5.744138\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=108, batch=0 train loss <loss>=5.744137763977051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[108] Batch[5] avg_epoch_loss=5.683982\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=108, batch=5 train loss <loss>=5.683982054392497\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[108] Batch [5]#011Speed: 3644.37 samples/sec#011loss=5.683982\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] processed a total of 625 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081801.9756627, \"EndTime\": 1681081802.3927245, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 415.6458377838135, \"count\": 1, \"min\": 415.6458377838135, \"max\": 415.6458377838135}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1501.9686753725357 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #progress_metric: host=algo-1, completed 27.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=108, train loss <loss>=5.704415798187256\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[109] Batch[0] avg_epoch_loss=5.736450\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=109, batch=0 train loss <loss>=5.7364501953125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[109] Batch[5] avg_epoch_loss=5.687662\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=109, batch=5 train loss <loss>=5.687662363052368\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[109] Batch [5]#011Speed: 3878.18 samples/sec#011loss=5.687662\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[109] Batch[10] avg_epoch_loss=5.738939\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=109, batch=10 train loss <loss>=5.800470447540283\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] Epoch[109] Batch [10]#011Speed: 3598.20 samples/sec#011loss=5.800470\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] processed a total of 653 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081802.3930771, \"EndTime\": 1681081802.851379, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 457.2441577911377, \"count\": 1, \"min\": 457.2441577911377, \"max\": 457.2441577911377}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1427.6163354789344 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #progress_metric: host=algo-1, completed 27.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] #quality_metric: host=algo-1, epoch=109, train loss <loss>=5.738938765092329\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:02 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[110] Batch[0] avg_epoch_loss=5.711717\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=110, batch=0 train loss <loss>=5.711717128753662\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[110] Batch[5] avg_epoch_loss=5.770579\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=110, batch=5 train loss <loss>=5.770578940709432\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[110] Batch [5]#011Speed: 2957.43 samples/sec#011loss=5.770579\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] processed a total of 607 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081802.851487, \"EndTime\": 1681081803.367143, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 515.1286125183105, \"count\": 1, \"min\": 515.1286125183105, \"max\": 515.1286125183105}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1178.004106912995 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #progress_metric: host=algo-1, completed 27.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=110, train loss <loss>=5.736051940917969\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[111] Batch[0] avg_epoch_loss=5.847408\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=111, batch=0 train loss <loss>=5.847407817840576\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[111] Batch[5] avg_epoch_loss=5.751646\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=111, batch=5 train loss <loss>=5.751645565032959\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] Epoch[111] Batch [5]#011Speed: 2862.57 samples/sec#011loss=5.751646\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] processed a total of 618 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081803.3672435, \"EndTime\": 1681081803.936941, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 567.1930313110352, \"count\": 1, \"min\": 567.1930313110352, \"max\": 567.1930313110352}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1089.2281812298822 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #progress_metric: host=algo-1, completed 28.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] #quality_metric: host=algo-1, epoch=111, train loss <loss>=5.756715726852417\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:03 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[112] Batch[0] avg_epoch_loss=5.660507\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=112, batch=0 train loss <loss>=5.660506725311279\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[112] Batch[5] avg_epoch_loss=5.699170\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=112, batch=5 train loss <loss>=5.699170192082723\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[112] Batch [5]#011Speed: 4265.08 samples/sec#011loss=5.699170\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081803.9370687, \"EndTime\": 1681081804.3963287, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 458.6331844329834, \"count\": 1, \"min\": 458.6331844329834, \"max\": 458.6331844329834}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1390.5087487288583 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #progress_metric: host=algo-1, completed 28.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=112, train loss <loss>=5.690479850769043\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[113] Batch[0] avg_epoch_loss=5.681543\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=113, batch=0 train loss <loss>=5.681543350219727\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[113] Batch[5] avg_epoch_loss=5.624453\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=113, batch=5 train loss <loss>=5.624452590942383\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] Epoch[113] Batch [5]#011Speed: 3613.54 samples/sec#011loss=5.624453\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] processed a total of 612 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081804.3964648, \"EndTime\": 1681081804.8322418, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 435.15849113464355, \"count\": 1, \"min\": 435.15849113464355, \"max\": 435.15849113464355}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1405.786452408948 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #progress_metric: host=algo-1, completed 28.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] #quality_metric: host=algo-1, epoch=113, train loss <loss>=5.638665437698364\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:04 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[114] Batch[0] avg_epoch_loss=5.752777\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=114, batch=0 train loss <loss>=5.752777099609375\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[114] Batch[5] avg_epoch_loss=5.684439\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=114, batch=5 train loss <loss>=5.684438546498616\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[114] Batch [5]#011Speed: 3444.73 samples/sec#011loss=5.684439\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081804.8323689, \"EndTime\": 1681081805.2472153, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 414.2429828643799, \"count\": 1, \"min\": 414.2429828643799, \"max\": 414.2429828643799}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1479.2092990074584 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #progress_metric: host=algo-1, completed 28.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=114, train loss <loss>=5.718462371826172\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[115] Batch[0] avg_epoch_loss=5.944872\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=115, batch=0 train loss <loss>=5.94487190246582\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[115] Batch[5] avg_epoch_loss=5.777314\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=115, batch=5 train loss <loss>=5.777314186096191\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[115] Batch [5]#011Speed: 3792.85 samples/sec#011loss=5.777314\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] processed a total of 638 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081805.247329, \"EndTime\": 1681081805.6488664, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 400.8791446685791, \"count\": 1, \"min\": 400.8791446685791, \"max\": 400.8791446685791}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1590.8322381855721 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #progress_metric: host=algo-1, completed 29.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=115, train loss <loss>=5.723788595199585\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] Epoch[116] Batch[0] avg_epoch_loss=5.652384\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:05 INFO 139996980188992] #quality_metric: host=algo-1, epoch=116, batch=0 train loss <loss>=5.652383804321289\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[116] Batch[5] avg_epoch_loss=5.716739\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=116, batch=5 train loss <loss>=5.716738700866699\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[116] Batch [5]#011Speed: 4301.46 samples/sec#011loss=5.716739\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] processed a total of 639 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081805.6489756, \"EndTime\": 1681081806.098905, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.2192268371582, \"count\": 1, \"min\": 449.2192268371582, \"max\": 449.2192268371582}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1421.7828812723135 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #progress_metric: host=algo-1, completed 29.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=116, train loss <loss>=5.718854379653931\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[117] Batch[0] avg_epoch_loss=5.564024\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=117, batch=0 train loss <loss>=5.564023971557617\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[117] Batch[5] avg_epoch_loss=5.609980\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=117, batch=5 train loss <loss>=5.609980424245198\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[117] Batch [5]#011Speed: 4117.36 samples/sec#011loss=5.609980\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] processed a total of 587 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081806.0990264, \"EndTime\": 1681081806.5476754, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 447.9959011077881, \"count\": 1, \"min\": 447.9959011077881, \"max\": 447.9959011077881}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1309.803750996162 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #progress_metric: host=algo-1, completed 29.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=117, train loss <loss>=5.658624982833862\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[118] Batch[0] avg_epoch_loss=5.730339\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=118, batch=0 train loss <loss>=5.7303385734558105\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[118] Batch[5] avg_epoch_loss=5.758966\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=118, batch=5 train loss <loss>=5.758965651194255\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] Epoch[118] Batch [5]#011Speed: 4374.34 samples/sec#011loss=5.758966\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] processed a total of 586 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081806.5477848, \"EndTime\": 1681081806.9416533, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 393.2216167449951, \"count\": 1, \"min\": 393.2216167449951, \"max\": 393.2216167449951}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1489.6007815691262 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #progress_metric: host=algo-1, completed 29.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] #quality_metric: host=algo-1, epoch=118, train loss <loss>=5.7379004001617435\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:06 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[119] Batch[0] avg_epoch_loss=5.612344\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=119, batch=0 train loss <loss>=5.612344264984131\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[119] Batch[5] avg_epoch_loss=5.636453\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=119, batch=5 train loss <loss>=5.63645339012146\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[119] Batch [5]#011Speed: 4461.71 samples/sec#011loss=5.636453\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] processed a total of 598 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081806.9417603, \"EndTime\": 1681081807.3829434, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 440.6149387359619, \"count\": 1, \"min\": 440.6149387359619, \"max\": 440.6149387359619}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1356.6516184447094 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #progress_metric: host=algo-1, completed 30.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=119, train loss <loss>=5.676292276382446\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[120] Batch[0] avg_epoch_loss=5.528094\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=120, batch=0 train loss <loss>=5.528094291687012\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[120] Batch[5] avg_epoch_loss=5.636742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=120, batch=5 train loss <loss>=5.636742273966472\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] Epoch[120] Batch [5]#011Speed: 4438.06 samples/sec#011loss=5.636742\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081807.3830528, \"EndTime\": 1681081807.7885473, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 404.7262668609619, \"count\": 1, \"min\": 404.7262668609619, \"max\": 404.7262668609619}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1528.8244699143102 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #progress_metric: host=algo-1, completed 30.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] #quality_metric: host=algo-1, epoch=120, train loss <loss>=5.639361047744751\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:07 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[121] Batch[0] avg_epoch_loss=5.733016\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=121, batch=0 train loss <loss>=5.733016014099121\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[121] Batch[5] avg_epoch_loss=5.625408\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=121, batch=5 train loss <loss>=5.625407854715983\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[121] Batch [5]#011Speed: 4290.08 samples/sec#011loss=5.625408\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] processed a total of 585 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081807.7886539, \"EndTime\": 1681081808.2190247, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.7611713409424, \"count\": 1, \"min\": 429.7611713409424, \"max\": 429.7611713409424}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1360.6950133259688 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #progress_metric: host=algo-1, completed 30.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=121, train loss <loss>=5.641326189041138\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[122] Batch[0] avg_epoch_loss=5.689930\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=122, batch=0 train loss <loss>=5.689930438995361\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[122] Batch[5] avg_epoch_loss=5.660896\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=122, batch=5 train loss <loss>=5.660895824432373\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[122] Batch [5]#011Speed: 3521.86 samples/sec#011loss=5.660896\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[122] Batch[10] avg_epoch_loss=5.663100\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=122, batch=10 train loss <loss>=5.665745735168457\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[122] Batch [10]#011Speed: 3836.81 samples/sec#011loss=5.665746\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] processed a total of 649 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081808.219134, \"EndTime\": 1681081808.7090032, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 489.30811882019043, \"count\": 1, \"min\": 489.30811882019043, \"max\": 489.30811882019043}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1325.9065071677578 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #progress_metric: host=algo-1, completed 30.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=122, train loss <loss>=5.663100329312411\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] Epoch[123] Batch[0] avg_epoch_loss=5.493186\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:08 INFO 139996980188992] #quality_metric: host=algo-1, epoch=123, batch=0 train loss <loss>=5.493185997009277\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[123] Batch[5] avg_epoch_loss=5.642338\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=123, batch=5 train loss <loss>=5.642337878545125\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[123] Batch [5]#011Speed: 4420.61 samples/sec#011loss=5.642338\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[123] Batch[10] avg_epoch_loss=5.634610\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=123, batch=10 train loss <loss>=5.6253362655639645\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[123] Batch [10]#011Speed: 3608.98 samples/sec#011loss=5.625336\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] processed a total of 652 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081808.7091198, \"EndTime\": 1681081809.1335397, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 423.8018989562988, \"count\": 1, \"min\": 423.8018989562988, \"max\": 423.8018989562988}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1537.9113094160216 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #progress_metric: host=algo-1, completed 31.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=123, train loss <loss>=5.634609872644598\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[124] Batch[0] avg_epoch_loss=5.635563\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=124, batch=0 train loss <loss>=5.635562896728516\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[124] Batch[5] avg_epoch_loss=5.658900\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=124, batch=5 train loss <loss>=5.658899625142415\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[124] Batch [5]#011Speed: 4369.52 samples/sec#011loss=5.658900\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[124] Batch[10] avg_epoch_loss=5.696030\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=124, batch=10 train loss <loss>=5.740587329864502\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[124] Batch [10]#011Speed: 4003.49 samples/sec#011loss=5.740587\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] processed a total of 643 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081809.1336367, \"EndTime\": 1681081809.5965676, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 462.3117446899414, \"count\": 1, \"min\": 462.3117446899414, \"max\": 462.3117446899414}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1390.3581412589194 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #progress_metric: host=algo-1, completed 31.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=124, train loss <loss>=5.696030400016091\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[125] Batch[0] avg_epoch_loss=6.000487\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=125, batch=0 train loss <loss>=6.000486850738525\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[125] Batch[5] avg_epoch_loss=5.822138\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] #quality_metric: host=algo-1, epoch=125, batch=5 train loss <loss>=5.822137514750163\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:09 INFO 139996980188992] Epoch[125] Batch [5]#011Speed: 4442.28 samples/sec#011loss=5.822138\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[125] Batch[10] avg_epoch_loss=5.796839\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=125, batch=10 train loss <loss>=5.766480731964111\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[125] Batch [10]#011Speed: 3457.99 samples/sec#011loss=5.766481\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] processed a total of 668 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081809.5966744, \"EndTime\": 1681081810.0753179, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 478.05309295654297, \"count\": 1, \"min\": 478.05309295654297, \"max\": 478.05309295654297}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1396.8397166232096 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #progress_metric: host=algo-1, completed 31.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=125, train loss <loss>=5.79683897712014\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[126] Batch[0] avg_epoch_loss=5.605794\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=126, batch=0 train loss <loss>=5.6057939529418945\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[126] Batch[5] avg_epoch_loss=5.694728\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=126, batch=5 train loss <loss>=5.694727977116902\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[126] Batch [5]#011Speed: 4336.18 samples/sec#011loss=5.694728\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] processed a total of 634 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081810.0754309, \"EndTime\": 1681081810.4630857, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 387.0513439178467, \"count\": 1, \"min\": 387.0513439178467, \"max\": 387.0513439178467}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1637.3518381802855 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #progress_metric: host=algo-1, completed 31.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=126, train loss <loss>=5.6939304828643795\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[127] Batch[0] avg_epoch_loss=5.761157\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=127, batch=0 train loss <loss>=5.7611565589904785\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[127] Batch[5] avg_epoch_loss=5.656884\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=127, batch=5 train loss <loss>=5.65688419342041\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[127] Batch [5]#011Speed: 4004.06 samples/sec#011loss=5.656884\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[127] Batch[10] avg_epoch_loss=5.705268\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=127, batch=10 train loss <loss>=5.763328647613525\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] Epoch[127] Batch [10]#011Speed: 3470.48 samples/sec#011loss=5.763329\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081810.4631898, \"EndTime\": 1681081810.8932483, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 429.4395446777344, \"count\": 1, \"min\": 429.4395446777344, \"max\": 429.4395446777344}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1538.6218087069446 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #progress_metric: host=algo-1, completed 32.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] #quality_metric: host=algo-1, epoch=127, train loss <loss>=5.705268036235463\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:10 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[128] Batch[0] avg_epoch_loss=5.613980\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=128, batch=0 train loss <loss>=5.613979816436768\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[128] Batch[5] avg_epoch_loss=5.662560\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=128, batch=5 train loss <loss>=5.662559747695923\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[128] Batch [5]#011Speed: 4243.68 samples/sec#011loss=5.662560\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[128] Batch[10] avg_epoch_loss=5.631680\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=128, batch=10 train loss <loss>=5.5946249008178714\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[128] Batch [10]#011Speed: 3044.80 samples/sec#011loss=5.594625\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081810.8933582, \"EndTime\": 1681081811.386297, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 492.3841953277588, \"count\": 1, \"min\": 492.3841953277588, \"max\": 492.3841953277588}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1341.9830274492188 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #progress_metric: host=algo-1, completed 32.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=128, train loss <loss>=5.631680271842263\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[129] Batch[0] avg_epoch_loss=5.656523\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=129, batch=0 train loss <loss>=5.65652322769165\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[129] Batch[5] avg_epoch_loss=5.637524\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=129, batch=5 train loss <loss>=5.6375235716501875\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] Epoch[129] Batch [5]#011Speed: 4172.94 samples/sec#011loss=5.637524\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] processed a total of 599 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081811.3864143, \"EndTime\": 1681081811.7886243, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 401.4701843261719, \"count\": 1, \"min\": 401.4701843261719, \"max\": 401.4701843261719}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1491.409452721847 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #progress_metric: host=algo-1, completed 32.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] #quality_metric: host=algo-1, epoch=129, train loss <loss>=5.624921703338623\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:11 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[130] Batch[0] avg_epoch_loss=5.600804\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=130, batch=0 train loss <loss>=5.600803852081299\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[130] Batch[5] avg_epoch_loss=5.671938\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=130, batch=5 train loss <loss>=5.671938180923462\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[130] Batch [5]#011Speed: 3994.93 samples/sec#011loss=5.671938\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] processed a total of 617 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081811.7887325, \"EndTime\": 1681081812.1954722, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 405.95030784606934, \"count\": 1, \"min\": 405.95030784606934, \"max\": 405.95030784606934}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1518.8681748418999 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #progress_metric: host=algo-1, completed 32.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=130, train loss <loss>=5.652709817886352\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[131] Batch[0] avg_epoch_loss=5.578928\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=131, batch=0 train loss <loss>=5.578928470611572\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[131] Batch[5] avg_epoch_loss=5.641227\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=131, batch=5 train loss <loss>=5.641226530075073\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[131] Batch [5]#011Speed: 3903.78 samples/sec#011loss=5.641227\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[131] Batch[10] avg_epoch_loss=5.626735\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=131, batch=10 train loss <loss>=5.60934534072876\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[131] Batch [10]#011Speed: 3828.04 samples/sec#011loss=5.609345\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] processed a total of 655 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081812.1956894, \"EndTime\": 1681081812.6742609, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 477.9036045074463, \"count\": 1, \"min\": 477.9036045074463, \"max\": 477.9036045074463}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1370.1010693421665 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #progress_metric: host=algo-1, completed 33.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=131, train loss <loss>=5.626735080372203\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] Epoch[132] Batch[0] avg_epoch_loss=5.784945\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:12 INFO 139996980188992] #quality_metric: host=algo-1, epoch=132, batch=0 train loss <loss>=5.784945487976074\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[132] Batch[5] avg_epoch_loss=5.625470\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=132, batch=5 train loss <loss>=5.625470002492269\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[132] Batch [5]#011Speed: 4335.51 samples/sec#011loss=5.625470\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[132] Batch[10] avg_epoch_loss=5.613919\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=132, batch=10 train loss <loss>=5.600057888031006\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[132] Batch [10]#011Speed: 3258.23 samples/sec#011loss=5.600058\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081812.674367, \"EndTime\": 1681081813.1304705, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 455.3220272064209, \"count\": 1, \"min\": 455.3220272064209, \"max\": 455.3220272064209}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1484.1179715262442 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #progress_metric: host=algo-1, completed 33.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=132, train loss <loss>=5.613919041373513\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[133] Batch[0] avg_epoch_loss=5.554674\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=133, batch=0 train loss <loss>=5.55467414855957\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[133] Batch[5] avg_epoch_loss=5.599371\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=133, batch=5 train loss <loss>=5.599370718002319\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[133] Batch [5]#011Speed: 4095.76 samples/sec#011loss=5.599371\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[133] Batch[10] avg_epoch_loss=5.594612\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=133, batch=10 train loss <loss>=5.5889019012451175\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[133] Batch [10]#011Speed: 3746.07 samples/sec#011loss=5.588902\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] processed a total of 650 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081813.1305857, \"EndTime\": 1681081813.6061156, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 474.7354984283447, \"count\": 1, \"min\": 474.7354984283447, \"max\": 474.7354984283447}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1368.7126167872402 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #progress_metric: host=algo-1, completed 33.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=133, train loss <loss>=5.594612164930864\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] Epoch[134] Batch[0] avg_epoch_loss=5.655457\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:13 INFO 139996980188992] #quality_metric: host=algo-1, epoch=134, batch=0 train loss <loss>=5.65545654296875\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[134] Batch[5] avg_epoch_loss=5.677129\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=134, batch=5 train loss <loss>=5.6771291097005205\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[134] Batch [5]#011Speed: 4044.54 samples/sec#011loss=5.677129\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[134] Batch[10] avg_epoch_loss=5.625416\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=134, batch=10 train loss <loss>=5.563360214233398\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[134] Batch [10]#011Speed: 3951.01 samples/sec#011loss=5.563360\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] processed a total of 642 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081813.606227, \"EndTime\": 1681081814.0837727, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 476.72104835510254, \"count\": 1, \"min\": 476.72104835510254, \"max\": 476.72104835510254}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1346.2268646120542 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #progress_metric: host=algo-1, completed 33.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=134, train loss <loss>=5.625415975397283\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[135] Batch[0] avg_epoch_loss=5.673316\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=135, batch=0 train loss <loss>=5.67331600189209\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[135] Batch[5] avg_epoch_loss=5.617423\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=135, batch=5 train loss <loss>=5.617422898610433\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[135] Batch [5]#011Speed: 4486.47 samples/sec#011loss=5.617423\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[135] Batch[10] avg_epoch_loss=5.638627\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=135, batch=10 train loss <loss>=5.664072704315186\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[135] Batch [10]#011Speed: 3923.21 samples/sec#011loss=5.664073\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] processed a total of 645 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081814.0838902, \"EndTime\": 1681081814.504203, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 419.6817874908447, \"count\": 1, \"min\": 419.6817874908447, \"max\": 419.6817874908447}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1536.303548348261 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #progress_metric: host=algo-1, completed 34.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=135, train loss <loss>=5.638627355748957\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[136] Batch[0] avg_epoch_loss=5.578316\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=136, batch=0 train loss <loss>=5.5783162117004395\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[136] Batch[5] avg_epoch_loss=5.616897\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=136, batch=5 train loss <loss>=5.616896947224935\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] Epoch[136] Batch [5]#011Speed: 4398.75 samples/sec#011loss=5.616897\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] processed a total of 632 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081814.5043085, \"EndTime\": 1681081814.9540098, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 449.1543769836426, \"count\": 1, \"min\": 449.1543769836426, \"max\": 449.1543769836426}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1406.5114707510043 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #progress_metric: host=algo-1, completed 34.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] #quality_metric: host=algo-1, epoch=136, train loss <loss>=5.634473419189453\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:14 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[137] Batch[0] avg_epoch_loss=5.580596\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=137, batch=0 train loss <loss>=5.580596446990967\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[137] Batch[5] avg_epoch_loss=5.593924\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=137, batch=5 train loss <loss>=5.593924442927043\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[137] Batch [5]#011Speed: 4214.08 samples/sec#011loss=5.593924\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] processed a total of 613 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081814.9541392, \"EndTime\": 1681081815.4070485, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 452.2838592529297, \"count\": 1, \"min\": 452.2838592529297, \"max\": 452.2838592529297}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1354.887852053114 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #progress_metric: host=algo-1, completed 34.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=137, train loss <loss>=5.610836839675903\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[138] Batch[0] avg_epoch_loss=5.634779\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=138, batch=0 train loss <loss>=5.634779453277588\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[138] Batch[5] avg_epoch_loss=5.643237\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=138, batch=5 train loss <loss>=5.6432366371154785\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[138] Batch [5]#011Speed: 3927.83 samples/sec#011loss=5.643237\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[138] Batch[10] avg_epoch_loss=5.621161\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=138, batch=10 train loss <loss>=5.594670295715332\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] Epoch[138] Batch [10]#011Speed: 3507.47 samples/sec#011loss=5.594670\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] processed a total of 676 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081815.407143, \"EndTime\": 1681081815.8343825, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 426.58352851867676, \"count\": 1, \"min\": 426.58352851867676, \"max\": 426.58352851867676}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1583.9410433169837 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #progress_metric: host=algo-1, completed 34.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] #quality_metric: host=algo-1, epoch=138, train loss <loss>=5.62116102738814\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:15 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[139] Batch[0] avg_epoch_loss=5.551909\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=139, batch=0 train loss <loss>=5.551909446716309\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[139] Batch[5] avg_epoch_loss=5.579121\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=139, batch=5 train loss <loss>=5.579121430714925\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[139] Batch [5]#011Speed: 4315.74 samples/sec#011loss=5.579121\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[139] Batch[10] avg_epoch_loss=5.581969\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=139, batch=10 train loss <loss>=5.58538703918457\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[139] Batch [10]#011Speed: 4078.56 samples/sec#011loss=5.585387\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] processed a total of 641 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081815.8345253, \"EndTime\": 1681081816.2541118, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 418.90430450439453, \"count\": 1, \"min\": 418.90430450439453, \"max\": 418.90430450439453}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1529.6313328345598 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #progress_metric: host=algo-1, completed 35.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=139, train loss <loss>=5.581969434564764\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[140] Batch[0] avg_epoch_loss=5.961203\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=140, batch=0 train loss <loss>=5.961202621459961\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[140] Batch[5] avg_epoch_loss=5.745244\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=140, batch=5 train loss <loss>=5.745243867238362\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[140] Batch [5]#011Speed: 3543.84 samples/sec#011loss=5.745244\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[140] Batch[10] avg_epoch_loss=5.727705\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=140, batch=10 train loss <loss>=5.706658267974854\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] Epoch[140] Batch [10]#011Speed: 3604.04 samples/sec#011loss=5.706658\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] processed a total of 651 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081816.25421, \"EndTime\": 1681081816.736762, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 481.9662570953369, \"count\": 1, \"min\": 481.9662570953369, \"max\": 481.9662570953369}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1350.2447330161901 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #progress_metric: host=algo-1, completed 35.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] #quality_metric: host=algo-1, epoch=140, train loss <loss>=5.727704958482222\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:16 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[141] Batch[0] avg_epoch_loss=5.621979\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=141, batch=0 train loss <loss>=5.621979236602783\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[141] Batch[5] avg_epoch_loss=5.605493\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=141, batch=5 train loss <loss>=5.60549275080363\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[141] Batch [5]#011Speed: 4323.05 samples/sec#011loss=5.605493\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[141] Batch[10] avg_epoch_loss=5.660195\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=141, batch=10 train loss <loss>=5.7258378028869625\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[141] Batch [10]#011Speed: 3592.91 samples/sec#011loss=5.725838\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] processed a total of 661 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081816.7368727, \"EndTime\": 1681081817.2136106, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 475.7649898529053, \"count\": 1, \"min\": 475.7649898529053, \"max\": 475.7649898529053}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1388.8340353225476 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #progress_metric: host=algo-1, completed 35.5 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=141, train loss <loss>=5.6601950472051445\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[142] Batch[0] avg_epoch_loss=5.506892\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=142, batch=0 train loss <loss>=5.50689172744751\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[142] Batch[5] avg_epoch_loss=5.584683\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=142, batch=5 train loss <loss>=5.584682782491048\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[142] Batch [5]#011Speed: 4122.51 samples/sec#011loss=5.584683\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] processed a total of 619 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081817.2137308, \"EndTime\": 1681081817.6089647, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 394.61612701416016, \"count\": 1, \"min\": 394.61612701416016, \"max\": 394.61612701416016}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1567.9489612831185 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #progress_metric: host=algo-1, completed 35.75 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=142, train loss <loss>=5.586532878875732\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[143] Batch[0] avg_epoch_loss=5.692622\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=143, batch=0 train loss <loss>=5.692622184753418\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[143] Batch[5] avg_epoch_loss=5.621320\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] #quality_metric: host=algo-1, epoch=143, batch=5 train loss <loss>=5.621320168177287\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:17 INFO 139996980188992] Epoch[143] Batch [5]#011Speed: 4332.04 samples/sec#011loss=5.621320\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[143] Batch[10] avg_epoch_loss=5.620252\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=143, batch=10 train loss <loss>=5.618971061706543\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[143] Batch [10]#011Speed: 2994.63 samples/sec#011loss=5.618971\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] processed a total of 688 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081817.6090739, \"EndTime\": 1681081818.0964997, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 486.82498931884766, \"count\": 1, \"min\": 486.82498931884766, \"max\": 486.82498931884766}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1412.753863349184 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #progress_metric: host=algo-1, completed 36.0 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=143, train loss <loss>=5.620252392508767\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[144] Batch[0] avg_epoch_loss=5.682554\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=144, batch=0 train loss <loss>=5.682553768157959\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[144] Batch[5] avg_epoch_loss=5.689288\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=144, batch=5 train loss <loss>=5.689288059870402\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[144] Batch [5]#011Speed: 4240.22 samples/sec#011loss=5.689288\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[144] Batch[10] avg_epoch_loss=5.664837\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=144, batch=10 train loss <loss>=5.635495853424072\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Epoch[144] Batch [10]#011Speed: 3019.16 samples/sec#011loss=5.635496\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] processed a total of 681 examples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.0966103, \"EndTime\": 1681081818.5226338, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"update.time\": {\"sum\": 425.4193305969238, \"count\": 1, \"min\": 425.4193305969238, \"max\": 425.4193305969238}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #throughput_metric: host=algo-1, train throughput=1600.1762595489981 records/second\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #progress_metric: host=algo-1, completed 36.25 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, epoch=144, train loss <loss>=5.664837056940252\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] loss did not improve\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Loading parameters from best epoch (104)\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.5227404, \"EndTime\": 1681081818.5294085, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"state.deserialize.time\": {\"sum\": 6.039857864379883, \"count\": 1, \"min\": 6.039857864379883, \"max\": 6.039857864379883}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] stopping training now\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Final loss: 5.564499092102051 (occurred at epoch 104)\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, train final_loss <loss>=5.564499092102051\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 WARNING 139996980188992] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.529527, \"EndTime\": 1681081818.5796173, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"get_graph.time\": {\"sum\": 49.11088943481445, \"count\": 1, \"min\": 49.11088943481445, \"max\": 49.11088943481445}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.5797453, \"EndTime\": 1681081818.6026192, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"finalize.time\": {\"sum\": 72.17860221862793, \"count\": 1, \"min\": 72.17860221862793, \"max\": 72.17860221862793}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.6027281, \"EndTime\": 1681081818.6070137, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.serialize.time\": {\"sum\": 4.227876663208008, \"count\": 1, \"min\": 4.227876663208008, \"max\": 4.227876663208008}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #memory_usage::<batchbuffer> = 1.34765625 mb\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.6070993, \"EndTime\": 1681081818.611899, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.bind.time\": {\"sum\": 0.04935264587402344, \"count\": 1, \"min\": 0.04935264587402344, \"max\": 0.04935264587402344}}}\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.6119702, \"EndTime\": 1681081818.7731595, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"model.score.time\": {\"sum\": 161.3001823425293, \"count\": 1, \"min\": 161.3001823425293, \"max\": 161.3001823425293}}}\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, RMSE): 76.96272913010206\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, mean_absolute_QuantileLoss): 763.7950412326389\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, mean_wQuantileLoss): 0.011826450454471065\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.1]): 0.003257540732074114\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.2]): 0.010128087324766919\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.3]): 0.014112179715156124\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.4]): 0.016182126516202832\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.5]): 0.01627188719954168\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.6]): 0.015418067466792086\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.7]): 0.013729619446879919\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.8]): 0.010752317368566718\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #test_score (algo-1, wQuantileLoss[0.9]): 0.006586228320259197\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, test RMSE <loss>=76.96272913010206\u001b[0m\n",
      "\u001b[34m[04/09/2023 23:10:18 INFO 139996980188992] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.011826450454471065\u001b[0m\n",
      "\u001b[34m#metrics {\"StartTime\": 1681081818.773297, \"EndTime\": 1681081818.780507, \"Dimensions\": {\"Algorithm\": \"AWS/DeepAR\", \"Host\": \"algo-1\", \"Operation\": \"training\"}, \"Metrics\": {\"setuptime\": {\"sum\": 7.267951965332031, \"count\": 1, \"min\": 7.267951965332031, \"max\": 7.267951965332031}, \"totaltime\": {\"sum\": 65617.40398406982, \"count\": 1, \"min\": 65617.40398406982, \"max\": 65617.40398406982}}}\u001b[0m\n",
      "\n",
      "2023-04-09 23:10:34 Uploading - Uploading generated training model\n",
      "2023-04-09 23:10:34 Completed - Training job completed\n",
      "Training seconds: 182\n",
      "Billable seconds: 182\n",
      "CPU times: user 1.87 s, sys: 53.4 ms, total: 1.93 s\n",
      "Wall time: 5min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "data_channels = {\"train\": \"{}/train/\".format(s3_data_path), \"test\": \"{}/test/\".format(s3_data_path)}\n",
    "\n",
    "estimator.fit(inputs=data_channels, wait=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd8750b-c61f-40ac-b0b9-68c4e2580368",
   "metadata": {},
   "source": [
    "### Creación de Endpoint y Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "d47cdca1-2dd9-44eb-9fad-872c228c0e1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.serializers import IdentitySerializer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffe36fa-9fc1-408d-923f-30a1a86ac08d",
   "metadata": {},
   "source": [
    " Esta clase (Creada en ejemplo de AWS para implementación del algoritmo) permite realizar solicitudes utilizando objetos pandas.Series en lugar de cadenas JSON sin procesar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "29f69aef-1af9-4122-a962-3872912dfbf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DeepARPredictor(sagemaker.predictor.Predictor):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(\n",
    "            *args,\n",
    "            # serializer=JSONSerializer(),\n",
    "            serializer=IdentitySerializer(content_type=\"application/json\"),\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        ts,\n",
    "        cat=None,\n",
    "        dynamic_feat=None,\n",
    "        num_samples=100,\n",
    "        return_samples=False,\n",
    "        quantiles=[\"0.1\", \"0.5\", \"0.9\"],\n",
    "    ):\n",
    "        \"\"\"Requests the prediction of for the time series listed in `ts`, each with the (optional)\n",
    "        corresponding category listed in `cat`.\n",
    "\n",
    "        ts -- `pandas.Series` object, the time series to predict\n",
    "        cat -- integer, the group associated to the time series (default: None)\n",
    "        num_samples -- integer, number of samples to compute at prediction time (default: 100)\n",
    "        return_samples -- boolean indicating whether to include samples in the response (default: False)\n",
    "        quantiles -- list of strings specifying the quantiles to compute (default: [\"0.1\", \"0.5\", \"0.9\"])\n",
    "\n",
    "        Return value: list of `pandas.DataFrame` objects, each containing the predictions\n",
    "        \"\"\"\n",
    "        ts.index.freq = 'D' # Establecer una frecuencia diaria\n",
    "        prediction_time = ts.index[-1] + ts.index.freq\n",
    "        quantiles = [str(q) for q in quantiles]\n",
    "        req = self.__encode_request(ts, cat, dynamic_feat, num_samples, return_samples, quantiles)\n",
    "        res = super(DeepARPredictor, self).predict(req)\n",
    "        return self.__decode_response(res, ts.index.freq, prediction_time, return_samples)\n",
    "\n",
    "    def __encode_request(self, ts, cat, dynamic_feat, num_samples, return_samples, quantiles):\n",
    "        instance = series_to_dict(\n",
    "            ts, cat if cat is not None else None, dynamic_feat if dynamic_feat else None\n",
    "        )\n",
    "\n",
    "        configuration = {\n",
    "            \"num_samples\": num_samples,\n",
    "            \"output_types\": [\"quantiles\", \"samples\"] if return_samples else [\"quantiles\"],\n",
    "            \"quantiles\": quantiles,\n",
    "        }\n",
    "\n",
    "        http_request_data = {\"instances\": [instance], \"configuration\": configuration}\n",
    "\n",
    "        return json.dumps(http_request_data).encode(\"utf-8\")\n",
    "\n",
    "    def __decode_response(self, response, freq, prediction_time, return_samples):\n",
    "        # we only sent one time series so we only receive one in return\n",
    "        # however, if possible one will pass multiple time series as predictions will then be faster\n",
    "        predictions = json.loads(response.decode(\"utf-8\"))[\"predictions\"][0]\n",
    "        prediction_length = len(next(iter(predictions[\"quantiles\"].values())))\n",
    "        prediction_index = pd.date_range(\n",
    "            start=prediction_time, freq=freq, periods=prediction_length\n",
    "        )\n",
    "        if return_samples:\n",
    "            dict_of_samples = {\"sample_\" + str(i): s for i, s in enumerate(predictions[\"samples\"])}\n",
    "        else:\n",
    "            dict_of_samples = {}\n",
    "        return pd.DataFrame(\n",
    "            data={**predictions[\"quantiles\"], **dict_of_samples}, index=prediction_index\n",
    "        )\n",
    "\n",
    "    def set_frequency(self, freq):\n",
    "        self.freq = freq\n",
    "\n",
    "\n",
    "def encode_target(ts):\n",
    "    return [x if np.isfinite(x) else \"NaN\" for x in ts]\n",
    "\n",
    "\n",
    "def series_to_dict(ts, cat=None, dynamic_feat=None):\n",
    "    \"\"\"Given a pandas.Series object, returns a dictionary encoding the time series.\n",
    "\n",
    "    ts -- a pands.Series object with the target time series\n",
    "    cat -- an integer indicating the time series category\n",
    "\n",
    "    Return value: a dictionary\n",
    "    \"\"\"\n",
    "    obj = {\"start\": str(ts.index[0]), \"target\": encode_target(ts)}\n",
    "    if cat is not None:\n",
    "        obj[\"cat\"] = cat\n",
    "    if dynamic_feat is not None:\n",
    "        obj[\"dynamic_feat\"] = dynamic_feat\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "aa84e560-17ad-458f-aab9-19422a297f6b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating model with name: deepar-dolar-2023-04-10-00-16-35-797\n",
      "INFO:sagemaker:Creating endpoint-config with name deepar-dolar-2023-04-10-00-16-35-797\n",
      "INFO:sagemaker:Creating endpoint with name deepar-dolar-2023-04-10-00-16-35-797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1, instance_type=\"ml.m5.large\", predictor_cls=DeepARPredictor\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2781694a-f821-4737-bcc2-e80f1dec1004",
   "metadata": {},
   "source": [
    "### Predicción para el día siguiente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4bc1b5d2-5c22-4a36-abd4-0110aed5ba29",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.1</th>\n",
       "      <th>0.5</th>\n",
       "      <th>0.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2023-04-11</th>\n",
       "      <td>4516.992188</td>\n",
       "      <td>4592.122559</td>\n",
       "      <td>4655.128418</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    0.1          0.5          0.9\n",
       "2023-04-11  4516.992188  4592.122559  4655.128418"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.predict(ts=timeseries, quantiles=[0.10, 0.5, 0.90]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ed409e-5a52-4210-b6f5-3634fd4e55c8",
   "metadata": {},
   "source": [
    "De acuerdo con lo anterior el valor más probable de la TRM del dolar con el con el modelo desplagado es $ 4592.122559"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
